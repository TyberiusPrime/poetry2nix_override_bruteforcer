{"info":{"author":"Daniel Bolya","author_email":"","bugtrack_url":null,"classifiers":[],"description":"# Token Merging for Stable Diffusion\n\nUsing nothing but pure python and pytorch, ToMe for SD speeds up diffusion by merging _redundant_ tokens.\n\n![ToMe for SD applied on a 2048x2048 image.](https://raw.githubusercontent.com/dbolya/tomesd/main/examples/assets/teaser.jpg)\n\nThis is the official implementation of **ToMe for SD** from our short paper:  \n**[Token Merging for Fast Stable Diffusion](https://arxiv.org/abs/2303.17604)**  \n[Daniel Bolya](https://dbolya.github.io), [Judy Hoffman](https://faculty.cc.gatech.edu/~judy/)  \n_[GitHub](https://github.com/dbolya/tomesd)_ | _[arXiv](https://arxiv.org/abs/2303.17604)_ | _[BibTeX](#citation)_\n\nToMe for SD is an extension of the original **ToMe**:  \n**[Token Merging: Your ViT but Faster](https://arxiv.org/abs/2210.09461)**  \n[Daniel Bolya](https://dbolya.github.io), \n[Cheng-Yang Fu](http://www.cs.unc.edu/~cyfu/),\n[Xiaoliang Dai](https://sites.google.com/view/xiaoliangdai/),\n[Peizhao Zhang](https://research.facebook.com/people/zhang-peizhao/),\n[Christoph Feichtenhofer](https://feichtenhofer.github.io/),\n[Judy Hoffman](https://faculty.cc.gatech.edu/~judy/)  \n_[ICLR '23 Oral (Top 5%)](https://openreview.net/forum?id=JroZRaRw7Eu)_ | _[GitHub](https://github.com/facebookresearch/ToMe)_ | _[arXiv](https://arxiv.org/abs/2210.09461)_ | _[Blog](https://research.facebook.com/blog/2023/2/token-merging-your-vit-but-faster/)_ | _[BibTeX](https://github.com/facebookresearch/ToMe#citation)_\n\n**Note:** this extension of ToMe is not affiliated in any way with Meta.\n\n\n## What is ToMe for SD?\n![A diffusion block with ToMe applied and the resulting images at different merge ratios.](https://raw.githubusercontent.com/dbolya/tomesd/main/examples/assets/method.jpg)\n\nToken Merging (**ToMe**) speeds up transformers by _merging redundant tokens_, which means the transformer has to do _less work_. We apply this to the underlying transformer blocks in Stable Diffusion in a clever way that minimizes quality loss while keeping most of the speed-up and memory benefits. ToMe for SD _doesn't_ require training and should work out of the box for any Stable Diffusion model.\n\n**Note:** this is a lossy process, so the image _will_ change, ideally not by much. Here are results with [FID](https://github.com/mseitzer/pytorch-fid) scores vs. time and memory usage (lower is better) when using Stable Diffusion v1.5 to generate 512x512 images of ImageNet-1k classes on a 4090 GPU with 50 PLMS steps using fp16:\n\n| Method                      | r% | FID â†“  | Time (s/im) â†“            | Memory (GB/im) â†“        |\n|-----------------------------|----|:------|:--------------------------|:------------------------|\n| Baseline _(Original Model)_ | 0  | 33.12 | 3.09                      | 3.41                    |\n| w/ **ToMe for SD**        | 10 | 32.86 | 2.56 (**1.21x** _faster_) | 2.99 (**1.14x** _less_) |\n|                             | 20 | 32.86 | 2.29 (**1.35x** _faster_) | 2.17 (**1.57x** _less_) |\n|                             | 30 | 32.80 | 2.06 (**1.50x** _faster_) | 1.71 (**1.99x** _less_) |\n|                             | 40 | 32.87 | 1.85 (**1.67x** _faster_) | 1.26 (**2.71x** _less_) |\n|                             | 50 | 33.02 | 1.65 (**1.87x** _faster_) | 0.89 (**3.83x** _less_) |\n|                             | 60 | 33.37 | 1.52 (**2.03x** _faster_) | 0.60 (**5.68x** _less_) |\n\nEven with more than half of the tokens merged (60%!), ToMe for SD still produces images close to the originals, while being _**2x** faster_ and using _**~5.7x** less memory_. Moreover, ToMe is not another efficient reimplementation of transformer modules. Instead, it actually _reduces_ the total work necessary to generate an image, so it can function _in conjunction_ with efficient implementations (see [Usage](#tome--xformers--flash-attn--torch-20)).\n\n## News\n - **[2023.04.02]** ToMe for SD is now available via pip as [tomesd](https://pypi.org/project/tomesd/). Thanks @mkshing!\n - **[2023.03.31]** ToMe for SD now supports [Diffusers](https://github.com/huggingface/diffusers). Thanks @JunnYu and @ExponentialML!\n - **[2023.03.30]** Initial release.\n\nSee the [changelog](CHANGELOG.md) for more details.\n\n\n## Supported Environments\n\nThis repo includes code to patch an existing Stable Diffusion environment. Currently, we support the following implementations:\n - [x] [Stable Diffusion v2](https://github.com/Stability-AI/stablediffusion)\n - [x] [Stable Diffusion v1](https://github.com/runwayml/stable-diffusion)\n - [x] [Latent Diffusion](https://github.com/CompVis/latent-diffusion)\n - [x] [Diffusers](https://github.com/huggingface/diffusers)\n - [ ] And potentially others\n\n**Note:** This also supports most downstream UIs that use these repositories.\n\n\n## Installation\n\nToMe for SD requires ``pytorch >= 1.12.1`` (for `scatter_reduce`), which you can get from [here](https://pytorch.org/get-started/locally/). Then after installing your choice of stable diffusion environment ([supported environments](#supported-environments)), use the corresponding python environment to install ToMe for SD:\n\n```bash\npip install tomesd\n```\n\n### Installing from source\nIf you'd like to install from source to get the latest updates, clone the repository:\n```bash\ngit clone https://github.com/dbolya/tomesd\ncd tomesd\n```\nThen set up the tomesd package with:\n```bash\npython setup.py build develop\n```\nThat's it! ToMe for SD is implemented in pure python, no CUDA compilation required. ðŸ™‚\n\n\n## Usage\nApply ToMe for SD to any Stable Diffusion model with\n```py\nimport tomesd\n\n# Patch a Stable Diffusion model with ToMe for SD using a 50% merging ratio.\n# Using the default options are recommended for the highest quality, tune ratio to suit your needs.\ntomesd.apply_patch(model, ratio=0.5)\n\n# However, if you want to tinker around with the settings, we expose several options.\n# See docstring and paper for details. Note: you can patch the same model multiple times.\ntomesd.apply_patch(model, ratio=0.9, sx=4, sy=4, max_downsample=2) # Extreme merging, expect diminishing returns\n```\nSee above for what speeds and memory savings you can expect with different ratios.\nIf you want to remove the patch later, simply use `tomesd.remove_patch(model)`.\n\n### Example\nTo apply ToMe to the txt2img script of SDv2 or SDv1 for instance, add the following to [this line](https://github.com/Stability-AI/stablediffusion/blob/fc1488421a2761937b9d54784194157882cbc3b1/scripts/txt2img.py#L220) (SDv2) or [this line](https://github.com/runwayml/stable-diffusion/blob/08ab4d326c96854026c4eb3454cd3b02109ee982/scripts/txt2img.py#L241) (SDv1):\n```py\nimport tomesd\ntomesd.apply_patch(model, ratio=0.5)\n```\nThat's it! More examples and demos coming soon (_hopefully_).  \n**Note:** You may not see the full speed-up for the first image generated (as pytorch sets up the graph). Since ToMe for SD uses random processes, you might need to set the seed every batch if you want consistent results.\n\n### Diffusers\nToMe can also be used to patch a ðŸ¤— Diffusers Stable Diffusion pipeline:\n```py\nimport torch, tomesd\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n\n# Apply ToMe with a 50% merging ratio\ntomesd.apply_patch(pipe, ratio=0.5) # Can also use pipe.unet in place of pipe here\n\nimage = pipe(\"a photo of an astronaut riding a horse on mars\").images[0]\nimage.save(\"astronaut.png\")\n```\nYou can remove the patch with `tomesd.remove_patch(pipe)`.\n\n### ToMe + xformers / flash attn / torch 2.0\nSince ToMe only affects the forward function of the block, it should support most efficient transformer implementations out of the box. Just apply the patch as normal!\n\n**Note:** when testing with xFormers, I observed the most speed-up with ToMe when using _big_ images (i.e., 2048x2048 in the parrot example above). You can get even more speed-up with more aggressive merging configs, but quality obviously suffers. For the result above, I had each method img2img from the same 512x512 res image (i.e., I only applied ToMe during the second pass of \"high res fix\") and used the default config with 60% merging. Also, the memory benefits didn't stack with xFormers (efficient attention already takes care of memory concerns).\n\n\n## Citation\n\nIf you use ToMe for SD or this codebase in your work, please cite:\n```\n@article{bolya2023tomesd,\n  title={Token Merging for Fast Stable Diffusion},\n  author={Bolya, Daniel and Hoffman, Judy},\n  journal={arXiv},\n  year={2023}\n}\n```\nIf you use ToMe in general please cite the original work:\n```\n@inproceedings{bolya2023tome,\n  title={Token Merging: Your {ViT} but Faster},\n  author={Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy},\n  booktitle={International Conference on Learning Representations},\n  year={2023}\n}\n```\n","description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/dbolya/tomesd","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"tomesd","package_url":"https://pypi.org/project/tomesd/","platform":null,"project_url":"https://pypi.org/project/tomesd/","project_urls":{"Homepage":"https://github.com/dbolya/tomesd"},"provides_extra":null,"release_url":"https://pypi.org/project/tomesd/0.1.3/","requires_dist":["torch (>=1.12.1)"],"requires_python":"","summary":"Token Merging for Stable Diffusion","version":"0.1.3","yanked":false,"yanked_reason":null},"last_serial":18100477,"releases":{"0.1.1":[{"comment_text":"","digests":{"blake2b_256":"4355c8ae58815cc5cd59e1f1aaf860be591cf54b4325572d90508e7a37219540","md5":"8aa379e5346128ac42bce2dea5844f1a","sha256":"273bd53283447ae09ca901210b98dcc2b7bd972b5c6637a2eb1cfbc7571bf272"},"downloads":-1,"filename":"tomesd-0.1.1-py3-none-any.whl","has_sig":false,"md5_digest":"8aa379e5346128ac42bce2dea5844f1a","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":10812,"upload_time":"2023-04-02T06:34:38","upload_time_iso_8601":"2023-04-02T06:34:38.845196Z","url":"https://files.pythonhosted.org/packages/43/55/c8ae58815cc5cd59e1f1aaf860be591cf54b4325572d90508e7a37219540/tomesd-0.1.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"ebb42eddc963fc3ccb96a6a6163c1476dc7f99bd4aca783379e2b21d748fd172","md5":"5cebbbba618a87758c672ebab12af27f","sha256":"3ce04804f1b2d41e214abcfa60b2cc32197fff9af008a5e53c3fde44587c84f1"},"downloads":-1,"filename":"tomesd-0.1.1.tar.gz","has_sig":false,"md5_digest":"5cebbbba618a87758c672ebab12af27f","packagetype":"sdist","python_version":"source","requires_python":null,"size":13399,"upload_time":"2023-04-02T06:34:40","upload_time_iso_8601":"2023-04-02T06:34:40.774126Z","url":"https://files.pythonhosted.org/packages/eb/b4/2eddc963fc3ccb96a6a6163c1476dc7f99bd4aca783379e2b21d748fd172/tomesd-0.1.1.tar.gz","yanked":false,"yanked_reason":null}],"0.1.2":[{"comment_text":"","digests":{"blake2b_256":"368c82d15c577c104df6a83ed323c6ba5fb83bb14f1f1a61bb72d72ea751b922","md5":"10688e5909c2be374dbe3d3be955c98a","sha256":"eb096fc11a953c5ff84faeb3ab64f5fa82120b2105db88f304c22dc936c0a519"},"downloads":-1,"filename":"tomesd-0.1.2-py3-none-any.whl","has_sig":false,"md5_digest":"10688e5909c2be374dbe3d3be955c98a","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":11069,"upload_time":"2023-04-05T03:28:09","upload_time_iso_8601":"2023-04-05T03:28:09.201598Z","url":"https://files.pythonhosted.org/packages/36/8c/82d15c577c104df6a83ed323c6ba5fb83bb14f1f1a61bb72d72ea751b922/tomesd-0.1.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"f498741c2556824ab7d23e0f585606fbdb45428f352623196acfcc299a090539","md5":"018a9ebfb5df7c54e2fd37c6821e81fd","sha256":"6489d6639a68d0d594bfdb922e76b085fcfd7da8dcdd5d63a59935b5df1c38f8"},"downloads":-1,"filename":"tomesd-0.1.2.tar.gz","has_sig":false,"md5_digest":"018a9ebfb5df7c54e2fd37c6821e81fd","packagetype":"sdist","python_version":"source","requires_python":null,"size":13670,"upload_time":"2023-04-05T03:28:10","upload_time_iso_8601":"2023-04-05T03:28:10.772474Z","url":"https://files.pythonhosted.org/packages/f4/98/741c2556824ab7d23e0f585606fbdb45428f352623196acfcc299a090539/tomesd-0.1.2.tar.gz","yanked":false,"yanked_reason":null}],"0.1.3":[{"comment_text":"","digests":{"blake2b_256":"0c02367c67c8f510313f143a7818e92254a5f861c7d94c98ad6a08d25db52fee","md5":"4f697c38563a446b15221d45fac3b538","sha256":"3d5aa0857fe2c2aab253891050601ca13a87d8d7a99b6760b9ca0856aa0c6355"},"downloads":-1,"filename":"tomesd-0.1.3-py3-none-any.whl","has_sig":false,"md5_digest":"4f697c38563a446b15221d45fac3b538","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":11467,"upload_time":"2023-05-14T22:19:51","upload_time_iso_8601":"2023-05-14T22:19:51.662064Z","url":"https://files.pythonhosted.org/packages/0c/02/367c67c8f510313f143a7818e92254a5f861c7d94c98ad6a08d25db52fee/tomesd-0.1.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"2937ed74c7449fe5a8a4726be3dab4d879d000babf6ea538658171933b1f000e","md5":"2c789e289e20ae55387717d030ae662d","sha256":"15bba2e952f4643c8355951e892fda918ddccbdff2238dc368d42bd078fcedc9"},"downloads":-1,"filename":"tomesd-0.1.3.tar.gz","has_sig":false,"md5_digest":"2c789e289e20ae55387717d030ae662d","packagetype":"sdist","python_version":"source","requires_python":null,"size":14032,"upload_time":"2023-05-14T22:19:53","upload_time_iso_8601":"2023-05-14T22:19:53.307003Z","url":"https://files.pythonhosted.org/packages/29/37/ed74c7449fe5a8a4726be3dab4d879d000babf6ea538658171933b1f000e/tomesd-0.1.3.tar.gz","yanked":false,"yanked_reason":null}]},"urls":[{"comment_text":"","digests":{"blake2b_256":"0c02367c67c8f510313f143a7818e92254a5f861c7d94c98ad6a08d25db52fee","md5":"4f697c38563a446b15221d45fac3b538","sha256":"3d5aa0857fe2c2aab253891050601ca13a87d8d7a99b6760b9ca0856aa0c6355"},"downloads":-1,"filename":"tomesd-0.1.3-py3-none-any.whl","has_sig":false,"md5_digest":"4f697c38563a446b15221d45fac3b538","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":11467,"upload_time":"2023-05-14T22:19:51","upload_time_iso_8601":"2023-05-14T22:19:51.662064Z","url":"https://files.pythonhosted.org/packages/0c/02/367c67c8f510313f143a7818e92254a5f861c7d94c98ad6a08d25db52fee/tomesd-0.1.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"2937ed74c7449fe5a8a4726be3dab4d879d000babf6ea538658171933b1f000e","md5":"2c789e289e20ae55387717d030ae662d","sha256":"15bba2e952f4643c8355951e892fda918ddccbdff2238dc368d42bd078fcedc9"},"downloads":-1,"filename":"tomesd-0.1.3.tar.gz","has_sig":false,"md5_digest":"2c789e289e20ae55387717d030ae662d","packagetype":"sdist","python_version":"source","requires_python":null,"size":14032,"upload_time":"2023-05-14T22:19:53","upload_time_iso_8601":"2023-05-14T22:19:53.307003Z","url":"https://files.pythonhosted.org/packages/29/37/ed74c7449fe5a8a4726be3dab4d879d000babf6ea538658171933b1f000e/tomesd-0.1.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}
