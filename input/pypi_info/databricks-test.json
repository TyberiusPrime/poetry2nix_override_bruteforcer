{"info":{"author":"Alexandre Gattiker","author_email":"algattik@microsoft.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Operating System :: OS Independent","Programming Language :: Python :: 3"],"description":"# databricks_test\n\n## About\n\nAn experimental unit test framework for Databricks notebooks.\n\n_This open-source project is not developed by nor affiliated with Databricks._\n\n## Installing\n\n```\npip install databricks_test\n```\n\n## Usage\n\nAdd a cell at the beginning of your Databricks notebook:\n\n```python\n# Instrument for unit tests. This is only executed in local unit tests, not in Databricks.\nif 'dbutils' not in locals():\n    import databricks_test\n    databricks_test.inject_variables()\n```\n\nThe `if` clause causes the inner code to be skipped when run in Databricks.\nTherefore there is no need to install the `databricks_test` module on your Databricks environment.\n\nAdd your notebook into a code project, for example using [GitHub version control in Azure Databricks](https://docs.microsoft.com/en-us/azure/databricks/notebooks/azure-devops-services-version-control).\n\nSet up pytest in your code project (outside of Databricks).\n\nCreate a test case with the following structure:\n\n```python\nimport databricks_test\n\ndef test_method():\n    with databricks_test.session() as dbrickstest:\n\n        # Set up mocks on dbrickstest\n        # ...\n\n        # Run notebook\n        dbrickstest.run_notebook(\"notebook_dir\", \"notebook_name_without_py_suffix\")\n\n        # Test assertions\n        # ...\n```\n\nYou can set up [mocks](https://docs.python.org/dev/library/unittest.mock.html) on `dbrickstest`, for example:\n\n```python\ndbrickstest.dbutils.widgets.get.return_value = \"myvalue\"\n```\n\nSee samples below for more examples.\n\n## Supported features\n\n* Spark context injected into Databricks notebooks: `spark`, `table`, `sql` etc.\n* PySpark with all Spark features including reading and writing to disk, UDFs and Pandas UDFs\n* Databricks Utilities (`dbutils`, `display`) with user-configurable mocks\n* Mocking connectors such as Azure Storage, S3 and SQL Data Warehouse\n\n## Unsupported features\n\n* Notebook formats other than `.py` (`.ipynb`, `.dbc`) are not supported\n* Non-python cells such as `%scala` and `%sql` (those cells are skipped, as they are stored in `.py` notebooks as comments)\n* Writing directly to `/dbfs` mount on local filesystem:\n  write to a local temporary file instead and use dbutils.fs.cp() to copy to DBFS, which you can intercept with a mock\n* Databricks extensions to Spark such as `spark.read.format(\"binaryFile\")`\n\n## Sample test\n\nSample test case for an ETL notebook reading CSV and writing Parquet.\n\n```python\nimport pandas as pd\nimport databricks_test\nfrom tempfile import TemporaryDirectory\n\nfrom pandas.testing import assert_frame_equal\n\ndef test_etl():\n    with databricks_test.session() as dbrickstest:\n        with TemporaryDirectory() as tmp_dir:\n            out_dir = f\"{tmp_dir}/out\"\n\n            # Provide input and output location as widgets to notebook\n            switch = {\n                \"input\": \"tests/etl_input.csv\",\n                \"output\": out_dir,\n            }\n            dbrickstest.dbutils.widgets.get.side_effect = lambda x: switch.get(\n                x, \"\")\n\n            # Run notebook\n            dbrickstest.run_notebook(\".\", \"etl_notebook\")\n\n            # Notebook produces a Parquet file (directory)\n            resultDF = pd.read_parquet(out_dir)\n\n        # Compare produced Parquet file and expected CSV file\n        expectedDF = pd.read_csv(\"tests/etl_expected.csv\")\n        assert_frame_equal(expectedDF, resultDF, check_dtype=False)\n```\n\nIn the notebook, we pass parameters using widgets.\nThis makes it easy to pass\na local file location in tests, and a remote URL (such as Azure Storage or S3)\nin production.\n\n```python\n# Databricks notebook source\n# This notebook processed the training dataset (imported by Data Factory)\n# and computes a cleaned dataset with additional features such as city.\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DoubleType, IntegerType\nfrom pyspark.sql.functions import col, pandas_udf, PandasUDFType\n\n# COMMAND ----------\n\n# Instrument for unit tests. This is only executed in local unit tests, not in Databricks.\nif 'dbutils' not in locals():\n    import databricks_test\n    databricks_test.inject_variables()\n\n# COMMAND ----------\n\n# Widgets for interactive development.\ndbutils.widgets.text(\"input\", \"\")\ndbutils.widgets.text(\"output\", \"\")\ndbutils.widgets.text(\"secretscope\", \"\")\ndbutils.widgets.text(\"secretname\", \"\")\ndbutils.widgets.text(\"keyname\", \"\")\n\n# COMMAND ----------\n\n# Set up storage credentials\n\nspark.conf.set(\n    dbutils.widgets.get(\"keyname\"),\n    dbutils.secrets.get(\n        scope=dbutils.widgets.get(\"secretscope\"),\n        key=dbutils.widgets.get(\"secretname\")\n    ),\n)\n\n# COMMAND ----------\n\n# Import CSV files\nschema = StructType(\n    [\n        StructField(\"aDouble\", DoubleType(), nullable=False),\n        StructField(\"anInteger\", IntegerType(), nullable=False),\n    ]\n)\n\ndf = (\n    spark.read.format(\"csv\")\n    .options(header=\"true\", mode=\"FAILFAST\")\n    .schema(schema)\n    .load(dbutils.widgets.get('input'))\n)\ndisplay(df)\n\n# COMMAND ----------\n\ndf.count()\n\n# COMMAND ----------\n\n# Inputs and output are pandas.Series of doubles\n@pandas_udf('integer', PandasUDFType.SCALAR)\ndef square(x):\n    return x * x\n\n\n# COMMAND ----------\n\n# Write out Parquet data\n(df\n    .withColumn(\"aSquaredInteger\", square(col(\"anInteger\")))\n    .write\n    .parquet(dbutils.widgets.get('output'))\n )\n```\n\n\n## Advanced mocking\n\nSample test case mocking PySpark classes for a notebook connecting to Azure SQL Data Warehouse.\n\n```python\nimport databricks_test\nimport pyspark\nimport pyspark.sql.functions as F\nfrom tempfile import TemporaryDirectory\nfrom pandas.testing import assert_frame_equal\nimport pandas as pd\n\n\ndef test_sqldw(monkeypatch):\n    with databricks_test.session() as dbrickstest, TemporaryDirectory() as tmp:\n\n        out_dir = f\"{tmp}/out\"\n\n        # Mock SQL DW loader, creating a Spark DataFrame instead\n        def mock_load(reader):\n            return (\n                dbrickstest.spark\n                .range(10)\n                .withColumn(\"age\", F.col(\"id\") * 6)\n                .withColumn(\"salary\", F.col(\"id\") * 10000)\n            )\n\n        monkeypatch.setattr(\n            pyspark.sql.readwriter.DataFrameReader, \"load\", mock_load)\n\n        # Mock SQL DW writer, writing to a local Parquet file instead\n        def mock_save(writer):\n            monkeypatch.undo()\n            writer.format(\"parquet\")\n            writer.save(out_dir)\n\n        monkeypatch.setattr(\n            pyspark.sql.readwriter.DataFrameWriter, \"save\", mock_save)\n\n        # Run notebook\n        dbrickstest.run_notebook(\".\", \"sqldw_notebook\")\n\n        # Notebook produces a Parquet file (directory)\n        resultDF = pd.read_parquet(out_dir)\n\n        # Compare produced Parquet file and expected CSV file\n        expectedDF = pd.read_csv(\"tests/sqldw_expected.csv\")\n        assert_frame_equal(expectedDF, resultDF, check_dtype=False)\n```\n\n## Issues\n\nPlease report issues at [https://github.com/microsoft/DataOps/issues](https://github.com/microsoft/DataOps/issues).\n\n\n","description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/microsoft/DataOps","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"databricks-test","package_url":"https://pypi.org/project/databricks-test/","platform":"","project_url":"https://pypi.org/project/databricks-test/","project_urls":{"Homepage":"https://github.com/microsoft/DataOps"},"provides_extra":null,"release_url":"https://pypi.org/project/databricks-test/0.0.4/","requires_dist":null,"requires_python":"","summary":"Unit testing and mocking for Databricks","version":"0.0.4","yanked":false,"yanked_reason":null},"last_serial":6747622,"releases":{"0.0.1":[{"comment_text":"","digests":{"blake2b_256":"c42bdd8ed56b57571ccc28f4639f21dcefe085d20e581c0b545dcadde8306c26","md5":"71d66d614b9166a4b51b9f3f885f057e","sha256":"56071a5d5cb5f5bd61aba893946b09bec8e3c1f7104fcb1bf9a926e530266da4"},"downloads":-1,"filename":"databricks_test-0.0.1-py3-none-any.whl","has_sig":false,"md5_digest":"71d66d614b9166a4b51b9f3f885f057e","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":7000,"upload_time":"2020-01-18T20:34:32","upload_time_iso_8601":"2020-01-18T20:34:32.002766Z","url":"https://files.pythonhosted.org/packages/c4/2b/dd8ed56b57571ccc28f4639f21dcefe085d20e581c0b545dcadde8306c26/databricks_test-0.0.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"1bd78f402a028f099d333c3539ff414c7f76393aa48f65ff5cbb3e9d1befe883","md5":"c1d7fba2f450aab1b592857782c07564","sha256":"998b4ad0f8d47e10c1c6d97e11dbc1eb4ace25cdae1c26335759b0daffd3d235"},"downloads":-1,"filename":"databricks_test-0.0.1.tar.gz","has_sig":false,"md5_digest":"c1d7fba2f450aab1b592857782c07564","packagetype":"sdist","python_version":"source","requires_python":null,"size":5158,"upload_time":"2020-01-18T20:36:09","upload_time_iso_8601":"2020-01-18T20:36:09.837057Z","url":"https://files.pythonhosted.org/packages/1b/d7/8f402a028f099d333c3539ff414c7f76393aa48f65ff5cbb3e9d1befe883/databricks_test-0.0.1.tar.gz","yanked":false,"yanked_reason":null}],"0.0.2":[{"comment_text":"","digests":{"blake2b_256":"c109abbde28e44790c7375dd998b770a445f4e99dab23004f446500c25901594","md5":"cce003a8540b094202da62f2845231ed","sha256":"bd1f9d8b560acd77375028de219f2b0bb3959ab334f63956ad280e1eeb6d5fb7"},"downloads":-1,"filename":"databricks_test-0.0.2-py3-none-any.whl","has_sig":false,"md5_digest":"cce003a8540b094202da62f2845231ed","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":5127,"upload_time":"2020-02-25T17:38:25","upload_time_iso_8601":"2020-02-25T17:38:25.861586Z","url":"https://files.pythonhosted.org/packages/c1/09/abbde28e44790c7375dd998b770a445f4e99dab23004f446500c25901594/databricks_test-0.0.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"860cf4c33f81418d9a5f8c90841fc3b62d67ec2c03044959449890d8c95e33cd","md5":"6e7099e5be934942b996f1f47a39112a","sha256":"0cecd589f8806ea33953660d48d9d6b521e26e6fb557bbcf813cbd9cb0c7194d"},"downloads":-1,"filename":"databricks_test-0.0.2.tar.gz","has_sig":false,"md5_digest":"6e7099e5be934942b996f1f47a39112a","packagetype":"sdist","python_version":"source","requires_python":null,"size":5368,"upload_time":"2020-02-25T17:38:27","upload_time_iso_8601":"2020-02-25T17:38:27.560934Z","url":"https://files.pythonhosted.org/packages/86/0c/f4c33f81418d9a5f8c90841fc3b62d67ec2c03044959449890d8c95e33cd/databricks_test-0.0.2.tar.gz","yanked":false,"yanked_reason":null}],"0.0.3":[{"comment_text":"","digests":{"blake2b_256":"afeead40c7f5f4caab83df0bf1094d69dd0891a4f5f66d83a789a2a4e97160b8","md5":"827dc4c0348b99ce5436076f402adfe5","sha256":"f98cdc47811da8400eee9d8068cfaa0f5596c9e3498a7bac9b0a2c54ed34afbb"},"downloads":-1,"filename":"databricks_test-0.0.3-py3-none-any.whl","has_sig":false,"md5_digest":"827dc4c0348b99ce5436076f402adfe5","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":5142,"upload_time":"2020-02-26T12:44:34","upload_time_iso_8601":"2020-02-26T12:44:34.768926Z","url":"https://files.pythonhosted.org/packages/af/ee/ad40c7f5f4caab83df0bf1094d69dd0891a4f5f66d83a789a2a4e97160b8/databricks_test-0.0.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"52d0be2bcc6b368bff7909fa03593894238d7a9a1cd0ac7fe86d2bff8b1f62a7","md5":"e9d3214b473820d25d3adef747d12053","sha256":"c1e03eaaf88736254d2b4f86e5bb9a5d0dee960ce4ee8df5b6dc18147a11c3f7"},"downloads":-1,"filename":"databricks_test-0.0.3.tar.gz","has_sig":false,"md5_digest":"e9d3214b473820d25d3adef747d12053","packagetype":"sdist","python_version":"source","requires_python":null,"size":5377,"upload_time":"2020-02-26T12:44:36","upload_time_iso_8601":"2020-02-26T12:44:36.883090Z","url":"https://files.pythonhosted.org/packages/52/d0/be2bcc6b368bff7909fa03593894238d7a9a1cd0ac7fe86d2bff8b1f62a7/databricks_test-0.0.3.tar.gz","yanked":false,"yanked_reason":null}],"0.0.4":[{"comment_text":"","digests":{"blake2b_256":"ddc952a96a597f107bb53aa467f9391424764518664a712476981b4417b104f6","md5":"e360238d171a136d645904afe94cfd4a","sha256":"b05120d883c5cb113367e39ce648c05fe2362878b442566897e56c4f3b5c4967"},"downloads":-1,"filename":"databricks_test-0.0.4-py3-none-any.whl","has_sig":false,"md5_digest":"e360238d171a136d645904afe94cfd4a","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":5158,"upload_time":"2020-03-04T12:50:29","upload_time_iso_8601":"2020-03-04T12:50:29.020963Z","url":"https://files.pythonhosted.org/packages/dd/c9/52a96a597f107bb53aa467f9391424764518664a712476981b4417b104f6/databricks_test-0.0.4-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"bf18590f8c5d0160d6afed83ec7e59b352b694228e9ecfbddf2fddc021df7fbe","md5":"2764975fb61273728e654d0db7687fb2","sha256":"0b40c9e94c07811aaf1a87ae592718f2e84f6ff388b645156479a4e6dcb9cd63"},"downloads":-1,"filename":"databricks_test-0.0.4.tar.gz","has_sig":false,"md5_digest":"2764975fb61273728e654d0db7687fb2","packagetype":"sdist","python_version":"source","requires_python":null,"size":5402,"upload_time":"2020-03-04T12:50:32","upload_time_iso_8601":"2020-03-04T12:50:32.696047Z","url":"https://files.pythonhosted.org/packages/bf/18/590f8c5d0160d6afed83ec7e59b352b694228e9ecfbddf2fddc021df7fbe/databricks_test-0.0.4.tar.gz","yanked":false,"yanked_reason":null}]},"urls":[{"comment_text":"","digests":{"blake2b_256":"ddc952a96a597f107bb53aa467f9391424764518664a712476981b4417b104f6","md5":"e360238d171a136d645904afe94cfd4a","sha256":"b05120d883c5cb113367e39ce648c05fe2362878b442566897e56c4f3b5c4967"},"downloads":-1,"filename":"databricks_test-0.0.4-py3-none-any.whl","has_sig":false,"md5_digest":"e360238d171a136d645904afe94cfd4a","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":5158,"upload_time":"2020-03-04T12:50:29","upload_time_iso_8601":"2020-03-04T12:50:29.020963Z","url":"https://files.pythonhosted.org/packages/dd/c9/52a96a597f107bb53aa467f9391424764518664a712476981b4417b104f6/databricks_test-0.0.4-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"bf18590f8c5d0160d6afed83ec7e59b352b694228e9ecfbddf2fddc021df7fbe","md5":"2764975fb61273728e654d0db7687fb2","sha256":"0b40c9e94c07811aaf1a87ae592718f2e84f6ff388b645156479a4e6dcb9cd63"},"downloads":-1,"filename":"databricks_test-0.0.4.tar.gz","has_sig":false,"md5_digest":"2764975fb61273728e654d0db7687fb2","packagetype":"sdist","python_version":"source","requires_python":null,"size":5402,"upload_time":"2020-03-04T12:50:32","upload_time_iso_8601":"2020-03-04T12:50:32.696047Z","url":"https://files.pythonhosted.org/packages/bf/18/590f8c5d0160d6afed83ec7e59b352b694228e9ecfbddf2fddc021df7fbe/databricks_test-0.0.4.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}
