{"info":{"author":"Maziyar PANAHI","author_email":"maziyar.panahi@cnrs.fr","bugtrack_url":null,"classifiers":["License :: OSI Approved :: Apache Software License","Programming Language :: Python :: 3","Programming Language :: Python :: 3.10","Programming Language :: Python :: 3.11","Programming Language :: Python :: 3.12","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9"],"description":"# Text Generation\n\nThe Hugging Face Text Generation Python library provides a convenient way of interfacing with a\n`text-generation-inference` instance running on\n[Hugging Face Inference Endpoints](https://huggingface.co/inference-endpoints) or on the Hugging Face Hub.\n\n## Get Started\n\n### Install\n\n```shell\npip install text-generation\n```\n\n### Inference API Usage\n\n```python\nfrom text_generation import InferenceAPIClient\n\nclient = InferenceAPIClient(\"bigscience/bloomz\")\ntext = client.generate(\"Why is the sky blue?\").generated_text\nprint(text)\n# ' Rayleigh scattering'\n\n# Token Streaming\ntext = \"\"\nfor response in client.generate_stream(\"Why is the sky blue?\"):\n    if not response.token.special:\n        text += response.token.text\n\nprint(text)\n# ' Rayleigh scattering'\n```\n\nor with the asynchronous client:\n\n```python\nfrom text_generation import InferenceAPIAsyncClient\n\nclient = InferenceAPIAsyncClient(\"bigscience/bloomz\")\nresponse = await client.generate(\"Why is the sky blue?\")\nprint(response.generated_text)\n# ' Rayleigh scattering'\n\n# Token Streaming\ntext = \"\"\nasync for response in client.generate_stream(\"Why is the sky blue?\"):\n    if not response.token.special:\n        text += response.token.text\n\nprint(text)\n# ' Rayleigh scattering'\n```\n\nCheck all currently deployed models on the Huggingface Inference API with `Text Generation` support:\n\n```python\nfrom text_generation.inference_api import deployed_models\n\nprint(deployed_models())\n```\n\n### Hugging Face Inference Endpoint usage\n\n```python\nfrom text_generation import Client\n\nendpoint_url = \"https://YOUR_ENDPOINT.endpoints.huggingface.cloud\"\n\nclient = Client(endpoint_url)\ntext = client.generate(\"Why is the sky blue?\").generated_text\nprint(text)\n# ' Rayleigh scattering'\n\n# Token Streaming\ntext = \"\"\nfor response in client.generate_stream(\"Why is the sky blue?\"):\n    if not response.token.special:\n        text += response.token.text\n\nprint(text)\n# ' Rayleigh scattering'\n```\n\nor with the asynchronous client:\n\n```python\nfrom text_generation import AsyncClient\n\nendpoint_url = \"https://YOUR_ENDPOINT.endpoints.huggingface.cloud\"\n\nclient = AsyncClient(endpoint_url)\nresponse = await client.generate(\"Why is the sky blue?\")\nprint(response.generated_text)\n# ' Rayleigh scattering'\n\n# Token Streaming\ntext = \"\"\nasync for response in client.generate_stream(\"Why is the sky blue?\"):\n    if not response.token.special:\n        text += response.token.text\n\nprint(text)\n# ' Rayleigh scattering'\n```\n\n### Types\n\n```python\n# enum for grammar type\nclass GrammarType(Enum):\n    Json = \"json\"\n    Regex = \"regex\"\n\n\n# Grammar type and value\nclass Grammar:\n    # Grammar type\n    type: GrammarType\n    # Grammar value\n    value: Union[str, dict]\n\nclass Parameters:\n    # Activate logits sampling\n    do_sample: bool\n    # Maximum number of generated tokens\n    max_new_tokens: int\n    # The parameter for repetition penalty. 1.0 means no penalty.\n    # See [this paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n    repetition_penalty: Optional[float]\n    # The parameter for frequency penalty. 1.0 means no penalty\n    # Penalize new tokens based on their existing frequency in the text so far,\n    # decreasing the model's likelihood to repeat the same line verbatim.\n    frequency_penalty: Optional[float]\n    # Whether to prepend the prompt to the generated text\n    return_full_text: bool\n    # Stop generating tokens if a member of `stop_sequences` is generated\n    stop: List[str]\n    # Random sampling seed\n    seed: Optional[int]\n    # The value used to module the logits distribution.\n    temperature: Optional[float]\n    # The number of highest probability vocabulary tokens to keep for top-k-filtering.\n    top_k: Optional[int]\n    # If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or\n    # higher are kept for generation.\n    top_p: Optional[float]\n    # truncate inputs tokens to the given size\n    truncate: Optional[int]\n    # Typical Decoding mass\n    # See [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666) for more information\n    typical_p: Optional[float]\n    # Generate best_of sequences and return the one if the highest token logprobs\n    best_of: Optional[int]\n    # Watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)\n    watermark: bool\n    # Get generation details\n    details: bool\n    # Get decoder input token logprobs and ids\n    decoder_input_details: bool\n    # Return the N most likely tokens at each step\n    top_n_tokens: Optional[int]\n    # grammar to use for generation\n    grammar: Optional[Grammar]\n\nclass Request:\n    # Prompt\n    inputs: str\n    # Generation parameters\n    parameters: Optional[Parameters]\n    # Whether to stream output tokens\n    stream: bool\n\n# Decoder input tokens\nclass InputToken:\n    # Token ID from the model tokenizer\n    id: int\n    # Token text\n    text: str\n    # Logprob\n    # Optional since the logprob of the first token cannot be computed\n    logprob: Optional[float]\n\n\n# Generated tokens\nclass Token:\n    # Token ID from the model tokenizer\n    id: int\n    # Token text\n    text: str\n    # Logprob\n    logprob: Optional[float]\n    # Is the token a special token\n    # Can be used to ignore tokens when concatenating\n    special: bool\n\n\n# Generation finish reason\nclass FinishReason(Enum):\n    # number of generated tokens == `max_new_tokens`\n    Length = \"length\"\n    # the model generated its end of sequence token\n    EndOfSequenceToken = \"eos_token\"\n    # the model generated a text included in `stop_sequences`\n    StopSequence = \"stop_sequence\"\n\n\n# Additional sequences when using the `best_of` parameter\nclass BestOfSequence:\n    # Generated text\n    generated_text: str\n    # Generation finish reason\n    finish_reason: FinishReason\n    # Number of generated tokens\n    generated_tokens: int\n    # Sampling seed if sampling was activated\n    seed: Optional[int]\n    # Decoder input tokens, empty if decoder_input_details is False\n    prefill: List[InputToken]\n    # Generated tokens\n    tokens: List[Token]\n    # Most likely tokens\n    top_tokens: Optional[List[List[Token]]]\n\n\n# `generate` details\nclass Details:\n    # Generation finish reason\n    finish_reason: FinishReason\n    # Number of generated tokens\n    generated_tokens: int\n    # Sampling seed if sampling was activated\n    seed: Optional[int]\n    # Decoder input tokens, empty if decoder_input_details is False\n    prefill: List[InputToken]\n    # Generated tokens\n    tokens: List[Token]\n    # Most likely tokens\n    top_tokens: Optional[List[List[Token]]]\n    # Additional sequences when using the `best_of` parameter\n    best_of_sequences: Optional[List[BestOfSequence]]\n\n\n# `generate` return value\nclass Response:\n    # Generated text\n    generated_text: str\n    # Generation details\n    details: Details\n\n\n# `generate_stream` details\nclass StreamDetails:\n    # Generation finish reason\n    finish_reason: FinishReason\n    # Number of generated tokens\n    generated_tokens: int\n    # Sampling seed if sampling was activated\n    seed: Optional[int]\n\n\n# `generate_stream` return value\nclass StreamResponse:\n    # Generated token\n    token: Token\n    # Most likely tokens\n    top_tokens: Optional[List[Token]]\n    # Complete generated text\n    # Only available when the generation is finished\n    generated_text: Optional[str]\n    # Generation details\n    # Only available when the generation is finished\n    details: Optional[StreamDetails]\n\n# Inference API currently deployed model\nclass DeployedModel:\n    model_id: str\n    sha: str\n```\n\n","description_content_type":"text/markdown","docs_url":null,"download_url":null,"downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/huggingface/text-generation-inference","keywords":null,"license":"Apache-2.0","maintainer":"Maziyar PANAHI","maintainer_email":"maziyar.panahi@cnrs.fr","name":"tgi","package_url":"https://pypi.org/project/tgi/","platform":null,"project_url":"https://pypi.org/project/tgi/","project_urls":{"Homepage":"https://github.com/huggingface/text-generation-inference","Repository":"https://github.com/huggingface/text-generation-inference"},"provides_extra":null,"release_url":"https://pypi.org/project/tgi/2.0.2/","requires_dist":["aiohttp<4.0,>=3.8","huggingface-hub<1.0,>=0.12","pydantic<3,>2"],"requires_python":"<4.0,>=3.7","summary":"Nightly release of Hugging Face Text Generation Python Client","version":"2.0.2","yanked":false,"yanked_reason":null},"last_serial":23162196,"releases":{"1.4.3":[{"comment_text":"","digests":{"blake2b_256":"2611191b8f000b83231f69f604a12604d2ed22f81ac0f4f02db441d0501a132d","md5":"5a0cadbbad300ca20f896b48187fa7e0","sha256":"4314048958382fe1aef2a89df7bc56aa7d456d443af47138eeff613c10fadb5e"},"downloads":-1,"filename":"tgi-1.4.3-py3-none-any.whl","has_sig":false,"md5_digest":"5a0cadbbad300ca20f896b48187fa7e0","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.7,<4.0","size":12096,"upload_time":"2024-03-19T10:11:20","upload_time_iso_8601":"2024-03-19T10:11:20.741777Z","url":"https://files.pythonhosted.org/packages/26/11/191b8f000b83231f69f604a12604d2ed22f81ac0f4f02db441d0501a132d/tgi-1.4.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"024f9403403cb154da4d961f8299500673a6f15d1e1a8ae2b89b2dc099d52fbe","md5":"2cbf3dacc505abae25788e33a213ecaf","sha256":"104d02cf343ca940ebd71ae4834c7f182382f42c994dcbfeb2d2e921dd8787b1"},"downloads":-1,"filename":"tgi-1.4.3.tar.gz","has_sig":false,"md5_digest":"2cbf3dacc505abae25788e33a213ecaf","packagetype":"sdist","python_version":"source","requires_python":">=3.7,<4.0","size":9920,"upload_time":"2024-03-19T10:11:24","upload_time_iso_8601":"2024-03-19T10:11:24.480343Z","url":"https://files.pythonhosted.org/packages/02/4f/9403403cb154da4d961f8299500673a6f15d1e1a8ae2b89b2dc099d52fbe/tgi-1.4.3.tar.gz","yanked":false,"yanked_reason":null}],"1.4.4":[{"comment_text":"","digests":{"blake2b_256":"36d4af2b39e51024061eefc11182b92ee7bef53090bc671c0b0faa1c06584aa4","md5":"bf8e636cd27c7813ee360af42e68fe4d","sha256":"632bae9a6bc0a81ba9126bfe080ec982dc3019b63213c8d1721f742bbe48f9f3"},"downloads":-1,"filename":"tgi-1.4.4-py3-none-any.whl","has_sig":false,"md5_digest":"bf8e636cd27c7813ee360af42e68fe4d","packagetype":"bdist_wheel","python_version":"py3","requires_python":"<4.0,>=3.7","size":12542,"upload_time":"2024-03-26T09:30:15","upload_time_iso_8601":"2024-03-26T09:30:15.684421Z","url":"https://files.pythonhosted.org/packages/36/d4/af2b39e51024061eefc11182b92ee7bef53090bc671c0b0faa1c06584aa4/tgi-1.4.4-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"78e318c48efe22a49ac244f653b1cff6812910dd026606b44e17757b5f144f6e","md5":"4b69d6885e826241eb143a0d68af2d98","sha256":"ab58fea3e07a583d9752ea0f5f3b708fe3167b721308a1fdbe93b2dbf073dde3"},"downloads":-1,"filename":"tgi-1.4.4.tar.gz","has_sig":false,"md5_digest":"4b69d6885e826241eb143a0d68af2d98","packagetype":"sdist","python_version":"source","requires_python":"<4.0,>=3.7","size":10380,"upload_time":"2024-03-26T09:30:18","upload_time_iso_8601":"2024-03-26T09:30:18.578727Z","url":"https://files.pythonhosted.org/packages/78/e3/18c48efe22a49ac244f653b1cff6812910dd026606b44e17757b5f144f6e/tgi-1.4.4.tar.gz","yanked":false,"yanked_reason":null}],"1.4.5":[{"comment_text":"","digests":{"blake2b_256":"f3e452f2f08be3ee777d30f13a9f913132039b58fa3d84a0f30c97b787c28cd1","md5":"23e8c118f5ddab473c455ec78e0d1975","sha256":"cc8bb72fefaa32c34d15a1db1f8b912d0308d5677f9cc7406ce94966bf059dbd"},"downloads":-1,"filename":"tgi-1.4.5-py3-none-any.whl","has_sig":false,"md5_digest":"23e8c118f5ddab473c455ec78e0d1975","packagetype":"bdist_wheel","python_version":"py3","requires_python":"<4.0,>=3.7","size":12543,"upload_time":"2024-04-04T18:46:37","upload_time_iso_8601":"2024-04-04T18:46:37.749784Z","url":"https://files.pythonhosted.org/packages/f3/e4/52f2f08be3ee777d30f13a9f913132039b58fa3d84a0f30c97b787c28cd1/tgi-1.4.5-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"a2f8c8a55c1902876d481d36e807ef4ae4a80bd12bbdfdab99c564e0973de617","md5":"b95f1be6cd35090ee26c072966417ea0","sha256":"468b5a4185f10d27480169b0f707958fdca76ac07ad3d04889355aacd350c700"},"downloads":-1,"filename":"tgi-1.4.5.tar.gz","has_sig":false,"md5_digest":"b95f1be6cd35090ee26c072966417ea0","packagetype":"sdist","python_version":"source","requires_python":"<4.0,>=3.7","size":10365,"upload_time":"2024-04-04T18:46:39","upload_time_iso_8601":"2024-04-04T18:46:39.350994Z","url":"https://files.pythonhosted.org/packages/a2/f8/c8a55c1902876d481d36e807ef4ae4a80bd12bbdfdab99c564e0973de617/tgi-1.4.5.tar.gz","yanked":false,"yanked_reason":null}],"2.0.2":[{"comment_text":"","digests":{"blake2b_256":"7d50c5d926513945676855d7ec1b6455b409d380336ae78fe6cd443ba8e7acfd","md5":"61462148c139d00820102b6767b6c226","sha256":"6f78b38ce80a707e9f25b448a64dc4c7702d5b02e5ff31d158c8fd8222fb0034"},"downloads":-1,"filename":"tgi-2.0.2-py3-none-any.whl","has_sig":false,"md5_digest":"61462148c139d00820102b6767b6c226","packagetype":"bdist_wheel","python_version":"py3","requires_python":"<4.0,>=3.7","size":12635,"upload_time":"2024-05-11T15:12:33","upload_time_iso_8601":"2024-05-11T15:12:33.988793Z","url":"https://files.pythonhosted.org/packages/7d/50/c5d926513945676855d7ec1b6455b409d380336ae78fe6cd443ba8e7acfd/tgi-2.0.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"a2467ee4366cd2efc8996d13c3f4d025c191f5b291042a9f41fd58fcc67b1b47","md5":"f6dca53e3ea00d521d62ff533182ffd1","sha256":"f1634dbb1f6d94e8d1db2213cda7b4af0753e36b1dfb1e5f39e5017fb56a88a1"},"downloads":-1,"filename":"tgi-2.0.2.tar.gz","has_sig":false,"md5_digest":"f6dca53e3ea00d521d62ff533182ffd1","packagetype":"sdist","python_version":"source","requires_python":"<4.0,>=3.7","size":10474,"upload_time":"2024-05-11T15:12:35","upload_time_iso_8601":"2024-05-11T15:12:35.629806Z","url":"https://files.pythonhosted.org/packages/a2/46/7ee4366cd2efc8996d13c3f4d025c191f5b291042a9f41fd58fcc67b1b47/tgi-2.0.2.tar.gz","yanked":false,"yanked_reason":null}]},"urls":[{"comment_text":"","digests":{"blake2b_256":"7d50c5d926513945676855d7ec1b6455b409d380336ae78fe6cd443ba8e7acfd","md5":"61462148c139d00820102b6767b6c226","sha256":"6f78b38ce80a707e9f25b448a64dc4c7702d5b02e5ff31d158c8fd8222fb0034"},"downloads":-1,"filename":"tgi-2.0.2-py3-none-any.whl","has_sig":false,"md5_digest":"61462148c139d00820102b6767b6c226","packagetype":"bdist_wheel","python_version":"py3","requires_python":"<4.0,>=3.7","size":12635,"upload_time":"2024-05-11T15:12:33","upload_time_iso_8601":"2024-05-11T15:12:33.988793Z","url":"https://files.pythonhosted.org/packages/7d/50/c5d926513945676855d7ec1b6455b409d380336ae78fe6cd443ba8e7acfd/tgi-2.0.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"a2467ee4366cd2efc8996d13c3f4d025c191f5b291042a9f41fd58fcc67b1b47","md5":"f6dca53e3ea00d521d62ff533182ffd1","sha256":"f1634dbb1f6d94e8d1db2213cda7b4af0753e36b1dfb1e5f39e5017fb56a88a1"},"downloads":-1,"filename":"tgi-2.0.2.tar.gz","has_sig":false,"md5_digest":"f6dca53e3ea00d521d62ff533182ffd1","packagetype":"sdist","python_version":"source","requires_python":"<4.0,>=3.7","size":10474,"upload_time":"2024-05-11T15:12:35","upload_time_iso_8601":"2024-05-11T15:12:35.629806Z","url":"https://files.pythonhosted.org/packages/a2/46/7ee4366cd2efc8996d13c3f4d025c191f5b291042a9f41fd58fcc67b1b47/tgi-2.0.2.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}
