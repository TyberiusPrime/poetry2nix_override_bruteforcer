{"info":{"author":"Pavel Iakubovskii","author_email":"qubvel@gmail.com","bugtrack_url":null,"classifiers":["License :: OSI Approved :: MIT License","Programming Language :: Python","Programming Language :: Python :: 3","Programming Language :: Python :: Implementation :: CPython","Programming Language :: Python :: Implementation :: PyPy"],"description":"\n<div align=\"center\">\n\n![logo](https://i.ibb.co/dc1XdhT/Segmentation-Models-V2-Side-1-1.png)  \n**Python library with Neural Networks for Image  \nSegmentation based on [PyTorch](https://pytorch.org/).**  \n\n[![Generic badge](https://img.shields.io/badge/License-MIT-<COLOR>.svg?style=for-the-badge)](https://github.com/qubvel/segmentation_models.pytorch/blob/master/LICENSE) \n[![GitHub Workflow Status (branch)](https://img.shields.io/github/actions/workflow/status/qubvel/segmentation_models.pytorch/tests.yml?branch=master&style=for-the-badge)](https://github.com/qubvel/segmentation_models.pytorch/actions/workflows/tests.yml) \n[![Read the Docs](https://img.shields.io/readthedocs/smp?style=for-the-badge&logo=readthedocs&logoColor=white)](https://smp.readthedocs.io/en/latest/) \n<br>\n[![PyPI](https://img.shields.io/pypi/v/segmentation-models-pytorch?color=blue&style=for-the-badge&logo=pypi&logoColor=white)](https://pypi.org/project/segmentation-models-pytorch/) \n[![PyPI - Downloads](https://img.shields.io/pypi/dm/segmentation-models-pytorch?style=for-the-badge&color=blue)](https://pepy.tech/project/segmentation-models-pytorch) \n<br>\n[![PyTorch - Version](https://img.shields.io/badge/PYTORCH-1.4+-red?style=for-the-badge&logo=pytorch)](https://pepy.tech/project/segmentation-models-pytorch) \n[![Python - Version](https://img.shields.io/badge/PYTHON-3.7+-red?style=for-the-badge&logo=python&logoColor=white)](https://pepy.tech/project/segmentation-models-pytorch) \n\n</div>\n\nThe main features of this library are:\n\n - High level API (just two lines to create a neural network)\n - 9 models architectures for binary and multi class segmentation (including legendary Unet)\n - 124 available encoders (and 500+ encoders from [timm](https://github.com/rwightman/pytorch-image-models))\n - All encoders have pre-trained weights for faster and better convergence\n - Popular metrics and losses for training routines\n\n### [üìö Project Documentation üìö](http://smp.readthedocs.io/)\n\nVisit [Read The Docs Project Page](https://smp.readthedocs.io/) or read following README to know more about Segmentation Models Pytorch (SMP for short) library\n\n### üìã Table of content\n 1. [Quick start](#start)\n 2. [Examples](#examples)\n 3. [Models](#models)\n    1. [Architectures](#architectures)\n    2. [Encoders](#encoders)\n    3. [Timm Encoders](#timm)\n 4. [Models API](#api)\n    1. [Input channels](#input-channels)\n    2. [Auxiliary classification output](#auxiliary-classification-output)\n    3. [Depth](#depth)\n 5. [Installation](#installation)\n 6. [Competitions won with the library](#competitions-won-with-the-library)\n 7. [Contributing](#contributing)\n 8. [Citing](#citing)\n 9. [License](#license)\n\n### ‚è≥ Quick start <a name=\"start\"></a>\n\n#### 1. Create your first Segmentation model with SMP\n\nSegmentation model is just a PyTorch nn.Module, which can be created as easy as:\n\n```python\nimport segmentation_models_pytorch as smp\n\nmodel = smp.Unet(\n    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n    classes=3,                      # model output channels (number of classes in your dataset)\n)\n```\n - see [table](#architectures) with available model architectures\n - see [table](#encoders) with available encoders and their corresponding weights\n\n#### 2. Configure data preprocessing\n\nAll encoders have pretrained weights. Preparing your data the same way as during weights pre-training may give you better results (higher metric score and faster convergence). It is **not necessary** in case you train the whole model, not only decoder.\n\n```python\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn\n\npreprocess_input = get_preprocessing_fn('resnet18', pretrained='imagenet')\n```\n\nCongratulations! You are done! Now you can train your model with your favorite framework!\n\n### üí° Examples <a name=\"examples\"></a>\n - Training model for pets binary segmentation with Pytorch-Lightning [notebook](https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/binary_segmentation_intro.ipynb) and [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/qubvel/segmentation_models.pytorch/blob/master/examples/binary_segmentation_intro.ipynb)\n - Training model for cars segmentation on CamVid dataset [here](https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb).\n - Training SMP model with [Catalyst](https://github.com/catalyst-team/catalyst) (high-level framework for PyTorch), [TTAch](https://github.com/qubvel/ttach) (TTA library for PyTorch) and [Albumentations](https://github.com/albu/albumentations) (fast image augmentation library) - [here](https://github.com/catalyst-team/catalyst/blob/v21.02rc0/examples/notebooks/segmentation-tutorial.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/v21.02rc0/examples/notebooks/segmentation-tutorial.ipynb)\n - Training SMP model with [Pytorch-Lightning](https://pytorch-lightning.readthedocs.io) framework - [here](https://github.com/ternaus/cloths_segmentation) (clothes binary segmentation by [@ternaus](https://github.com/ternaus)).\n\n### üì¶ Models <a name=\"models\"></a>\n\n#### Architectures <a name=\"architectures\"></a>\n - Unet [[paper](https://arxiv.org/abs/1505.04597)] [[docs](https://smp.readthedocs.io/en/latest/models.html#unet)]\n - Unet++ [[paper](https://arxiv.org/pdf/1807.10165.pdf)] [[docs](https://smp.readthedocs.io/en/latest/models.html#id2)]\n - MAnet [[paper](https://ieeexplore.ieee.org/abstract/document/9201310)] [[docs](https://smp.readthedocs.io/en/latest/models.html#manet)]\n - Linknet [[paper](https://arxiv.org/abs/1707.03718)] [[docs](https://smp.readthedocs.io/en/latest/models.html#linknet)]\n - FPN [[paper](http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf)] [[docs](https://smp.readthedocs.io/en/latest/models.html#fpn)]\n - PSPNet [[paper](https://arxiv.org/abs/1612.01105)] [[docs](https://smp.readthedocs.io/en/latest/models.html#pspnet)]\n - PAN [[paper](https://arxiv.org/abs/1805.10180)] [[docs](https://smp.readthedocs.io/en/latest/models.html#pan)]\n - DeepLabV3 [[paper](https://arxiv.org/abs/1706.05587)] [[docs](https://smp.readthedocs.io/en/latest/models.html#deeplabv3)]\n - DeepLabV3+ [[paper](https://arxiv.org/abs/1802.02611)] [[docs](https://smp.readthedocs.io/en/latest/models.html#id9)]\n\n#### Encoders <a name=\"encoders\"></a>\n\nThe following is a list of supported encoders in the SMP. Select the appropriate family of encoders and click to expand the table and select a specific encoder and its pre-trained weights (`encoder_name` and `encoder_weights` parameters).\n\n<details>\n<summary style=\"margin-left: 25px;\">ResNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|resnet18                        |imagenet / ssl / swsl           |11M                             |\n|resnet34                        |imagenet                        |21M                             |\n|resnet50                        |imagenet / ssl / swsl           |23M                             |\n|resnet101                       |imagenet                        |42M                             |\n|resnet152                       |imagenet                        |58M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">ResNeXt</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|resnext50_32x4d                 |imagenet / ssl / swsl           |22M                             |\n|resnext101_32x4d                |ssl / swsl                      |42M                             |\n|resnext101_32x8d                |imagenet / instagram / ssl / swsl|86M                         |\n|resnext101_32x16d               |instagram / ssl / swsl          |191M                            |\n|resnext101_32x32d               |instagram                       |466M                            |\n|resnext101_32x48d               |instagram                       |826M                            |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">ResNeSt</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-resnest14d                 |imagenet                        |8M                              |\n|timm-resnest26d                 |imagenet                        |15M                             |\n|timm-resnest50d                 |imagenet                        |25M                             |\n|timm-resnest101e                |imagenet                        |46M                             |\n|timm-resnest200e                |imagenet                        |68M                             |\n|timm-resnest269e                |imagenet                        |108M                            |\n|timm-resnest50d_4s2x40d         |imagenet                        |28M                             |\n|timm-resnest50d_1s4x24d         |imagenet                        |23M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">Res2Ne(X)t</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-res2net50_26w_4s           |imagenet                        |23M                             |\n|timm-res2net101_26w_4s          |imagenet                        |43M                             |\n|timm-res2net50_26w_6s           |imagenet                        |35M                             |\n|timm-res2net50_26w_8s           |imagenet                        |46M                             |\n|timm-res2net50_48w_2s           |imagenet                        |23M                             |\n|timm-res2net50_14w_8s           |imagenet                        |23M                             |\n|timm-res2next50                 |imagenet                        |22M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">RegNet(x/y)</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-regnetx_002                |imagenet                        |2M                              |\n|timm-regnetx_004                |imagenet                        |4M                              |\n|timm-regnetx_006                |imagenet                        |5M                              |\n|timm-regnetx_008                |imagenet                        |6M                              |\n|timm-regnetx_016                |imagenet                        |8M                              |\n|timm-regnetx_032                |imagenet                        |14M                             |\n|timm-regnetx_040                |imagenet                        |20M                             |\n|timm-regnetx_064                |imagenet                        |24M                             |\n|timm-regnetx_080                |imagenet                        |37M                             |\n|timm-regnetx_120                |imagenet                        |43M                             |\n|timm-regnetx_160                |imagenet                        |52M                             |\n|timm-regnetx_320                |imagenet                        |105M                            |\n|timm-regnety_002                |imagenet                        |2M                              |\n|timm-regnety_004                |imagenet                        |3M                              |\n|timm-regnety_006                |imagenet                        |5M                              |\n|timm-regnety_008                |imagenet                        |5M                              |\n|timm-regnety_016                |imagenet                        |10M                             |\n|timm-regnety_032                |imagenet                        |17M                             |\n|timm-regnety_040                |imagenet                        |19M                             |\n|timm-regnety_064                |imagenet                        |29M                             |\n|timm-regnety_080                |imagenet                        |37M                             |\n|timm-regnety_120                |imagenet                        |49M                             |\n|timm-regnety_160                |imagenet                        |80M                             |\n|timm-regnety_320                |imagenet                        |141M                            |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">GERNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-gernet_s                   |imagenet                        |6M                              |\n|timm-gernet_m                   |imagenet                        |18M                             |\n|timm-gernet_l                   |imagenet                        |28M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">SE-Net</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|senet154                        |imagenet                        |113M                            |\n|se_resnet50                     |imagenet                        |26M                             |\n|se_resnet101                    |imagenet                        |47M                             |\n|se_resnet152                    |imagenet                        |64M                             |\n|se_resnext50_32x4d              |imagenet                        |25M                             |\n|se_resnext101_32x4d             |imagenet                        |46M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">SK-ResNe(X)t</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-skresnet18                 |imagenet                        |11M                             |\n|timm-skresnet34                 |imagenet                        |21M                             |\n|timm-skresnext50_32x4d          |imagenet                        |25M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">DenseNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|densenet121                     |imagenet                        |6M                              |\n|densenet169                     |imagenet                        |12M                             |\n|densenet201                     |imagenet                        |18M                             |\n|densenet161                     |imagenet                        |26M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">Inception</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|inceptionresnetv2               |imagenet /  imagenet+background |54M                             |\n|inceptionv4                     |imagenet /  imagenet+background |41M                             |\n|xception                        |imagenet                        |22M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">EfficientNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|efficientnet-b0                 |imagenet                        |4M                              |\n|efficientnet-b1                 |imagenet                        |6M                              |\n|efficientnet-b2                 |imagenet                        |7M                              |\n|efficientnet-b3                 |imagenet                        |10M                             |\n|efficientnet-b4                 |imagenet                        |17M                             |\n|efficientnet-b5                 |imagenet                        |28M                             |\n|efficientnet-b6                 |imagenet                        |40M                             |\n|efficientnet-b7                 |imagenet                        |63M                             |\n|timm-efficientnet-b0            |imagenet / advprop / noisy-student|4M                              |\n|timm-efficientnet-b1            |imagenet / advprop / noisy-student|6M                              |\n|timm-efficientnet-b2            |imagenet / advprop / noisy-student|7M                              |\n|timm-efficientnet-b3            |imagenet / advprop / noisy-student|10M                             |\n|timm-efficientnet-b4            |imagenet / advprop / noisy-student|17M                             |\n|timm-efficientnet-b5            |imagenet / advprop / noisy-student|28M                             |\n|timm-efficientnet-b6            |imagenet / advprop / noisy-student|40M                             |\n|timm-efficientnet-b7            |imagenet / advprop / noisy-student|63M                             |\n|timm-efficientnet-b8            |imagenet / advprop             |84M                             |\n|timm-efficientnet-l2            |noisy-student                   |474M                            |\n|timm-efficientnet-lite0         |imagenet                        |4M                              |\n|timm-efficientnet-lite1         |imagenet                        |5M                              |\n|timm-efficientnet-lite2         |imagenet                        |6M                              |\n|timm-efficientnet-lite3         |imagenet                        |8M                             |\n|timm-efficientnet-lite4         |imagenet                        |13M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">MobileNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|mobilenet_v2                    |imagenet                        |2M                              |\n|timm-mobilenetv3_large_075      |imagenet                        |1.78M                       |\n|timm-mobilenetv3_large_100      |imagenet                        |2.97M                       |\n|timm-mobilenetv3_large_minimal_100|imagenet                        |1.41M                       |\n|timm-mobilenetv3_small_075      |imagenet                        |0.57M                        |\n|timm-mobilenetv3_small_100      |imagenet                        |0.93M                       |\n|timm-mobilenetv3_small_minimal_100|imagenet                        |0.43M                       |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">DPN</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|dpn68                           |imagenet                        |11M                             |\n|dpn68b                          |imagenet+5k                     |11M                             |\n|dpn92                           |imagenet+5k                     |34M                             |\n|dpn98                           |imagenet                        |58M                             |\n|dpn107                          |imagenet+5k                     |84M                             |\n|dpn131                          |imagenet                        |76M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">VGG</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|vgg11                           |imagenet                        |9M                              |\n|vgg11_bn                        |imagenet                        |9M                              |\n|vgg13                           |imagenet                        |9M                              |\n|vgg13_bn                        |imagenet                        |9M                              |\n|vgg16                           |imagenet                        |14M                             |\n|vgg16_bn                        |imagenet                        |14M                             |\n|vgg19                           |imagenet                        |20M                             |\n|vgg19_bn                        |imagenet                        |20M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">Mix Vision Transformer</summary>\n<div style=\"margin-left: 25px;\">\n\nBackbone from SegFormer pretrained on Imagenet! Can be used with other decoders from package, you can combine Mix Vision Transformer with Unet, FPN and others!\n\nLimitations:  \n\n   - encoder is **not** supported by Linknet, Unet++\n   - encoder is supported by FPN only for encoder **depth = 5**\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|mit_b0                          |imagenet                        |3M                              |\n|mit_b1                          |imagenet                        |13M                             |\n|mit_b2                          |imagenet                        |24M                             |\n|mit_b3                          |imagenet                        |44M                             |\n|mit_b4                          |imagenet                        |60M                             |\n|mit_b5                          |imagenet                        |81M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">MobileOne</summary>\n<div style=\"margin-left: 25px;\">\n\nApple's \"sub-one-ms\" Backbone pretrained on Imagenet! Can be used with all decoders.\n\nNote: In the official github repo the s0 variant has additional num_conv_branches, leading to more params than s1.\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|mobileone_s0                    |imagenet                        |4.6M                              |\n|mobileone_s1                    |imagenet                        |4.0M                              |\n|mobileone_s2                    |imagenet                        |6.5M                              |\n|mobileone_s3                    |imagenet                        |8.8M                              |\n|mobileone_s4                    |imagenet                        |13.6M                             |\n\n</div>\n</details>\n\n\n\\* `ssl`, `swsl` - semi-supervised and weakly-supervised learning on ImageNet ([repo](https://github.com/facebookresearch/semi-supervised-ImageNet1K-models)).\n\n#### Timm Encoders <a name=\"timm\"></a>\n\n[docs](https://smp.readthedocs.io/en/latest/encoders_timm.html)\n\nPytorch Image Models (a.k.a. timm) has a lot of pretrained models and interface which allows using these models as encoders in smp, however, not all models are supported\n\n - not all transformer models have ``features_only`` functionality implemented that is required for encoder\n - some models have inappropriate strides\n\nTotal number of supported encoders: 549\n - [table with available encoders](https://smp.readthedocs.io/en/latest/encoders_timm.html)\n\n### üîÅ Models API <a name=\"api\"></a>\n\n - `model.encoder` - pretrained backbone to extract features of different spatial resolution\n - `model.decoder` - depends on models architecture (`Unet`/`Linknet`/`PSPNet`/`FPN`)\n - `model.segmentation_head` - last block to produce required number of mask channels (include also optional upsampling and activation)\n - `model.classification_head` - optional block which create classification head on top of encoder\n - `model.forward(x)` - sequentially pass `x` through model\\`s encoder, decoder and segmentation head (and classification head if specified)\n\n##### Input channels\nInput channels parameter allows you to create models, which process tensors with arbitrary number of channels.\nIf you use pretrained weights from imagenet - weights of first convolution will be reused. For\n1-channel case it would be a sum of weights of first convolution layer, otherwise channels would be \npopulated with weights like `new_weight[:, i] = pretrained_weight[:, i % 3]` and than scaled with `new_weight * 3 / new_in_channels`.\n```python\nmodel = smp.FPN('resnet34', in_channels=1)\nmask = model(torch.ones([1, 1, 64, 64]))\n```\n\n##### Auxiliary classification output  \nAll models support `aux_params` parameters, which is default set to `None`. \nIf `aux_params = None` then classification auxiliary output is not created, else\nmodel produce not only `mask`, but also `label` output with shape `NC`.\nClassification head consists of GlobalPooling->Dropout(optional)->Linear->Activation(optional) layers, which can be \nconfigured by `aux_params` as follows:\n```python\naux_params=dict(\n    pooling='avg',             # one of 'avg', 'max'\n    dropout=0.5,               # dropout ratio, default is None\n    activation='sigmoid',      # activation function, default is None\n    classes=4,                 # define number of output labels\n)\nmodel = smp.Unet('resnet34', classes=4, aux_params=aux_params)\nmask, label = model(x)\n```\n\n##### Depth\nDepth parameter specify a number of downsampling operations in encoder, so you can make\nyour model lighter if specify smaller `depth`.\n```python\nmodel = smp.Unet('resnet34', encoder_depth=4)\n```\n\n\n### üõ† Installation <a name=\"installation\"></a>\nPyPI version:\n```bash\n$ pip install segmentation-models-pytorch\n````\nLatest version from source:\n```bash\n$ pip install git+https://github.com/qubvel/segmentation_models.pytorch\n````\n\n### üèÜ Competitions won with the library\n\n`Segmentation Models` package is widely used in the image segmentation competitions.\n[Here](https://github.com/qubvel/segmentation_models.pytorch/blob/master/HALLOFFAME.md) you can find competitions, names of the winners and links to their solutions.\n\n### ü§ù Contributing\n\n#### Install SMP  \n\n```bash\nmake install_dev  # create .venv, install SMP in dev mode\n```\n\n#### Run tests and code checks  \n\n```bash\nmake all          # run flake8, black, tests\n```\n\n#### Update table with encoders  \n\n```bash\nmake table        # generate table with encoders and print to stdout\n```\n\n### üìù Citing\n```\n@misc{Iakubovskii:2019,\n  Author = {Pavel Iakubovskii},\n  Title = {Segmentation Models Pytorch},\n  Year = {2019},\n  Publisher = {GitHub},\n  Journal = {GitHub repository},\n  Howpublished = {\\url{https://github.com/qubvel/segmentation_models.pytorch}}\n}\n```\n\n### üõ°Ô∏è License <a name=\"license\"></a>\nProject is distributed under [MIT License](https://github.com/qubvel/segmentation_models.pytorch/blob/master/LICENSE)\n\n\n","description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/qubvel/segmentation_models.pytorch","keywords":"","license":"MIT","maintainer":"","maintainer_email":"","name":"segmentation-models-pytorch","package_url":"https://pypi.org/project/segmentation-models-pytorch/","platform":null,"project_url":"https://pypi.org/project/segmentation-models-pytorch/","project_urls":{"Homepage":"https://github.com/qubvel/segmentation_models.pytorch"},"provides_extra":null,"release_url":"https://pypi.org/project/segmentation-models-pytorch/0.3.3/","requires_dist":["torchvision (>=0.5.0)","pretrainedmodels (==0.7.4)","efficientnet-pytorch (==0.7.1)","timm (==0.9.2)","tqdm","pillow","pytest ; extra == 'test'","mock ; extra == 'test'","pre-commit ; extra == 'test'","black (==22.3.0) ; extra == 'test'","flake8 (==4.0.1) ; extra == 'test'","flake8-docstrings (==1.6.0) ; extra == 'test'"],"requires_python":">=3.7.0","summary":"Image segmentation models with pre-trained backbones. PyTorch.","version":"0.3.3","yanked":false,"yanked_reason":null},"last_serial":18281904,"releases":{"0.0.1":[{"comment_text":"","digests":{"blake2b_256":"537fa009f9d116ca46be5ce8be2e2de318c4da57b62e32fa0b11a938b2808bb8","md5":"eec9faf0a8a6f82e8d71c77fda701fd7","sha256":"694e7985d98e2a1a2caece322c5fa3c4b2b7fcc2c78cd00afb0372026a4a62cb"},"downloads":-1,"filename":"segmentation_models_pytorch-0.0.1-py2.py3-none-any.whl","has_sig":false,"md5_digest":"eec9faf0a8a6f82e8d71c77fda701fd7","packagetype":"bdist_wheel","python_version":"py2.py3","requires_python":">=3.0.0","size":24051,"upload_time":"2019-04-20T10:22:42","upload_time_iso_8601":"2019-04-20T10:22:42.050844Z","url":"https://files.pythonhosted.org/packages/53/7f/a009f9d116ca46be5ce8be2e2de318c4da57b62e32fa0b11a938b2808bb8/segmentation_models_pytorch-0.0.1-py2.py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"525c60d03e239f3c49ab293b4c26b16262fcde77ca1eb95d8b794f75f9e0ecf0","md5":"6166491a531663816d3f3484b546adde","sha256":"f4b2338f995565f9a0018ac869fc7240569d87a4a63dfb666ed59533c7405423"},"downloads":-1,"filename":"segmentation_models_pytorch-0.0.1.tar.gz","has_sig":false,"md5_digest":"6166491a531663816d3f3484b546adde","packagetype":"sdist","python_version":"source","requires_python":">=3.0.0","size":16828,"upload_time":"2019-04-20T10:22:44","upload_time_iso_8601":"2019-04-20T10:22:44.374619Z","url":"https://files.pythonhosted.org/packages/52/5c/60d03e239f3c49ab293b4c26b16262fcde77ca1eb95d8b794f75f9e0ecf0/segmentation_models_pytorch-0.0.1.tar.gz","yanked":false,"yanked_reason":null}],"0.0.2":[{"comment_text":"","digests":{"blake2b_256":"024fa067a7f424cd087e1adc7b0245ce9837a403a063631a94bff7dcb96c6f69","md5":"14f368a5bbcd7a35854c2ed7161a6b42","sha256":"38e4c9869505b350c1a278c07c1cada0cd8d0761b3dbc7bd3ffb47dbbd3e91a5"},"downloads":-1,"filename":"segmentation_models_pytorch-0.0.2.tar.gz","has_sig":false,"md5_digest":"14f368a5bbcd7a35854c2ed7161a6b42","packagetype":"sdist","python_version":"source","requires_python":">=3.0.0","size":16539,"upload_time":"2019-09-19T11:35:13","upload_time_iso_8601":"2019-09-19T11:35:13.799131Z","url":"https://files.pythonhosted.org/packages/02/4f/a067a7f424cd087e1adc7b0245ce9837a403a063631a94bff7dcb96c6f69/segmentation_models_pytorch-0.0.2.tar.gz","yanked":false,"yanked_reason":null}],"0.0.3":[{"comment_text":"","digests":{"blake2b_256":"20c667e9d555d41094988aaaf033b1d7e732a326a2ef41a15b81211b56e464ce","md5":"1ac87e28f00df7ee8d538cc64e8527a1","sha256":"2c4c4f4d843c438813193eaaae6eb7fad1057cf2cd7e9490932e302a5ebfb99e"},"downloads":-1,"filename":"segmentation_models_pytorch-0.0.3-py3-none-any.whl","has_sig":false,"md5_digest":"1ac87e28f00df7ee8d538cc64e8527a1","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.0.0","size":27265,"upload_time":"2019-09-28T18:54:19","upload_time_iso_8601":"2019-09-28T18:54:19.435987Z","url":"https://files.pythonhosted.org/packages/20/c6/67e9d555d41094988aaaf033b1d7e732a326a2ef41a15b81211b56e464ce/segmentation_models_pytorch-0.0.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"43e7294488cbb0696e215f9c40ef31b82603915c7618cb956bb5e3402325e846","md5":"e8a2e9b6a6c74098dab56cdd4aae96d8","sha256":"3f5d95d6adc595814797d3e531ff8dc6f63c4f35d5bb6886fb7569533f8d538a"},"downloads":-1,"filename":"segmentation_models_pytorch-0.0.3.tar.gz","has_sig":false,"md5_digest":"e8a2e9b6a6c74098dab56cdd4aae96d8","packagetype":"sdist","python_version":"source","requires_python":">=3.0.0","size":16649,"upload_time":"2019-09-28T18:54:21","upload_time_iso_8601":"2019-09-28T18:54:21.054692Z","url":"https://files.pythonhosted.org/packages/43/e7/294488cbb0696e215f9c40ef31b82603915c7618cb956bb5e3402325e846/segmentation_models_pytorch-0.0.3.tar.gz","yanked":false,"yanked_reason":null}],"0.1.0":[{"comment_text":"","digests":{"blake2b_256":"7088763a25dfe076a9f30f33466b1bd0f2d31b915b88d4cb4481fe4043cf26b4","md5":"f22d9474ab28db0d5eb9e671c7fc4b4d","sha256":"e328af0998363cd2d03b936e224e08c68e87da3f03f19a3f1f5fe78262f43c77"},"downloads":-1,"filename":"segmentation_models_pytorch-0.1.0-py3-none-any.whl","has_sig":false,"md5_digest":"f22d9474ab28db0d5eb9e671c7fc4b4d","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.0.0","size":42615,"upload_time":"2019-12-09T14:09:27","upload_time_iso_8601":"2019-12-09T14:09:27.360398Z","url":"https://files.pythonhosted.org/packages/70/88/763a25dfe076a9f30f33466b1bd0f2d31b915b88d4cb4481fe4043cf26b4/segmentation_models_pytorch-0.1.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"89f35e73a5942fff361dd79d36bbff73414fae1e273b02aa1ec89c1be148f9c4","md5":"049f93b7646de17575223b6852a8b5d0","sha256":"dd4500710d9150c8b11b4f0c4e8c2020c8310aa257a1d1a7df8a19b9dbb4fd04"},"downloads":-1,"filename":"segmentation_models_pytorch-0.1.0.tar.gz","has_sig":false,"md5_digest":"049f93b7646de17575223b6852a8b5d0","packagetype":"sdist","python_version":"source","requires_python":">=3.0.0","size":23032,"upload_time":"2019-12-09T14:09:31","upload_time_iso_8601":"2019-12-09T14:09:31.002151Z","url":"https://files.pythonhosted.org/packages/89/f3/5e73a5942fff361dd79d36bbff73414fae1e273b02aa1ec89c1be148f9c4/segmentation_models_pytorch-0.1.0.tar.gz","yanked":false,"yanked_reason":null}],"0.1.1":[{"comment_text":"","digests":{"blake2b_256":"a2a0aeadd5e5edb4807705c5ab2b360e9c597dd79474331a3ef74580849e60f4","md5":"536494b10e9cb29c5f66c82cc05e2f36","sha256":"bfa3e4caf5672d5b6fffbf2c25fdd15523c04293dbacd115dc626a01bd86edec"},"downloads":-1,"filename":"segmentation_models_pytorch-0.1.1-py3-none-any.whl","has_sig":false,"md5_digest":"536494b10e9cb29c5f66c82cc05e2f36","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.0.0","size":53920,"upload_time":"2020-09-26T13:20:15","upload_time_iso_8601":"2020-09-26T13:20:15.566776Z","url":"https://files.pythonhosted.org/packages/a2/a0/aeadd5e5edb4807705c5ab2b360e9c597dd79474331a3ef74580849e60f4/segmentation_models_pytorch-0.1.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"ff0759ccfa41f267740116cba232b63309ae168afc000f8fa12fe3eaa995516e","md5":"5804a80f6b76e4ac5a01c8f2dc4cba5c","sha256":"21336d59687991218ffceeefe7d461fd5269ac9342a8234e75e29c7849594111"},"downloads":-1,"filename":"segmentation_models_pytorch-0.1.1.tar.gz","has_sig":false,"md5_digest":"5804a80f6b76e4ac5a01c8f2dc4cba5c","packagetype":"sdist","python_version":"source","requires_python":">=3.0.0","size":30097,"upload_time":"2020-09-26T13:20:16","upload_time_iso_8601":"2020-09-26T13:20:16.790903Z","url":"https://files.pythonhosted.org/packages/ff/07/59ccfa41f267740116cba232b63309ae168afc000f8fa12fe3eaa995516e/segmentation_models_pytorch-0.1.1.tar.gz","yanked":false,"yanked_reason":null}],"0.1.2":[{"comment_text":"","digests":{"blake2b_256":"033637b6b0e54a98ff15eb36ce36c9181fdb627b3e789e23fc764f9e5f01dc68","md5":"10be9e4a390a910cc60b1e71df90e45e","sha256":"6868476e193ab5ad487a4fb3de2d752020c1f47adce4dbcfe6894bc01060f843"},"downloads":-1,"filename":"segmentation_models_pytorch-0.1.2-py3-none-any.whl","has_sig":false,"md5_digest":"10be9e4a390a910cc60b1e71df90e45e","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.0.0","size":53923,"upload_time":"2020-09-28T21:27:07","upload_time_iso_8601":"2020-09-28T21:27:07.382957Z","url":"https://files.pythonhosted.org/packages/03/36/37b6b0e54a98ff15eb36ce36c9181fdb627b3e789e23fc764f9e5f01dc68/segmentation_models_pytorch-0.1.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"08b000ec3cecfae6b00b76f0f66c8d0d9ea13edbc6fce82db19a7647aa6bdb94","md5":"75b91e97c5547a278e0181f7e32a37d2","sha256":"c850f604f540baf0ddb010004857d95798ee6dbdde4c4e4708dd5d744f6e7699"},"downloads":-1,"filename":"segmentation_models_pytorch-0.1.2.tar.gz","has_sig":false,"md5_digest":"75b91e97c5547a278e0181f7e32a37d2","packagetype":"sdist","python_version":"source","requires_python":">=3.0.0","size":30088,"upload_time":"2020-09-28T21:27:10","upload_time_iso_8601":"2020-09-28T21:27:10.367090Z","url":"https://files.pythonhosted.org/packages/08/b0/00ec3cecfae6b00b76f0f66c8d0d9ea13edbc6fce82db19a7647aa6bdb94/segmentation_models_pytorch-0.1.2.tar.gz","yanked":false,"yanked_reason":null}],"0.1.3":[{"comment_text":"","digests":{"blake2b_256":"65548953f9f7ee9d451b0f3be8d635aa3a654579abf898d17502a090efe1155a","md5":"5bc61b4d83d0ae11950a099d9dc078ca","sha256":"3e5bc4c4b8f2efb45eda65e86f0efc8501e1a215a71bef04d8a1bca557e815e8"},"downloads":-1,"filename":"segmentation_models_pytorch-0.1.3-py3-none-any.whl","has_sig":false,"md5_digest":"5bc61b4d83d0ae11950a099d9dc078ca","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.0.0","size":66118,"upload_time":"2020-12-13T10:39:51","upload_time_iso_8601":"2020-12-13T10:39:51.756870Z","url":"https://files.pythonhosted.org/packages/65/54/8953f9f7ee9d451b0f3be8d635aa3a654579abf898d17502a090efe1155a/segmentation_models_pytorch-0.1.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"5c642bbd5508d60b73d9ae0583e040647f190f30cc478abd013cd6fe6a44b488","md5":"2801f9f2393f681b493a1c9e6506fa3d","sha256":"8fb86acb82f8783394c725eea837688f001a2500567ac7eac7a7c2a0e80c814e"},"downloads":-1,"filename":"segmentation_models_pytorch-0.1.3.tar.gz","has_sig":false,"md5_digest":"2801f9f2393f681b493a1c9e6506fa3d","packagetype":"sdist","python_version":"source","requires_python":">=3.0.0","size":38165,"upload_time":"2020-12-13T10:39:53","upload_time_iso_8601":"2020-12-13T10:39:53.409460Z","url":"https://files.pythonhosted.org/packages/5c/64/2bbd5508d60b73d9ae0583e040647f190f30cc478abd013cd6fe6a44b488/segmentation_models_pytorch-0.1.3.tar.gz","yanked":false,"yanked_reason":null}],"0.2.0":[{"comment_text":"","digests":{"blake2b_256":"6362f0c1614f07df58317c85c3203adcfdf2249b505c2f05990f5e0af872a5c4","md5":"b295724e6829434be5eccbd3d0aeaa0d","sha256":"d5528719a03c343d20cdaf6ede40160d66ae9fdd14280e5ce47240de46f76992"},"downloads":-1,"filename":"segmentation_models_pytorch-0.2.0-py3-none-any.whl","has_sig":false,"md5_digest":"b295724e6829434be5eccbd3d0aeaa0d","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.0.0","size":87638,"upload_time":"2021-07-05T09:05:47","upload_time_iso_8601":"2021-07-05T09:05:47.003205Z","url":"https://files.pythonhosted.org/packages/63/62/f0c1614f07df58317c85c3203adcfdf2249b505c2f05990f5e0af872a5c4/segmentation_models_pytorch-0.2.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"78c9e2915a7063f9e9c4771117cdafdfa594da41f623e486524a452ca7441b88","md5":"b8ecd5e91a61238120810fabbf5eb446","sha256":"247266722c23feeef16b0862456c5ce815e5f0a77f95c2cd624a71bf00d955df"},"downloads":-1,"filename":"segmentation_models_pytorch-0.2.0.tar.gz","has_sig":false,"md5_digest":"b8ecd5e91a61238120810fabbf5eb446","packagetype":"sdist","python_version":"source","requires_python":">=3.0.0","size":53057,"upload_time":"2021-07-05T09:05:48","upload_time_iso_8601":"2021-07-05T09:05:48.644805Z","url":"https://files.pythonhosted.org/packages/78/c9/e2915a7063f9e9c4771117cdafdfa594da41f623e486524a452ca7441b88/segmentation_models_pytorch-0.2.0.tar.gz","yanked":false,"yanked_reason":null}],"0.2.1":[{"comment_text":"","digests":{"blake2b_256":"f24d839cd59ce604e3fae72d1d0d45928accf92fa4a3f6f07dc70d1d78f5db65","md5":"585ea667a3e19183c82edc8f1a2e5c1b","sha256":"98822571470867fb0f416c112c32f7f1d21702dd32195ec8f7736932c2de0486"},"downloads":-1,"filename":"segmentation_models_pytorch-0.2.1-py3-none-any.whl","has_sig":false,"md5_digest":"585ea667a3e19183c82edc8f1a2e5c1b","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.0.0","size":88588,"upload_time":"2021-11-18T10:48:51","upload_time_iso_8601":"2021-11-18T10:48:51.849401Z","url":"https://files.pythonhosted.org/packages/f2/4d/839cd59ce604e3fae72d1d0d45928accf92fa4a3f6f07dc70d1d78f5db65/segmentation_models_pytorch-0.2.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"664387d170007500bdceb6864f642c52063a5ec0ce6d77e8fe0565ac192c5c16","md5":"cf0c698bdc1bee499aed71d91165a543","sha256":"86744552b04c6bedf7e10f7928791894d8d9b399b9ed58ed1a3236d2bf69ead6"},"downloads":-1,"filename":"segmentation_models_pytorch-0.2.1.tar.gz","has_sig":false,"md5_digest":"cf0c698bdc1bee499aed71d91165a543","packagetype":"sdist","python_version":"source","requires_python":">=3.0.0","size":58704,"upload_time":"2021-11-18T10:48:53","upload_time_iso_8601":"2021-11-18T10:48:53.725885Z","url":"https://files.pythonhosted.org/packages/66/43/87d170007500bdceb6864f642c52063a5ec0ce6d77e8fe0565ac192c5c16/segmentation_models_pytorch-0.2.1.tar.gz","yanked":false,"yanked_reason":null}],"0.3.0":[{"comment_text":"","digests":{"blake2b_256":"98d37bd25846310d7dad6bd30f9ffc7eea3f354c95cdf7bc5421937a9d40c6ee","md5":"5b305fbfffaf4dc82cc4dfbf998416eb","sha256":"753fb1b301a294bf014d846f320f2b714af03cc90b31bfbcdd88819652de3806"},"downloads":-1,"filename":"segmentation_models_pytorch-0.3.0-py3-none-any.whl","has_sig":false,"md5_digest":"5b305fbfffaf4dc82cc4dfbf998416eb","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.6.0","size":97949,"upload_time":"2022-07-29T10:33:05","upload_time_iso_8601":"2022-07-29T10:33:05.205980Z","url":"https://files.pythonhosted.org/packages/98/d3/7bd25846310d7dad6bd30f9ffc7eea3f354c95cdf7bc5421937a9d40c6ee/segmentation_models_pytorch-0.3.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"a8e413cd27ec965a728f92b4b4a3eb4bca02ecccf78e53459eb1f94798dfde36","md5":"f3c9cae9bd6f77a6a6ffa105e2783e64","sha256":"8e00ed1707698d309d23f207aef15f21465e091aa0f1dc8043ec3300f5f67216"},"downloads":-1,"filename":"segmentation_models_pytorch-0.3.0.tar.gz","has_sig":false,"md5_digest":"f3c9cae9bd6f77a6a6ffa105e2783e64","packagetype":"sdist","python_version":"source","requires_python":">=3.6.0","size":63606,"upload_time":"2022-07-29T10:33:07","upload_time_iso_8601":"2022-07-29T10:33:07.093607Z","url":"https://files.pythonhosted.org/packages/a8/e4/13cd27ec965a728f92b4b4a3eb4bca02ecccf78e53459eb1f94798dfde36/segmentation_models_pytorch-0.3.0.tar.gz","yanked":false,"yanked_reason":null}],"0.3.1":[{"comment_text":"","digests":{"blake2b_256":"18318211a419a327ba506cc398a68e02e6fa3bf0275161c5f82339f13fbc4009","md5":"d9d9199fe03a01ef9cfb2b116d0839bc","sha256":"02c261825aba831e849ec28481321294cc9796acf004c53bdf1844d594fb71e1"},"downloads":-1,"filename":"segmentation_models_pytorch-0.3.1-py3-none-any.whl","has_sig":false,"md5_digest":"d9d9199fe03a01ef9cfb2b116d0839bc","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.6.0","size":102063,"upload_time":"2022-11-30T12:31:40","upload_time_iso_8601":"2022-11-30T12:31:40.437315Z","url":"https://files.pythonhosted.org/packages/18/31/8211a419a327ba506cc398a68e02e6fa3bf0275161c5f82339f13fbc4009/segmentation_models_pytorch-0.3.1-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"bdbbe0406a0e9d5ad9251c8f7ef645c11e8037e75687a2beb6f500e29421b3fa","md5":"13ce150a0cf64b2368ccc25f789b14f1","sha256":"d4a4817cf48872c3461bb7d22864c00f9d491719a6460adb252c035f9b0e8d51"},"downloads":-1,"filename":"segmentation_models_pytorch-0.3.1.tar.gz","has_sig":false,"md5_digest":"13ce150a0cf64b2368ccc25f789b14f1","packagetype":"sdist","python_version":"source","requires_python":">=3.6.0","size":66002,"upload_time":"2022-11-30T12:31:42","upload_time_iso_8601":"2022-11-30T12:31:42.654443Z","url":"https://files.pythonhosted.org/packages/bd/bb/e0406a0e9d5ad9251c8f7ef645c11e8037e75687a2beb6f500e29421b3fa/segmentation_models_pytorch-0.3.1.tar.gz","yanked":false,"yanked_reason":null}],"0.3.2":[{"comment_text":"","digests":{"blake2b_256":"407d6a91c9608fd54c115e617e233c44b7023dc4445d478b258d04314afbd02a","md5":"bfee43eb6c2cc87651f01cf91edba49c","sha256":"dba48e7ead5d34fcb6e5c6d04d6d7c5a61a53fa84264e5481df788a22a1bd66b"},"downloads":-1,"filename":"segmentation_models_pytorch-0.3.2-py3-none-any.whl","has_sig":false,"md5_digest":"bfee43eb6c2cc87651f01cf91edba49c","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.7.0","size":106746,"upload_time":"2023-01-07T10:37:24","upload_time_iso_8601":"2023-01-07T10:37:24.417753Z","url":"https://files.pythonhosted.org/packages/40/7d/6a91c9608fd54c115e617e233c44b7023dc4445d478b258d04314afbd02a/segmentation_models_pytorch-0.3.2-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"9768119271a9693f516cfb1659023fed92a9048904af163243dd7a24a47d5115","md5":"e350fbbe2fafbba41b43bf2cb15e45d2","sha256":"8372733e57a10cb8f6b9e18a20577fbb3fb83549b6945664dc774a9b6d3ecd13"},"downloads":-1,"filename":"segmentation_models_pytorch-0.3.2.tar.gz","has_sig":false,"md5_digest":"e350fbbe2fafbba41b43bf2cb15e45d2","packagetype":"sdist","python_version":"source","requires_python":">=3.7.0","size":69763,"upload_time":"2023-01-07T10:37:26","upload_time_iso_8601":"2023-01-07T10:37:26.111403Z","url":"https://files.pythonhosted.org/packages/97/68/119271a9693f516cfb1659023fed92a9048904af163243dd7a24a47d5115/segmentation_models_pytorch-0.3.2.tar.gz","yanked":false,"yanked_reason":null}],"0.3.3":[{"comment_text":"","digests":{"blake2b_256":"cb704aac1b240b399b108ce58029ae54bc14497e1bbc275dfab8fd3c84c1e35d","md5":"b9297a37f3715b9ecf7a5bf528543afe","sha256":"b4317d6f72cb1caf4b7e1d384096970e202600275f54deb8e774fc04d6c8b82e"},"downloads":-1,"filename":"segmentation_models_pytorch-0.3.3-py3-none-any.whl","has_sig":false,"md5_digest":"b9297a37f3715b9ecf7a5bf528543afe","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.7.0","size":106733,"upload_time":"2023-05-28T15:49:39","upload_time_iso_8601":"2023-05-28T15:49:39.042825Z","url":"https://files.pythonhosted.org/packages/cb/70/4aac1b240b399b108ce58029ae54bc14497e1bbc275dfab8fd3c84c1e35d/segmentation_models_pytorch-0.3.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"dfc199939498a3dc2a25c9b6a6d8659a343b795a9cd914fcec4190320804506f","md5":"31e2bdf8df5f2012cc3895c8939c7fde","sha256":"b3b21ab4cd26a6b2b9e7a6ed466ace6452eb26ed3c31ae491ea2d7cbb01e384b"},"downloads":-1,"filename":"segmentation_models_pytorch-0.3.3.tar.gz","has_sig":false,"md5_digest":"31e2bdf8df5f2012cc3895c8939c7fde","packagetype":"sdist","python_version":"source","requires_python":">=3.7.0","size":69697,"upload_time":"2023-05-28T15:49:41","upload_time_iso_8601":"2023-05-28T15:49:41.567745Z","url":"https://files.pythonhosted.org/packages/df/c1/99939498a3dc2a25c9b6a6d8659a343b795a9cd914fcec4190320804506f/segmentation_models_pytorch-0.3.3.tar.gz","yanked":false,"yanked_reason":null}]},"urls":[{"comment_text":"","digests":{"blake2b_256":"cb704aac1b240b399b108ce58029ae54bc14497e1bbc275dfab8fd3c84c1e35d","md5":"b9297a37f3715b9ecf7a5bf528543afe","sha256":"b4317d6f72cb1caf4b7e1d384096970e202600275f54deb8e774fc04d6c8b82e"},"downloads":-1,"filename":"segmentation_models_pytorch-0.3.3-py3-none-any.whl","has_sig":false,"md5_digest":"b9297a37f3715b9ecf7a5bf528543afe","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.7.0","size":106733,"upload_time":"2023-05-28T15:49:39","upload_time_iso_8601":"2023-05-28T15:49:39.042825Z","url":"https://files.pythonhosted.org/packages/cb/70/4aac1b240b399b108ce58029ae54bc14497e1bbc275dfab8fd3c84c1e35d/segmentation_models_pytorch-0.3.3-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"dfc199939498a3dc2a25c9b6a6d8659a343b795a9cd914fcec4190320804506f","md5":"31e2bdf8df5f2012cc3895c8939c7fde","sha256":"b3b21ab4cd26a6b2b9e7a6ed466ace6452eb26ed3c31ae491ea2d7cbb01e384b"},"downloads":-1,"filename":"segmentation_models_pytorch-0.3.3.tar.gz","has_sig":false,"md5_digest":"31e2bdf8df5f2012cc3895c8939c7fde","packagetype":"sdist","python_version":"source","requires_python":">=3.7.0","size":69697,"upload_time":"2023-05-28T15:49:41","upload_time_iso_8601":"2023-05-28T15:49:41.567745Z","url":"https://files.pythonhosted.org/packages/df/c1/99939498a3dc2a25c9b6a6d8659a343b795a9cd914fcec4190320804506f/segmentation_models_pytorch-0.3.3.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}
