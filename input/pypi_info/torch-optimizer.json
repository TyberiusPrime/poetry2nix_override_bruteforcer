{"info":{"author":"Nikolay Novik","author_email":"nickolainovik@gmail.com","bugtrack_url":null,"classifiers":["Development Status :: 3 - Alpha","Intended Audience :: Developers","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.6","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Topic :: Scientific/Engineering :: Artificial Intelligence"],"description":"torch-optimizer\n===============\n.. image:: https://github.com/jettify/pytorch-optimizer/workflows/CI/badge.svg\n   :target: https://github.com/jettify/pytorch-optimizer/actions?query=workflow%3ACI\n   :alt: GitHub Actions status for master branch\n.. image:: https://codecov.io/gh/jettify/pytorch-optimizer/branch/master/graph/badge.svg\n    :target: https://codecov.io/gh/jettify/pytorch-optimizer\n.. image:: https://img.shields.io/pypi/pyversions/torch-optimizer.svg\n    :target: https://pypi.org/project/torch-optimizer\n.. image:: https://readthedocs.org/projects/pytorch-optimizer/badge/?version=latest\n    :target: https://pytorch-optimizer.readthedocs.io/en/latest/?badge=latest\n    :alt: Documentation Status\n.. image:: https://img.shields.io/pypi/v/torch-optimizer.svg\n    :target: https://pypi.python.org/pypi/torch-optimizer\n.. image:: https://static.deepsource.io/deepsource-badge-light-mini.svg\n    :target: https://deepsource.io/gh/jettify/pytorch-optimizer/?ref=repository-badge\n\n\n**torch-optimizer** -- collection of optimizers for PyTorch_ compatible with optim_\nmodule.\n\n\nSimple example\n--------------\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.DiffGrad(model.parameters(), lr=0.001)\n    optimizer.step()\n\n\nInstallation\n------------\nInstallation process is simple, just::\n\n    $ pip install torch_optimizer\n\n\nDocumentation\n-------------\nhttps://pytorch-optimizer.rtfd.io\n\n\nCitation\n--------\nPlease cite original authors of optimization algorithms. If you like this\npackage::\n\n    @software{Novik_torchoptimizers,\n    \ttitle        = {{torch-optimizer -- collection of optimization algorithms for PyTorch.}},\n    \tauthor       = {Novik, Mykola},\n    \tyear         = 2020,\n    \tmonth        = 1,\n    \tversion      = {1.0.1}\n    }\n\nOr use github feature: \"cite this repository\" button.\n\n\nSupported Optimizers\n====================\n\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `A2GradExp`_  | https://arxiv.org/abs/1810.00553                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `A2GradInc`_  | https://arxiv.org/abs/1810.00553                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `A2GradUni`_  | https://arxiv.org/abs/1810.00553                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `AccSGD`_     | https://arxiv.org/abs/1803.05591                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `AdaBelief`_  | https://arxiv.org/abs/2010.07468                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `AdaBound`_   | https://arxiv.org/abs/1902.09843                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `AdaMod`_     | https://arxiv.org/abs/1910.12249                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `Adafactor`_  | https://arxiv.org/abs/1804.04235                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `Adahessian`_ | https://arxiv.org/abs/2006.00719                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `AdamP`_      | https://arxiv.org/abs/2006.08217                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `AggMo`_      | https://arxiv.org/abs/1804.00325                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `Apollo`_     | https://arxiv.org/abs/2009.13586                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `DiffGrad`_   | https://arxiv.org/abs/1909.11015                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `Lamb`_       | https://arxiv.org/abs/1904.00962                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `Lookahead`_  | https://arxiv.org/abs/1907.08610                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `MADGRAD`_    | https://arxiv.org/abs/2101.11075                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `NovoGrad`_   | https://arxiv.org/abs/1905.11286                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `PID`_        | https://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR18_PID.pdf                                                                        |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `QHAdam`_     | https://arxiv.org/abs/1810.06801                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `QHM`_        | https://arxiv.org/abs/1810.06801                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `RAdam`_      | https://arxiv.org/abs/1908.03265                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `Ranger`_     | https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `RangerQH`_   | https://arxiv.org/abs/1810.06801                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `RangerVA`_   | https://arxiv.org/abs/1908.00700v2                                                                                                   |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `SGDP`_       | https://arxiv.org/abs/2006.08217                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `SGDW`_       | https://arxiv.org/abs/1608.03983                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `SWATS`_      | https://arxiv.org/abs/1712.07628                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `Shampoo`_    | https://arxiv.org/abs/1802.09568                                                                                                     |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n|               |                                                                                                                                      |\n| `Yogi`_       | https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization                                                        |\n+---------------+--------------------------------------------------------------------------------------------------------------------------------------+\n\n\nVisualizations\n--------------\nVisualizations help us to see how different algorithms deals with simple\nsituations like: saddle points, local minima, valleys etc, and may provide\ninteresting insights into inner workings of algorithm. Rosenbrock_ and Rastrigin_\nbenchmark_ functions was selected, because:\n\n* Rosenbrock_ (also known as banana function), is non-convex function that has\n  one global minima  `(1.0. 1.0)`. The global minimum is inside a long,\n  narrow, parabolic shaped flat valley. To find the valley is trivial. To\n  converge to the global minima, however, is difficult. Optimization\n  algorithms might pay a lot of attention to one coordinate, and have\n  problems to follow valley which is relatively flat.\n\n .. image::  https://upload.wikimedia.org/wikipedia/commons/3/32/Rosenbrock_function.svg\n\n* Rastrigin_ function is a non-convex and has one global minima in `(0.0, 0.0)`.\n  Finding the minimum of this function is a fairly difficult problem due to\n  its large search space and its large number of local minima.\n\n  .. image::  https://upload.wikimedia.org/wikipedia/commons/8/8b/Rastrigin_function.png\n\nEach optimizer performs `501` optimization steps. Learning rate is best one found\nby hyper parameter search algorithm, rest of tuning parameters are default. It\nis very easy to extend script and tune other optimizer parameters.\n\n\n.. code::\n\n    python examples/viz_optimizers.py\n\n\nWarning\n-------\nDo not pick optimizer based on visualizations, optimization approaches\nhave unique properties and may be tailored for different purposes or may\nrequire explicit learning rate schedule etc. Best way to find out, is to try one\non your particular problem and see if it improves scores.\n\nIf you do not know which optimizer to use start with built in SGD/Adam, once\ntraining logic is ready and baseline scores are established, swap optimizer and\nsee if there is any improvement.\n\n\nA2GradExp\n---------\n\n+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_A2GradExp.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_A2GradExp.png  |\n+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.A2GradExp(\n        model.parameters(),\n        kappa=1000.0,\n        beta=10.0,\n        lips=10.0,\n        rho=0.5,\n    )\n    optimizer.step()\n\n\n**Paper**: *Optimal Adaptive and Accelerated Stochastic Gradient Descent* (2018) [https://arxiv.org/abs/1810.00553]\n\n**Reference Code**: https://github.com/severilov/A2Grad_optimizer\n\n\nA2GradInc\n---------\n\n+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_A2GradInc.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_A2GradInc.png  |\n+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.A2GradInc(\n        model.parameters(),\n        kappa=1000.0,\n        beta=10.0,\n        lips=10.0,\n    )\n    optimizer.step()\n\n\n**Paper**: *Optimal Adaptive and Accelerated Stochastic Gradient Descent* (2018) [https://arxiv.org/abs/1810.00553]\n\n**Reference Code**: https://github.com/severilov/A2Grad_optimizer\n\n\nA2GradUni\n---------\n\n+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_A2GradUni.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_A2GradUni.png  |\n+--------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.A2GradUni(\n        model.parameters(),\n        kappa=1000.0,\n        beta=10.0,\n        lips=10.0,\n    )\n    optimizer.step()\n\n\n**Paper**: *Optimal Adaptive and Accelerated Stochastic Gradient Descent* (2018) [https://arxiv.org/abs/1810.00553]\n\n**Reference Code**: https://github.com/severilov/A2Grad_optimizer\n\n\nAccSGD\n------\n\n+-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_AccSGD.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_AccSGD.png  |\n+-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.AccSGD(\n        model.parameters(),\n        lr=1e-3,\n        kappa=1000.0,\n        xi=10.0,\n        small_const=0.7,\n        weight_decay=0\n    )\n    optimizer.step()\n\n\n**Paper**: *On the insufficiency of existing momentum schemes for Stochastic Optimization* (2019) [https://arxiv.org/abs/1803.05591]\n\n**Reference Code**: https://github.com/rahulkidambi/AccSGD\n\n\nAdaBelief\n---------\n\n+-------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_AdaBelief.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_AdaBelief.png |\n+-------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.AdaBelief(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-3,\n        weight_decay=0,\n        amsgrad=False,\n        weight_decouple=False,\n        fixed_decay=False,\n        rectify=False,\n    )\n    optimizer.step()\n\n\n**Paper**: *AdaBelief Optimizer, adapting stepsizes by the belief in observed gradients* (2020) [https://arxiv.org/abs/2010.07468]\n\n**Reference Code**: https://github.com/juntang-zhuang/Adabelief-Optimizer\n\n\nAdaBound\n--------\n\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_AdaBound.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_AdaBound.png |\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.AdaBound(\n        m.parameters(),\n        lr= 1e-3,\n        betas= (0.9, 0.999),\n        final_lr = 0.1,\n        gamma=1e-3,\n        eps= 1e-8,\n        weight_decay=0,\n        amsbound=False,\n    )\n    optimizer.step()\n\n\n**Paper**: *Adaptive Gradient Methods with Dynamic Bound of Learning Rate* (2019) [https://arxiv.org/abs/1902.09843]\n\n**Reference Code**: https://github.com/Luolc/AdaBound\n\nAdaMod\n------\nAdaMod method restricts the adaptive learning rates with adaptive and momental\nupper bounds. The dynamic learning rate bounds are based on the exponential\nmoving averages of the adaptive learning rates themselves, which smooth out\nunexpected large learning rates and stabilize the training of deep neural networks.\n\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_AdaMod.png    |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_AdaMod.png   |\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.AdaMod(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        beta3=0.999,\n        eps=1e-8,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n**Paper**: *An Adaptive and Momental Bound Method for Stochastic Learning.* (2019) [https://arxiv.org/abs/1910.12249]\n\n**Reference Code**: https://github.com/lancopku/AdaMod\n\n\nAdafactor\n---------\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Adafactor.png |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Adafactor.png |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.Adafactor(\n        m.parameters(),\n        lr= 1e-3,\n        eps2= (1e-30, 1e-3),\n        clip_threshold=1.0,\n        decay_rate=-0.8,\n        beta1=None,\n        weight_decay=0.0,\n        scale_parameter=True,\n        relative_step=True,\n        warmup_init=False,\n    )\n    optimizer.step()\n\n**Paper**: *Adafactor: Adaptive Learning Rates with Sublinear Memory Cost.* (2018) [https://arxiv.org/abs/1804.04235]\n\n**Reference Code**: https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py\n\n\nAdahessian\n----------\n+-------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Adahessian.png |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Adahessian.png  |\n+-------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.Adahessian(\n        m.parameters(),\n        lr= 1.0,\n        betas= (0.9, 0.999),\n        eps= 1e-4,\n        weight_decay=0.0,\n        hessian_power=1.0,\n    )\n\t  loss_fn(m(input), target).backward(create_graph = True) # create_graph=True is necessary for Hessian calculation\n    optimizer.step()\n\n\n**Paper**: *ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning* (2020) [https://arxiv.org/abs/2006.00719]\n\n**Reference Code**: https://github.com/amirgholami/adahessian\n\n\nAdamP\n------\nAdamP propose a simple and effective solution: at each iteration of Adam optimizer\napplied on scale-invariant weights (e.g., Conv weights preceding a BN layer), AdamP\nremove the radial component (i.e., parallel to the weight vector) from the update vector.\nIntuitively, this operation prevents the unnecessary update along the radial direction\nthat only increases the weight norm without contributing to the loss minimization.\n\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_AdamP.png     |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_AdamP.png    |\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.AdamP(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n        delta = 0.1,\n        wd_ratio = 0.1\n    )\n    optimizer.step()\n\n**Paper**: *Slowing Down the Weight Norm Increase in Momentum-based Optimizers.* (2020) [https://arxiv.org/abs/2006.08217]\n\n**Reference Code**: https://github.com/clovaai/AdamP\n\n\nAggMo\n-----\n\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_AggMo.png     |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_AggMo.png    |\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.AggMo(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.0, 0.9, 0.99),\n        weight_decay=0,\n    )\n    optimizer.step()\n\n**Paper**: *Aggregated Momentum: Stability Through Passive Damping.* (2019) [https://arxiv.org/abs/1804.00325]\n\n**Reference Code**: https://github.com/AtheMathmo/AggMo\n\n\nApollo\n------\n\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Apollo.png    |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Apollo.png   |\n+------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.Apollo(\n        m.parameters(),\n        lr= 1e-2,\n        beta=0.9,\n        eps=1e-4,\n        warmup=0,\n        init_lr=0.01,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n**Paper**: *Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization.* (2020) [https://arxiv.org/abs/2009.13586]\n\n**Reference Code**: https://github.com/XuezheMax/apollo\n\n\nDiffGrad\n--------\nOptimizer based on the difference between the present and the immediate past\ngradient, the step size is adjusted for each parameter in such\na way that it should have a larger step size for faster gradient changing\nparameters and a lower step size for lower gradient changing parameters.\n\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_DiffGrad.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_DiffGrad.png  |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.DiffGrad(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n\n**Paper**: *diffGrad: An Optimization Method for Convolutional Neural Networks.* (2019) [https://arxiv.org/abs/1909.11015]\n\n**Reference Code**: https://github.com/shivram1987/diffGrad\n\nLamb\n----\n\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Lamb.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Lamb.png  |\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.Lamb(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n\n**Paper**: *Large Batch Optimization for Deep Learning: Training BERT in 76 minutes* (2019) [https://arxiv.org/abs/1904.00962]\n\n**Reference Code**: https://github.com/cybertronai/pytorch-lamb\n\nLookahead\n---------\n\n+-----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_LookaheadYogi.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_LookaheadYogi.png  |\n+-----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    # base optimizer, any other optimizer can be used like Adam or DiffGrad\n    yogi = optim.Yogi(\n        m.parameters(),\n        lr= 1e-2,\n        betas=(0.9, 0.999),\n        eps=1e-3,\n        initial_accumulator=1e-6,\n        weight_decay=0,\n    )\n\n    optimizer = optim.Lookahead(yogi, k=5, alpha=0.5)\n    optimizer.step()\n\n\n**Paper**: *Lookahead Optimizer: k steps forward, 1 step back* (2019) [https://arxiv.org/abs/1907.08610]\n\n**Reference Code**: https://github.com/alphadl/lookahead.pytorch\n\n\nMADGRAD\n---------\n\n+-----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_MADGRAD.png        |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_MADGRAD.png        |\n+-----------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.MADGRAD(\n        m.parameters(),\n        lr=1e-2,\n        momentum=0.9,\n        weight_decay=0,\n        eps=1e-6,\n    )\n    optimizer.step()\n\n\n**Paper**: *Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization* (2021) [https://arxiv.org/abs/2101.11075]\n\n**Reference Code**: https://github.com/facebookresearch/madgrad\n\n\nNovoGrad\n--------\n\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_NovoGrad.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_NovoGrad.png  |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.NovoGrad(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n        grad_averaging=False,\n        amsgrad=False,\n    )\n    optimizer.step()\n\n\n**Paper**: *Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks* (2019) [https://arxiv.org/abs/1905.11286]\n\n**Reference Code**: https://github.com/NVIDIA/DeepLearningExamples/\n\n\nPID\n---\n\n+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_PID.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_PID.png  |\n+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.PID(\n        m.parameters(),\n        lr=1e-3,\n        momentum=0,\n        dampening=0,\n        weight_decay=1e-2,\n        integral=5.0,\n        derivative=10.0,\n    )\n    optimizer.step()\n\n\n**Paper**: *A PID Controller Approach for Stochastic Optimization of Deep Networks* (2018) [http://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR18_PID.pdf]\n\n**Reference Code**: https://github.com/tensorboy/PIDOptimizer\n\n\nQHAdam\n------\n\n+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_QHAdam.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_QHAdam.png  |\n+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.QHAdam(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        nus=(1.0, 1.0),\n        weight_decay=0,\n        decouple_weight_decay=False,\n        eps=1e-8,\n    )\n    optimizer.step()\n\n\n**Paper**: *Quasi-hyperbolic momentum and Adam for deep learning* (2019) [https://arxiv.org/abs/1810.06801]\n\n**Reference Code**: https://github.com/facebookresearch/qhoptim\n\n\nQHM\n---\n\n+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_QHM.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_QHM.png  |\n+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.QHM(\n        m.parameters(),\n        lr=1e-3,\n        momentum=0,\n        nu=0.7,\n        weight_decay=1e-2,\n        weight_decay_type='grad',\n    )\n    optimizer.step()\n\n\n**Paper**: *Quasi-hyperbolic momentum and Adam for deep learning* (2019) [https://arxiv.org/abs/1810.06801]\n\n**Reference Code**: https://github.com/facebookresearch/qhoptim\n\n\nRAdam\n-----\n\n+---------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_RAdam.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_RAdam.png  |\n+---------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.RAdam(\n        m.parameters(),\n        lr= 1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n\n**Paper**: *On the Variance of the Adaptive Learning Rate and Beyond* (2019) [https://arxiv.org/abs/1908.03265]\n\n**Reference Code**: https://github.com/LiyuanLucasLiu/RAdam\n\n\nRanger\n------\n\n+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Ranger.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Ranger.png  |\n+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.Ranger(\n        m.parameters(),\n        lr=1e-3,\n        alpha=0.5,\n        k=6,\n        N_sma_threshhold=5,\n        betas=(.95, 0.999),\n        eps=1e-5,\n        weight_decay=0\n    )\n    optimizer.step()\n\n\n**Paper**: *New Deep Learning Optimizer, Ranger: Synergistic combination of RAdam + LookAhead for the best of both* (2019) [https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d]\n\n**Reference Code**: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n\n\nRangerQH\n--------\n\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_RangerQH.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_RangerQH.png  |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.RangerQH(\n        m.parameters(),\n        lr=1e-3,\n        betas=(0.9, 0.999),\n        nus=(.7, 1.0),\n        weight_decay=0.0,\n        k=6,\n        alpha=.5,\n        decouple_weight_decay=False,\n        eps=1e-8,\n    )\n    optimizer.step()\n\n\n**Paper**: *Quasi-hyperbolic momentum and Adam for deep learning* (2018) [https://arxiv.org/abs/1810.06801]\n\n**Reference Code**: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n\n\nRangerVA\n--------\n\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_RangerVA.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_RangerVA.png  |\n+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.RangerVA(\n        m.parameters(),\n        lr=1e-3,\n        alpha=0.5,\n        k=6,\n        n_sma_threshhold=5,\n        betas=(.95, 0.999),\n        eps=1e-5,\n        weight_decay=0,\n        amsgrad=True,\n        transformer='softplus',\n        smooth=50,\n        grad_transformer='square'\n    )\n    optimizer.step()\n\n\n**Paper**: *Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM* (2019) [https://arxiv.org/abs/1908.00700v2]\n\n**Reference Code**: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n\n\nSGDP\n----\n\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_SGDP.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_SGDP.png  |\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.SGDP(\n        m.parameters(),\n        lr= 1e-3,\n        momentum=0,\n        dampening=0,\n        weight_decay=1e-2,\n        nesterov=False,\n        delta = 0.1,\n        wd_ratio = 0.1\n    )\n    optimizer.step()\n\n\n**Paper**: *Slowing Down the Weight Norm Increase in Momentum-based Optimizers.* (2020) [https://arxiv.org/abs/2006.08217]\n\n**Reference Code**: https://github.com/clovaai/AdamP\n\n\nSGDW\n----\n\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_SGDW.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_SGDW.png  |\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.SGDW(\n        m.parameters(),\n        lr= 1e-3,\n        momentum=0,\n        dampening=0,\n        weight_decay=1e-2,\n        nesterov=False,\n    )\n    optimizer.step()\n\n\n**Paper**: *SGDR: Stochastic Gradient Descent with Warm Restarts* (2017) [https://arxiv.org/abs/1608.03983]\n\n**Reference Code**: https://github.com/pytorch/pytorch/pull/22466\n\n\nSWATS\n-----\n\n+---------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_SWATS.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_SWATS.png  |\n+---------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.SWATS(\n        model.parameters(),\n        lr=1e-1,\n        betas=(0.9, 0.999),\n        eps=1e-3,\n        weight_decay= 0.0,\n        amsgrad=False,\n        nesterov=False,\n    )\n    optimizer.step()\n\n\n**Paper**: *Improving Generalization Performance by Switching from Adam to SGD* (2017) [https://arxiv.org/abs/1712.07628]\n\n**Reference Code**: https://github.com/Mrpatekful/swats\n\n\nShampoo\n-------\n\n+-----------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Shampoo.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Shampoo.png  |\n+-----------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.Shampoo(\n        m.parameters(),\n        lr=1e-1,\n        momentum=0.0,\n        weight_decay=0.0,\n        epsilon=1e-4,\n        update_freq=1,\n    )\n    optimizer.step()\n\n\n**Paper**: *Shampoo: Preconditioned Stochastic Tensor Optimization* (2018) [https://arxiv.org/abs/1802.09568]\n\n**Reference Code**: https://github.com/moskomule/shampoo.pytorch\n\n\nYogi\n----\n\nYogi is optimization algorithm based on ADAM with more fine grained effective\nlearning rate control, and has similar theoretical guarantees on convergence as ADAM.\n\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Yogi.png  |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Yogi.png  |\n+--------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\n.. code:: python\n\n    import torch_optimizer as optim\n\n    # model = ...\n    optimizer = optim.Yogi(\n        m.parameters(),\n        lr= 1e-2,\n        betas=(0.9, 0.999),\n        eps=1e-3,\n        initial_accumulator=1e-6,\n        weight_decay=0,\n    )\n    optimizer.step()\n\n\n**Paper**: *Adaptive Methods for Nonconvex Optimization* (2018) [https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization]\n\n**Reference Code**: https://github.com/4rtemi5/Yogi-Optimizer_Keras\n\n\nAdam (PyTorch built-in)\n-----------------------\n\n+---------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_Adam.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_Adam.png  |\n+---------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n\nSGD (PyTorch built-in)\n----------------------\n\n+--------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n| .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rastrigin_SGD.png   |  .. image:: https://raw.githubusercontent.com/jettify/pytorch-optimizer/master/docs/rosenbrock_SGD.png  |\n+--------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+\n\n.. _Python: https://www.python.org\n.. _PyTorch: https://github.com/pytorch/pytorch\n.. _Rastrigin: https://en.wikipedia.org/wiki/Rastrigin_function\n.. _Rosenbrock: https://en.wikipedia.org/wiki/Rosenbrock_function\n.. _benchmark: https://en.wikipedia.org/wiki/Test_functions_for_optimization\n.. _optim: https://pytorch.org/docs/stable/optim.html\n\nChanges\n-------\n\n0.3.0 (2021-10-30)\n------------------\n* Revert for Drop RAdam.\n\n0.2.0 (2021-10-25)\n------------------\n* Drop RAdam optimizer since it is included in pytorch.\n* Do not include tests as installable package.\n* Preserver memory layout where possible.\n* Add MADGRAD optimizer.\n\n0.1.0 (2021-01-01)\n------------------\n* Initial release.\n* Added support for A2GradExp, A2GradInc, A2GradUni, AccSGD, AdaBelief,\n  AdaBound, AdaMod, Adafactor, Adahessian, AdamP, AggMo, Apollo,\n  DiffGrad, Lamb, Lookahead, NovoGrad, PID, QHAdam, QHM, RAdam, Ranger,\n  RangerQH, RangerVA, SGDP, SGDW, SWATS, Shampoo, Yogi.\n\n","description_content_type":"text/x-rst","docs_url":null,"download_url":"https://pypi.org/project/torch-optimizer/","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/jettify/pytorch-optimizer","keywords":"torch-optimizer,pytorch,accsgd,adabound,adamod,diffgrad,lamb,lookahead,madgrad,novograd,pid,qhadam,qhm,radam,sgdw,yogi,ranger","license":"Apache 2","maintainer":"","maintainer_email":"","name":"torch-optimizer","package_url":"https://pypi.org/project/torch-optimizer/","platform":"POSIX","project_url":"https://pypi.org/project/torch-optimizer/","project_urls":{"Documentation":"https://pytorch-optimizer.readthedocs.io","Download":"https://pypi.org/project/torch-optimizer/","Homepage":"https://github.com/jettify/pytorch-optimizer","Issues":"https://github.com/jettify/pytorch-optimizer/issues","Website":"https://github.com/jettify/pytorch-optimizer"},"provides_extra":null,"release_url":"https://pypi.org/project/torch-optimizer/0.3.0/","requires_dist":["torch (>=1.5.0)","pytorch-ranger (>=0.1.1)"],"requires_python":">=3.6.0","summary":"pytorch-optimizer","version":"0.3.0","yanked":false,"yanked_reason":null},"last_serial":11881319,"releases":{"0.0.1a0":[{"comment_text":"","digests":{"blake2b_256":"eb69a72a7f4560258b1dfe32fa813202e06fe2e9311df4b878680848f129e609","md5":"3d338a31dfac5f77d730883a37a99a68","sha256":"0d1778111fa9e1a54698ec0cd62c6689a7ae91d3688e50e91cb56865d1885996"},"downloads":-1,"filename":"torch-optimizer-0.0.1a0.tar.gz","has_sig":false,"md5_digest":"3d338a31dfac5f77d730883a37a99a68","packagetype":"sdist","python_version":"source","requires_python":null,"size":10119,"upload_time":"2020-01-05T01:36:30","upload_time_iso_8601":"2020-01-05T01:36:30.307831Z","url":"https://files.pythonhosted.org/packages/eb/69/a72a7f4560258b1dfe32fa813202e06fe2e9311df4b878680848f129e609/torch-optimizer-0.0.1a0.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a1":[{"comment_text":"","digests":{"blake2b_256":"73e4d490cd10deb415c4cb232690a0e4668ed53a399cb42c6bb57b12f0d5ce2c","md5":"f88cf06a8068397f41785780c3ebdfec","sha256":"ffab7bce904e618171a88fdb7614d9abf5e08db54df4a6cccc7960f6ecd49959"},"downloads":-1,"filename":"torch-optimizer-0.0.1a1.tar.gz","has_sig":false,"md5_digest":"f88cf06a8068397f41785780c3ebdfec","packagetype":"sdist","python_version":"source","requires_python":null,"size":16812,"upload_time":"2020-01-22T01:59:44","upload_time_iso_8601":"2020-01-22T01:59:44.336241Z","url":"https://files.pythonhosted.org/packages/73/e4/d490cd10deb415c4cb232690a0e4668ed53a399cb42c6bb57b12f0d5ce2c/torch-optimizer-0.0.1a1.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a10":[{"comment_text":"","digests":{"blake2b_256":"d7c862c4c2c350b0993dc88eef34ba5548983f4f6367f2e441727a0f60ab91cf","md5":"f9feffd605ea065fef4a5a1beff04427","sha256":"23e4800ca1e54fee5cd439b88e741eaf829bd52539b36f40ef4bf8c45b9841ee"},"downloads":-1,"filename":"torch_optimizer-0.0.1a10-py3-none-any.whl","has_sig":false,"md5_digest":"f9feffd605ea065fef4a5a1beff04427","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":30056,"upload_time":"2020-03-15T21:25:58","upload_time_iso_8601":"2020-03-15T21:25:58.652483Z","url":"https://files.pythonhosted.org/packages/d7/c8/62c4c2c350b0993dc88eef34ba5548983f4f6367f2e441727a0f60ab91cf/torch_optimizer-0.0.1a10-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"917024efab24e5f44e279cf9c3ffe0cbd65c2fc6d5dd7b0b5d72f9378f12b190","md5":"29916f14229aeb51be08ea24f80ee36f","sha256":"ae4a73870fcee05a60901e51a04bb58a1495be9ca792eee888bd73e943fd0e79"},"downloads":-1,"filename":"torch-optimizer-0.0.1a10.tar.gz","has_sig":false,"md5_digest":"29916f14229aeb51be08ea24f80ee36f","packagetype":"sdist","python_version":"source","requires_python":null,"size":26948,"upload_time":"2020-03-15T21:26:00","upload_time_iso_8601":"2020-03-15T21:26:00.644078Z","url":"https://files.pythonhosted.org/packages/91/70/24efab24e5f44e279cf9c3ffe0cbd65c2fc6d5dd7b0b5d72f9378f12b190/torch-optimizer-0.0.1a10.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a11":[{"comment_text":"","digests":{"blake2b_256":"388f8c9fdfb199a8f7e06a16c2315d076aa1505057f2496b7a7f2c73ece66215","md5":"fd9f491b7f7cea7822468eec386d3253","sha256":"14b767da66327eb188b5b08ecb3de6b5a2f84c43e0bf57c9f050df89782b7fa0"},"downloads":-1,"filename":"torch_optimizer-0.0.1a11-py3-none-any.whl","has_sig":false,"md5_digest":"fd9f491b7f7cea7822468eec386d3253","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":32961,"upload_time":"2020-04-05T18:47:18","upload_time_iso_8601":"2020-04-05T18:47:18.801321Z","url":"https://files.pythonhosted.org/packages/38/8f/8c9fdfb199a8f7e06a16c2315d076aa1505057f2496b7a7f2c73ece66215/torch_optimizer-0.0.1a11-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"6b79b4b0bb8eac2c192e93a04c9a5313354e22c38d93d4bb434970006e257c27","md5":"ed0ee74f32db5ff6f2c466f65715b4d5","sha256":"1f7d37f795a997634151807fa615785046621668b3410924da7ea342c36f93bc"},"downloads":-1,"filename":"torch-optimizer-0.0.1a11.tar.gz","has_sig":false,"md5_digest":"ed0ee74f32db5ff6f2c466f65715b4d5","packagetype":"sdist","python_version":"source","requires_python":null,"size":29470,"upload_time":"2020-04-05T18:47:20","upload_time_iso_8601":"2020-04-05T18:47:20.181406Z","url":"https://files.pythonhosted.org/packages/6b/79/b4b0bb8eac2c192e93a04c9a5313354e22c38d93d4bb434970006e257c27/torch-optimizer-0.0.1a11.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a12":[{"comment_text":"","digests":{"blake2b_256":"33d34ff0ce01ccbedf3a3c86e7882f428097a37a9b9a55eca73548e657da5518","md5":"6c27a9dce0197e399dd26a46036d7cd5","sha256":"3c8db4263407ccf8413e5a14e4dcb0f912c0a06081b9fa32c9e607390fee2169"},"downloads":-1,"filename":"torch_optimizer-0.0.1a12-py3-none-any.whl","has_sig":false,"md5_digest":"6c27a9dce0197e399dd26a46036d7cd5","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":33121,"upload_time":"2020-04-26T18:11:17","upload_time_iso_8601":"2020-04-26T18:11:17.355735Z","url":"https://files.pythonhosted.org/packages/33/d3/4ff0ce01ccbedf3a3c86e7882f428097a37a9b9a55eca73548e657da5518/torch_optimizer-0.0.1a12-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"948c7bd8fa05b00e9b1de63841cab0fb071366811422afa014a7487c343569c0","md5":"3bdfc896f8f2d6694e89ce2f42b85f2e","sha256":"5012c0162f0f45c74c9e7cd4c5be0a5b480b8d6fe54787117b1e9e1bee2e2196"},"downloads":-1,"filename":"torch-optimizer-0.0.1a12.tar.gz","has_sig":false,"md5_digest":"3bdfc896f8f2d6694e89ce2f42b85f2e","packagetype":"sdist","python_version":"source","requires_python":null,"size":29579,"upload_time":"2020-04-26T18:11:19","upload_time_iso_8601":"2020-04-26T18:11:19.075456Z","url":"https://files.pythonhosted.org/packages/94/8c/7bd8fa05b00e9b1de63841cab0fb071366811422afa014a7487c343569c0/torch-optimizer-0.0.1a12.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a13":[{"comment_text":"","digests":{"blake2b_256":"2a4035a2c9eed42fb32ffc41011eb7c92d03cfb8197a94ebb87619c0d63625b2","md5":"a1b1f4b6b91b39b5bfce99535dac2dd6","sha256":"11d0d2ed005738704777ac245b9e4b0170d8ee84d539a8d71252a206266d29b2"},"downloads":-1,"filename":"torch_optimizer-0.0.1a13-py3-none-any.whl","has_sig":false,"md5_digest":"a1b1f4b6b91b39b5bfce99535dac2dd6","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":35677,"upload_time":"2020-06-17T02:04:54","upload_time_iso_8601":"2020-06-17T02:04:54.075670Z","url":"https://files.pythonhosted.org/packages/2a/40/35a2c9eed42fb32ffc41011eb7c92d03cfb8197a94ebb87619c0d63625b2/torch_optimizer-0.0.1a13-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"9b557eec3d5a34b85badb3a6413466502f78386dd0539ec54e70f4336d6514f9","md5":"ddecfb048d402ca13939e3ea790664d2","sha256":"2be837111a095ed9a8eb33cc860dd4edaf0dcca196ec7a92a06fefa24ce47c57"},"downloads":-1,"filename":"torch-optimizer-0.0.1a13.tar.gz","has_sig":false,"md5_digest":"ddecfb048d402ca13939e3ea790664d2","packagetype":"sdist","python_version":"source","requires_python":null,"size":33485,"upload_time":"2020-06-17T02:04:55","upload_time_iso_8601":"2020-06-17T02:04:55.821203Z","url":"https://files.pythonhosted.org/packages/9b/55/7eec3d5a34b85badb3a6413466502f78386dd0539ec54e70f4336d6514f9/torch-optimizer-0.0.1a13.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a14":[{"comment_text":"","digests":{"blake2b_256":"101aa3f86e67c4f650916cb7d16849331bda302bf0b155f7c8240607cc97664b","md5":"61b8e8d989c413b71b784f8b586748ad","sha256":"fde9015dbbadcba3b285da3980f90d200be5b10e4c4ad33b75c70167ec53a0e9"},"downloads":-1,"filename":"torch_optimizer-0.0.1a14-py3-none-any.whl","has_sig":false,"md5_digest":"61b8e8d989c413b71b784f8b586748ad","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":40188,"upload_time":"2020-07-13T01:34:08","upload_time_iso_8601":"2020-07-13T01:34:08.766004Z","url":"https://files.pythonhosted.org/packages/10/1a/a3f86e67c4f650916cb7d16849331bda302bf0b155f7c8240607cc97664b/torch_optimizer-0.0.1a14-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"facf26ea80296424f55dfcd06d88cb41fe4d3e7b0543ff6ff1658290500acf3c","md5":"387a1bcd7a6b4fe031ce65eb3c54c7bc","sha256":"01163cf91ea6d802c5267ea6f340303d848e93565fc192b17f15b1f4f257542c"},"downloads":-1,"filename":"torch-optimizer-0.0.1a14.tar.gz","has_sig":false,"md5_digest":"387a1bcd7a6b4fe031ce65eb3c54c7bc","packagetype":"sdist","python_version":"source","requires_python":null,"size":36989,"upload_time":"2020-07-13T01:34:10","upload_time_iso_8601":"2020-07-13T01:34:10.530934Z","url":"https://files.pythonhosted.org/packages/fa/cf/26ea80296424f55dfcd06d88cb41fe4d3e7b0543ff6ff1658290500acf3c/torch-optimizer-0.0.1a14.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a15":[{"comment_text":"","digests":{"blake2b_256":"c948f670cf4b47c315861d0547f0c2be579cd801304c86e55008492f1acebd01","md5":"5aa18525fb72daa7a3019ad28633e226","sha256":"1afca8afa22fc93506329f08e2a358aadea9665ff03788377fca032835150a6c"},"downloads":-1,"filename":"torch_optimizer-0.0.1a15-py3-none-any.whl","has_sig":false,"md5_digest":"5aa18525fb72daa7a3019ad28633e226","packagetype":"bdist_wheel","python_version":"py3","requires_python":">3.5.0","size":41774,"upload_time":"2020-08-11T02:12:39","upload_time_iso_8601":"2020-08-11T02:12:39.989788Z","url":"https://files.pythonhosted.org/packages/c9/48/f670cf4b47c315861d0547f0c2be579cd801304c86e55008492f1acebd01/torch_optimizer-0.0.1a15-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"6d012a3cade021508b447be44cebdfb4e54cff39157c34914cf731ccccee3720","md5":"7c9555e49c31c87bf55e021b980a7e62","sha256":"5ca6d9632bb5a00d738aae93078ff257499e060345f482afb1216a336e394be7"},"downloads":-1,"filename":"torch-optimizer-0.0.1a15.tar.gz","has_sig":false,"md5_digest":"7c9555e49c31c87bf55e021b980a7e62","packagetype":"sdist","python_version":"source","requires_python":">3.5.0","size":37902,"upload_time":"2020-08-11T02:12:42","upload_time_iso_8601":"2020-08-11T02:12:42.473471Z","url":"https://files.pythonhosted.org/packages/6d/01/2a3cade021508b447be44cebdfb4e54cff39157c34914cf731ccccee3720/torch-optimizer-0.0.1a15.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a16":[{"comment_text":"","digests":{"blake2b_256":"231629a9a7f0403b491cef1af417c0d06f0f65cffd65660954476f887bf94da8","md5":"14ac7897de4beb1bcc23942f8de993dc","sha256":"d69451b79c47575d7cb99dd040fca3e7fecf96a8d7b675cc3c29170f58c6efb5"},"downloads":-1,"filename":"torch_optimizer-0.0.1a16-py3-none-any.whl","has_sig":false,"md5_digest":"14ac7897de4beb1bcc23942f8de993dc","packagetype":"bdist_wheel","python_version":"py3","requires_python":">3.5.0","size":51639,"upload_time":"2020-10-20T00:33:10","upload_time_iso_8601":"2020-10-20T00:33:10.146951Z","url":"https://files.pythonhosted.org/packages/23/16/29a9a7f0403b491cef1af417c0d06f0f65cffd65660954476f887bf94da8/torch_optimizer-0.0.1a16-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"9216bc2c10a68bb37ebb1e828cda51164e4eb4cdc62692be18ad84b35e1ee65d","md5":"edb70587c2547554d3264c100cfa4512","sha256":"a55fde7e26cfb088b9e2878a0a71b3d5dc3f025455d1f859860b37a2bce09f12"},"downloads":-1,"filename":"torch-optimizer-0.0.1a16.tar.gz","has_sig":false,"md5_digest":"edb70587c2547554d3264c100cfa4512","packagetype":"sdist","python_version":"source","requires_python":">3.5.0","size":44560,"upload_time":"2020-10-20T00:33:11","upload_time_iso_8601":"2020-10-20T00:33:11.955276Z","url":"https://files.pythonhosted.org/packages/92/16/bc2c10a68bb37ebb1e828cda51164e4eb4cdc62692be18ad84b35e1ee65d/torch-optimizer-0.0.1a16.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a17":[{"comment_text":"","digests":{"blake2b_256":"ce70ca0cd259662eef5c9448d3ecf14af880bbfe76331e4eeab7b19827d6dbe6","md5":"1ad093750eac5160a97ce76f87f0f79a","sha256":"728a4158a75c66273cc2962bbf617d60e82f623d933dc827a226624b39faf3cd"},"downloads":-1,"filename":"torch_optimizer-0.0.1a17-py3-none-any.whl","has_sig":false,"md5_digest":"1ad093750eac5160a97ce76f87f0f79a","packagetype":"bdist_wheel","python_version":"py3","requires_python":">3.5.0","size":69347,"upload_time":"2020-11-27T01:53:47","upload_time_iso_8601":"2020-11-27T01:53:47.020667Z","url":"https://files.pythonhosted.org/packages/ce/70/ca0cd259662eef5c9448d3ecf14af880bbfe76331e4eeab7b19827d6dbe6/torch_optimizer-0.0.1a17-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"5ec8c3ce7b328e41210fd18a8aa0846fc27687f9a4a44b4b088266228b45e28f","md5":"b4e5844f20d203898b88a0c680f754c4","sha256":"85eaa2e16ee109dddf7e3e6595b850738342514a975032ccba8de5e8e293ec0b"},"downloads":-1,"filename":"torch-optimizer-0.0.1a17.tar.gz","has_sig":false,"md5_digest":"b4e5844f20d203898b88a0c680f754c4","packagetype":"sdist","python_version":"source","requires_python":">3.5.0","size":14315468,"upload_time":"2020-11-27T01:53:49","upload_time_iso_8601":"2020-11-27T01:53:49.425252Z","url":"https://files.pythonhosted.org/packages/5e/c8/c3ce7b328e41210fd18a8aa0846fc27687f9a4a44b4b088266228b45e28f/torch-optimizer-0.0.1a17.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a2":[{"comment_text":"","digests":{"blake2b_256":"2d78d5237b1ed914768135e70f014b4c2a01b16176048d1fe1ae57db4701eaf7","md5":"1d652568f8f5c840d08297bd88a4d106","sha256":"bd164de32045e1aaa50e1f0d69c42521b29730c92abd16b10d46fd5f7ed8af5d"},"downloads":-1,"filename":"torch-optimizer-0.0.1a2.tar.gz","has_sig":false,"md5_digest":"1d652568f8f5c840d08297bd88a4d106","packagetype":"sdist","python_version":"source","requires_python":null,"size":18126,"upload_time":"2020-02-03T02:00:48","upload_time_iso_8601":"2020-02-03T02:00:48.904587Z","url":"https://files.pythonhosted.org/packages/2d/78/d5237b1ed914768135e70f014b4c2a01b16176048d1fe1ae57db4701eaf7/torch-optimizer-0.0.1a2.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a3":[{"comment_text":"","digests":{"blake2b_256":"51c42e3535a133b6041c9d5b2acc197ed83de91112844b727057797034c0c428","md5":"c4cfffcabca14eb608414457f8970bdb","sha256":"1289d3295dcbb61cb9ff030bfd89cb0eeae518cbaf9c0139173805c786a91039"},"downloads":-1,"filename":"torch-optimizer-0.0.1a3.tar.gz","has_sig":false,"md5_digest":"c4cfffcabca14eb608414457f8970bdb","packagetype":"sdist","python_version":"source","requires_python":null,"size":19669,"upload_time":"2020-02-09T03:42:57","upload_time_iso_8601":"2020-02-09T03:42:57.281686Z","url":"https://files.pythonhosted.org/packages/51/c4/2e3535a133b6041c9d5b2acc197ed83de91112844b727057797034c0c428/torch-optimizer-0.0.1a3.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a4":[{"comment_text":"","digests":{"blake2b_256":"63b523ad42b267c4c834cde6e8dd9b02f23f72f2d88b6c5952d97c585e346b33","md5":"66b76b26fbb5835ba95da9f36e477921","sha256":"839601e18e95383b5c277474f9a96349875064df00de79b50f00e6b8ba327f18"},"downloads":-1,"filename":"torch-optimizer-0.0.1a4.tar.gz","has_sig":false,"md5_digest":"66b76b26fbb5835ba95da9f36e477921","packagetype":"sdist","python_version":"source","requires_python":null,"size":19475,"upload_time":"2020-02-11T03:05:35","upload_time_iso_8601":"2020-02-11T03:05:35.683279Z","url":"https://files.pythonhosted.org/packages/63/b5/23ad42b267c4c834cde6e8dd9b02f23f72f2d88b6c5952d97c585e346b33/torch-optimizer-0.0.1a4.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a5":[{"comment_text":"","digests":{"blake2b_256":"a62245f1ebab7fd773370d9758fcb85bded3e3e4bd10159b948bed00d3bc4e35","md5":"b026a52fb17462585c699bd3e99f9640","sha256":"9b723b751cdd8849dc12ab486d20534c9a76dc5a57117d6c56955943e291af20"},"downloads":-1,"filename":"torch_optimizer-0.0.1a5-py3-none-any.whl","has_sig":false,"md5_digest":"b026a52fb17462585c699bd3e99f9640","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":21904,"upload_time":"2020-02-15T23:34:44","upload_time_iso_8601":"2020-02-15T23:34:44.583190Z","url":"https://files.pythonhosted.org/packages/a6/22/45f1ebab7fd773370d9758fcb85bded3e3e4bd10159b948bed00d3bc4e35/torch_optimizer-0.0.1a5-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"41bd64e4363c65c1b55d5e9c67d5225f6b26e171fa9d4625d3ed7c25385e3d50","md5":"822ada9b126a35265d898dd7be763349","sha256":"84cdc5150764c799dbf2a2b0558950c50871a85b93e7577e9efbc544d16b565b"},"downloads":-1,"filename":"torch-optimizer-0.0.1a5.tar.gz","has_sig":false,"md5_digest":"822ada9b126a35265d898dd7be763349","packagetype":"sdist","python_version":"source","requires_python":null,"size":20816,"upload_time":"2020-02-15T23:34:46","upload_time_iso_8601":"2020-02-15T23:34:46.295851Z","url":"https://files.pythonhosted.org/packages/41/bd/64e4363c65c1b55d5e9c67d5225f6b26e171fa9d4625d3ed7c25385e3d50/torch-optimizer-0.0.1a5.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a6":[{"comment_text":"","digests":{"blake2b_256":"5f82168a55e13ed8098c4dce4507a53bd20031a1ef8be261ac793758902e19bd","md5":"669088b858e19a6e91e523848c05421b","sha256":"9afdd966c7177699c2937013949fe5f08f3efb65bd08d59a3fdc20e7f041f891"},"downloads":-1,"filename":"torch_optimizer-0.0.1a6-py3-none-any.whl","has_sig":false,"md5_digest":"669088b858e19a6e91e523848c05421b","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":23963,"upload_time":"2020-02-22T03:50:18","upload_time_iso_8601":"2020-02-22T03:50:18.337031Z","url":"https://files.pythonhosted.org/packages/5f/82/168a55e13ed8098c4dce4507a53bd20031a1ef8be261ac793758902e19bd/torch_optimizer-0.0.1a6-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"1a31d630f2c051324c5088021333541ac9095f42d13a81a0fbfb0eb07762a44e","md5":"c430e6ff0faa2f95b6edcbef888ec686","sha256":"2b3a6bc1947983977c84001714291de43a9203648f54124275d72b78071c8671"},"downloads":-1,"filename":"torch-optimizer-0.0.1a6.tar.gz","has_sig":false,"md5_digest":"c430e6ff0faa2f95b6edcbef888ec686","packagetype":"sdist","python_version":"source","requires_python":null,"size":21653,"upload_time":"2020-02-22T03:50:19","upload_time_iso_8601":"2020-02-22T03:50:19.718910Z","url":"https://files.pythonhosted.org/packages/1a/31/d630f2c051324c5088021333541ac9095f42d13a81a0fbfb0eb07762a44e/torch-optimizer-0.0.1a6.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a7":[{"comment_text":"","digests":{"blake2b_256":"54cc7c390224710db1c4a3a32d8c42ae2bf69d826c81b566deeca7cec8cf69b6","md5":"8490bfcbc700a2b504f7ecefccaed5ce","sha256":"7a14470fb7ffd780df48b42ad3bd3e3743c506febc861dd6054deddf9434c046"},"downloads":-1,"filename":"torch_optimizer-0.0.1a7-py3-none-any.whl","has_sig":false,"md5_digest":"8490bfcbc700a2b504f7ecefccaed5ce","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":25636,"upload_time":"2020-02-27T00:33:47","upload_time_iso_8601":"2020-02-27T00:33:47.692423Z","url":"https://files.pythonhosted.org/packages/54/cc/7c390224710db1c4a3a32d8c42ae2bf69d826c81b566deeca7cec8cf69b6/torch_optimizer-0.0.1a7-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"97eee39de19ddc9f9747e3165fc17c765c76c3354db5dcf9394743b8b48e3423","md5":"708645bdd21958b4ea21c43165fe11b2","sha256":"e08998303d385efb74a4f16fe02308a2f3d9ebbf63be66056ef2d0074686f1db"},"downloads":-1,"filename":"torch-optimizer-0.0.1a7.tar.gz","has_sig":false,"md5_digest":"708645bdd21958b4ea21c43165fe11b2","packagetype":"sdist","python_version":"source","requires_python":null,"size":22908,"upload_time":"2020-02-27T00:33:49","upload_time_iso_8601":"2020-02-27T00:33:49.486410Z","url":"https://files.pythonhosted.org/packages/97/ee/e39de19ddc9f9747e3165fc17c765c76c3354db5dcf9394743b8b48e3423/torch-optimizer-0.0.1a7.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a8":[{"comment_text":"","digests":{"blake2b_256":"d784822966979c1fcc95272f46f62b20d09bc81295c17ef6d456ded5b8baeccd","md5":"49d52957238733912b072227b69e2900","sha256":"6fa5bf2c08b16c1e55391417f29bf3d620d8d93a285d1ca019ac821168099096"},"downloads":-1,"filename":"torch_optimizer-0.0.1a8-py3-none-any.whl","has_sig":false,"md5_digest":"49d52957238733912b072227b69e2900","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":26168,"upload_time":"2020-03-02T02:10:41","upload_time_iso_8601":"2020-03-02T02:10:41.219476Z","url":"https://files.pythonhosted.org/packages/d7/84/822966979c1fcc95272f46f62b20d09bc81295c17ef6d456ded5b8baeccd/torch_optimizer-0.0.1a8-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"1dee0cec414f5a4bba1a8a2deb1acc14c89dd47d19a3b0ca4a80bdc16ca2e404","md5":"788cf4d715b49b212f4153a1f821ca02","sha256":"429e006a90a97627f57eb4625dd39ab137b737e16297ffde7adef29bfb4750d8"},"downloads":-1,"filename":"torch-optimizer-0.0.1a8.tar.gz","has_sig":false,"md5_digest":"788cf4d715b49b212f4153a1f821ca02","packagetype":"sdist","python_version":"source","requires_python":null,"size":24334,"upload_time":"2020-03-02T02:10:42","upload_time_iso_8601":"2020-03-02T02:10:42.880115Z","url":"https://files.pythonhosted.org/packages/1d/ee/0cec414f5a4bba1a8a2deb1acc14c89dd47d19a3b0ca4a80bdc16ca2e404/torch-optimizer-0.0.1a8.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1a9":[{"comment_text":"","digests":{"blake2b_256":"74e323bec2c68a74820505d15c43c4b858ba7d8845fe101cef04b16334484633","md5":"85263c6b0bbf64934deca7180c1807c0","sha256":"ca1f1e2625dc0ffacd18c0b891af7b9440391bcda00051d7f0b40d8a525e34ed"},"downloads":-1,"filename":"torch_optimizer-0.0.1a9-py3-none-any.whl","has_sig":false,"md5_digest":"85263c6b0bbf64934deca7180c1807c0","packagetype":"bdist_wheel","python_version":"py3","requires_python":null,"size":27800,"upload_time":"2020-03-04T03:02:52","upload_time_iso_8601":"2020-03-04T03:02:52.562636Z","url":"https://files.pythonhosted.org/packages/74/e3/23bec2c68a74820505d15c43c4b858ba7d8845fe101cef04b16334484633/torch_optimizer-0.0.1a9-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"307bfd6721d6c38b9dd4bf2636e023d9177ee73d29e77685f997e9f415def4c9","md5":"d2e130565201f0326dae96050aadb125","sha256":"d036edab9d76f2b983986b97ab6287b50e8209a1f0b95f1e1021f77396269e78"},"downloads":-1,"filename":"torch-optimizer-0.0.1a9.tar.gz","has_sig":false,"md5_digest":"d2e130565201f0326dae96050aadb125","packagetype":"sdist","python_version":"source","requires_python":null,"size":25234,"upload_time":"2020-03-04T03:02:54","upload_time_iso_8601":"2020-03-04T03:02:54.209417Z","url":"https://files.pythonhosted.org/packages/30/7b/fd6721d6c38b9dd4bf2636e023d9177ee73d29e77685f997e9f415def4c9/torch-optimizer-0.0.1a9.tar.gz","yanked":false,"yanked_reason":null}],"0.1.0":[{"comment_text":"","digests":{"blake2b_256":"af0fbc49a0f714a1896b80f31db9ba82eebcb2bad9e0f5757184574f8ecfe2f1","md5":"1516aa599bcc11fb390023701f7e0962","sha256":"b7adaed38b66a5c5105a59b30a71c4ab7c9954baf0acabd969fee3dac954657d"},"downloads":-1,"filename":"torch_optimizer-0.1.0-py3-none-any.whl","has_sig":false,"md5_digest":"1516aa599bcc11fb390023701f7e0962","packagetype":"bdist_wheel","python_version":"3.7","requires_python":null,"size":72545,"upload_time":"2021-01-01T16:51:15","upload_time_iso_8601":"2021-01-01T16:51:15.199408Z","url":"https://files.pythonhosted.org/packages/af/0f/bc49a0f714a1896b80f31db9ba82eebcb2bad9e0f5757184574f8ecfe2f1/torch_optimizer-0.1.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"768712f8646a46018d8c84cc74456f82aa263f55eb93b6f96c195f85cd0d2f72","md5":"a1c68c175f941ca5445fbea758174dc8","sha256":"3f67b71838a23a149afa7b3e1d9d7d3fd7c83b5758c8f3744900de1584969cf5"},"downloads":-1,"filename":"torch-optimizer-0.1.0.tar.gz","has_sig":false,"md5_digest":"a1c68c175f941ca5445fbea758174dc8","packagetype":"sdist","python_version":"source","requires_python":null,"size":14318377,"upload_time":"2021-01-01T16:51:32","upload_time_iso_8601":"2021-01-01T16:51:32.121536Z","url":"https://files.pythonhosted.org/packages/76/87/12f8646a46018d8c84cc74456f82aa263f55eb93b6f96c195f85cd0d2f72/torch-optimizer-0.1.0.tar.gz","yanked":false,"yanked_reason":null}],"0.2.0":[{"comment_text":"","digests":{"blake2b_256":"389fdfa4a804a1a5f0d90adc4017e8b48258ce4b2263de4861ed81b9cd04b073","md5":"072126da92d2702aa06611061f35d2fe","sha256":"2a19bd285169c4cc9f53e843ec507bcb7e923eaa5ce55da7b1d09715cbdc5bd5"},"downloads":-1,"filename":"torch_optimizer-0.2.0-py3-none-any.whl","has_sig":false,"md5_digest":"072126da92d2702aa06611061f35d2fe","packagetype":"bdist_wheel","python_version":"3.8","requires_python":null,"size":68815,"upload_time":"2021-10-26T01:33:28","upload_time_iso_8601":"2021-10-26T01:33:28.144863Z","url":"https://files.pythonhosted.org/packages/38/9f/dfa4a804a1a5f0d90adc4017e8b48258ce4b2263de4861ed81b9cd04b073/torch_optimizer-0.2.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"ed9ef1f6f804c775a1e2bcdc1cbc562b219dbffa8a6dda5676a7cf765f89c054","md5":"b649b09590fdda71274eb2411774c919","sha256":"c338c6bfcd111e36875e55b5988125a82ab86fba6fe8ba2adcef9d1698d24b59"},"downloads":-1,"filename":"torch-optimizer-0.2.0.tar.gz","has_sig":false,"md5_digest":"b649b09590fdda71274eb2411774c919","packagetype":"sdist","python_version":"source","requires_python":null,"size":13132684,"upload_time":"2021-10-26T01:33:25","upload_time_iso_8601":"2021-10-26T01:33:25.206762Z","url":"https://files.pythonhosted.org/packages/ed/9e/f1f6f804c775a1e2bcdc1cbc562b219dbffa8a6dda5676a7cf765f89c054/torch-optimizer-0.2.0.tar.gz","yanked":false,"yanked_reason":null}],"0.3.0":[{"comment_text":"","digests":{"blake2b_256":"f654bbb1b4c15afc2dac525c8359c340ade685542113394fd4c6564ee3c71da3","md5":"43e2ffdf564e56a782df458c84c586e5","sha256":"7de8e57315e43561cdd0370a1b67303cc8ef1b053f9b5573de629a62390f2af9"},"downloads":-1,"filename":"torch_optimizer-0.3.0-py3-none-any.whl","has_sig":false,"md5_digest":"43e2ffdf564e56a782df458c84c586e5","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.6.0","size":61897,"upload_time":"2021-10-31T03:00:19","upload_time_iso_8601":"2021-10-31T03:00:19.812935Z","url":"https://files.pythonhosted.org/packages/f6/54/bbb1b4c15afc2dac525c8359c340ade685542113394fd4c6564ee3c71da3/torch_optimizer-0.3.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"1813c4c0a206131e978d8ceaa095ad1e3153d7daf48efad207b6057efe3491a2","md5":"8d1d81b4266c3f77017a33c1bdecda82","sha256":"b2180629df9d6cd7a2aeabe71fa4a872bba938e8e275965092568cd9931b924c"},"downloads":-1,"filename":"torch-optimizer-0.3.0.tar.gz","has_sig":false,"md5_digest":"8d1d81b4266c3f77017a33c1bdecda82","packagetype":"sdist","python_version":"source","requires_python":">=3.6.0","size":54409,"upload_time":"2021-10-31T03:00:22","upload_time_iso_8601":"2021-10-31T03:00:22.084776Z","url":"https://files.pythonhosted.org/packages/18/13/c4c0a206131e978d8ceaa095ad1e3153d7daf48efad207b6057efe3491a2/torch-optimizer-0.3.0.tar.gz","yanked":false,"yanked_reason":null}]},"urls":[{"comment_text":"","digests":{"blake2b_256":"f654bbb1b4c15afc2dac525c8359c340ade685542113394fd4c6564ee3c71da3","md5":"43e2ffdf564e56a782df458c84c586e5","sha256":"7de8e57315e43561cdd0370a1b67303cc8ef1b053f9b5573de629a62390f2af9"},"downloads":-1,"filename":"torch_optimizer-0.3.0-py3-none-any.whl","has_sig":false,"md5_digest":"43e2ffdf564e56a782df458c84c586e5","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.6.0","size":61897,"upload_time":"2021-10-31T03:00:19","upload_time_iso_8601":"2021-10-31T03:00:19.812935Z","url":"https://files.pythonhosted.org/packages/f6/54/bbb1b4c15afc2dac525c8359c340ade685542113394fd4c6564ee3c71da3/torch_optimizer-0.3.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"1813c4c0a206131e978d8ceaa095ad1e3153d7daf48efad207b6057efe3491a2","md5":"8d1d81b4266c3f77017a33c1bdecda82","sha256":"b2180629df9d6cd7a2aeabe71fa4a872bba938e8e275965092568cd9931b924c"},"downloads":-1,"filename":"torch-optimizer-0.3.0.tar.gz","has_sig":false,"md5_digest":"8d1d81b4266c3f77017a33c1bdecda82","packagetype":"sdist","python_version":"source","requires_python":">=3.6.0","size":54409,"upload_time":"2021-10-31T03:00:22","upload_time_iso_8601":"2021-10-31T03:00:22.084776Z","url":"https://files.pythonhosted.org/packages/18/13/c4c0a206131e978d8ceaa095ad1e3153d7daf48efad207b6057efe3491a2/torch-optimizer-0.3.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}
