{"info":{"author":"Nikita Kitaev","author_email":"kitaev@cs.berkeley.edu","bugtrack_url":null,"classifiers":["Intended Audience :: Developers","Intended Audience :: Education","Intended Audience :: Science/Research","License :: OSI Approved :: Apache Software License","Operating System :: OS Independent","Programming Language :: Python :: 3","Programming Language :: Python :: 3.6","Programming Language :: Python :: 3.7","Programming Language :: Python :: 3.8","Programming Language :: Python :: 3.9","Topic :: Scientific/Engineering :: Artificial Intelligence","Topic :: Text Processing :: Linguistic"],"description":"# Berkeley Neural Parser\n\nA high-accuracy parser with models for 11 languages, implemented in Python. Based on [Constituency Parsing with a Self-Attentive Encoder](https://arxiv.org/abs/1805.01052) from ACL 2018, with additional changes described in [Multilingual Constituency Parsing with Self-Attention and Pre-Training](https://arxiv.org/abs/1812.11760).\n\n**New February 2021:** Version 0.2.0 of the Berkeley Neural Parser is now out, with higher-quality pre-trained models for all languages. Inference now uses PyTorch instead of TensorFlow (training has always been PyTorch-only). Drops support for Python 2.7 and 3.5. Includes updated support for training and using your own parsers, based on your choice of [pre-trained model](https://huggingface.co/models).\n\n## Contents\n1. [Installation](#installation)\n2. [Usage](#usage)\n3. [Available Models](#available-models)\n4. [Training](#training)\n5. [Reproducing Experiments](#reproducing-experiments)\n6. [Citation](#citation)\n7. [Credits](#credits)\n\nIf you are primarily interested in training your own parsing models, skip to the [Training](#training) section of this README.\n\n## Installation\n\nTo install the parser, run the command:\n```bash\n$ pip install benepar\n```\n*Note: benepar 0.2.0 is a major upgrade over the previous version, and comes with entirely new and higher-quality parser models. If you are not ready to upgrade, you can pin your benepar version to [the previous release (0.1.3)](https://github.com/nikitakit/self-attentive-parser/tree/acl2019).*\n\nPython 3.6 (or newer) and [PyTorch](https://pytorch.org/) 1.6 (or newer) are required. See the PyTorch website for instruction on how to select between GPU-enabled and CPU-only versions of PyTorch; benepar will automatically use the GPU if it is available to pytorch.\n\nThe recommended way of using benepar is through integration with [spaCy](https://spacy.io/). If using spaCy, you should install a spaCy model for your language. For English, the installation command is:\n```sh\n$ python -m spacy download en_core_web_md\n```\n\nThe spaCy model is only used for tokenization and sentence segmentation. If language-specific analysis beyond parsing is not required, you may also forego a language-specific model and instead use a multi-language model that only performs tokenization and segmentation. [One such model](https://spacy.io/models/xx#xx_sent_ud_sm), newly added in spaCy 3.0, should work for English, German, Korean, Polish, and Swedish (but not Chinese, since it doesn't seem to support Chinese word segmentation).\n\nParsing models need to be downloaded separately, using the commands:\n```python\n>>> import benepar\n>>> benepar.download('benepar_en3')\n```\n\nSee the [Available Models](#available-models) section below for a full list of models.\n\n## Usage\n\n### Usage with spaCy (recommended)\n\nThe recommended way of using benepar is through its integration with spaCy:\n```python\n>>> import benepar, spacy\n>>> nlp = spacy.load('en_core_web_md')\n>>> if spacy.__version__.startswith('2'):\n        nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n    else:\n        nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n>>> doc = nlp(\"The time for action is now. It's never too late to do something.\")\n>>> sent = list(doc.sents)[0]\n>>> print(sent._.parse_string)\n(S (NP (NP (DT The) (NN time)) (PP (IN for) (NP (NN action)))) (VP (VBZ is) (ADVP (RB now))) (. .))\n>>> sent._.labels\n('S',)\n>>> list(sent._.children)[0]\nThe time for action\n```\n\nSince spaCy does not provide an official constituency parsing API, all methods are accessible through the extension namespaces `Span._` and `Token._`.\n\nThe following extension properties are available:\n- `Span._.labels`: a tuple of labels for the given span. A span may have multiple labels when there are unary chains in the parse tree.\n- `Span._.parse_string`: a string representation of the parse tree for a given span.\n- `Span._.constituents`: an iterator over `Span` objects for sub-constituents in a pre-order traversal of the parse tree.\n- `Span._.parent`: the parent `Span` in the parse tree.\n- `Span._.children`: an iterator over child `Span`s in the parse tree.\n- `Token._.labels`, `Token._.parse_string`, `Token._.parent`: these behave the same as calling the corresponding method on the length-one Span containing the token.\n\nThese methods will raise an exception when called on a span that is not a constituent in the parse tree. Such errors can be avoided by traversing the parse tree starting at either sentence level (by iterating over `doc.sents`) or with an individual `Token` object.\n\n### Usage with NLTK\n\nThere is also an NLTK interface, which is designed for use with pre-tokenized datasets and treebanks, or when integrating the parser into an NLP pipeline that already performs (at minimum) tokenization and sentence splitting. For parsing starting with raw text, it is **strongly encouraged** that you use spaCy and `benepar.BeneparComponent` instead.\n\nSample usage with NLTK:\n```python\n>>> import benepar\n>>> parser = benepar.Parser(\"benepar_en3\")\n>>> input_sentence = benepar.InputSentence(\n    words=['\"', 'Fly', 'safely', '.', '\"'],\n    space_after=[False, True, False, False, False],\n    tags=['``', 'VB', 'RB', '.', \"''\"],\n    escaped_words=['``', 'Fly', 'safely', '.', \"''\"],\n)\n>>> tree = parser.parse(input_sentence)\n>>> print(tree)\n(TOP (S (`` ``) (VP (VB Fly) (ADVP (RB safely))) (. .) ('' '')))\n```\n\nNot all fields of `benepar.InputSentence` are required, but at least one of `words` and `escaped_words` must be specified. The parser will attempt to guess the value for missing fields, for example:\n```python\n>>> input_sentence = benepar.InputSentence(\n    words=['\"', 'Fly', 'safely', '.', '\"'],\n)\n>>> parser.parse(input_sentence)\n```\n\nUse `parse_sents` to parse multiple sentences.\n```python\n>>> input_sentence1 = benepar.InputSentence(\n    words=['The', 'time', 'for', 'action', 'is', 'now', '.'],\n)\n>>> input_sentence2 = benepar.InputSentence(\n    words=['It', \"'s\", 'never', 'too', 'late', 'to', 'do', 'something', '.'],\n)\n>>> parser.parse_sents([input_sentence1, input_sentence2])\n```\n\nSome parser models also allow Unicode text input for debugging/interactive use, but passing in raw text strings is *strongly discouraged* for any application where parsing accuracy matters.\n```python\n>>> parser.parse('\"Fly safely.\"')  # For debugging/interactive use only.\n```\nWhen parsing from raw text, we recommend using spaCy and `benepar.BeneparComponent` instead. The reason is that parser models do not ship with a tokenizer or sentence splitter, and some models may not include a part-of-speech tagger either. A toolkit must be used to fill in these pipeline components, and spaCy outperforms NLTK in all of these areas (sometimes by a large margin). \n\n\n\n## Available Models\n\nThe following trained parser models are available. To use spaCy integration, you will also need to install a [spaCy model for the appropriate language](https://spacy.io/models).\n\nModel       | Language | Info\n----------- | -------- | ----\n`benepar_en3` | English | 95.40 F1 on [revised](https://catalog.ldc.upenn.edu/LDC2015T13) WSJ test set. The training data uses revised tokenization and syntactic annotation based on the same guidelines as the English Web Treebank and OntoNotes, which better matches modern tokenization practices in libraries like spaCy. Based on T5-small.\n`benepar_en3_large` | English | 96.29 F1 on [revised](https://catalog.ldc.upenn.edu/LDC2015T13) WSJ test set. The training data uses revised tokenization and syntactic annotation based on the same guidelines as the English Web Treebank and OntoNotes, which better matches modern tokenization practices in libraries like spaCy. Based on T5-large.\n`benepar_zh2` | Chinese | 92.56 F1 on CTB 5.1 test set. Usage with spaCy allows supports parsing from raw text, but the NLTK API only supports parsing previously tokenized sentences. Based on Chinese ELECTRA-180G-large.\n`benepar_ar` | Arabic | 90.52 F1 on SPMRL2013/2014 test set. Only supports using the NLTK API for parsing previously tokenized sentences. Parsing from raw text and spaCy integration are not supported. Based on XLM-R.\n`benepar_de` | German | 92.10 F1 on SPMRL2013/2014 test set. Based on XLM-R.\n`benepar_eu` | Basque | 93.36 F1 on SPMRL2013/2014 test set. Usage with spaCy first requires implementing Basque support in spaCy. Based on XLM-R.\n`benepar_fr` | French | 88.43 F1 on SPMRL2013/2014 test set. Based on XLM-R.\n`benepar_he` | Hebrew | 93.98 F1 on SPMRL2013/2014 test set. Only supports using the NLTK API for parsing previously tokenized sentences. Parsing from raw text and spaCy integration are not supported. Based on XLM-R.\n`benepar_hu` | Hungarian | 96.19 F1 on SPMRL2013/2014 test set. Usage with spaCy requires a [Hungarian model for spaCy](https://github.com/oroszgy/spacy-hungarian-models). The NLTK API only supports parsing previously tokenized sentences. Based on XLM-R.\n`benepar_ko` | Korean | 91.72 F1 on SPMRL2013/2014 test set. Can be used with spaCy's [multi-language sentence segmentation model](https://spacy.io/models/xx#xx_sent_ud_sm) (requires spaCy v3.0). The NLTK API only supports parsing previously tokenized sentences. Based on XLM-R.\n`benepar_pl` | Polish | 97.15 F1 on SPMRL2013/2014 test set. Based on XLM-R.\n`benepar_sv` | Swedish | 92.21 F1 on SPMRL2013/2014 test set. Can be used with spaCy's [multi-language sentence segmentation model](https://spacy.io/models/xx#xx_sent_ud_sm) (requires spaCy v3.0). Based on XLM-R.\n`benepar_en3_wsj` | English | **Consider using `benepar_en3` or `benepar_en3_large` instead**. 95.55 F1 on [canonical](https://catalog.ldc.upenn.edu/LDC99T42) WSJ test set used for decades of English constituency parsing publications. Based on BERT-large-uncased. We believe that the revised annotation guidelines used for training `benepar_en3`/`benepar_en3_large` are more suitable for downstream use because they better handle language usage in web text, and are more consistent with modern practices in dependency parsing and libraries like spaCy. Nevertheless, we provide the `benepar_en3_wsj` model for cases where using the revised treebanking conventions are not appropriate, such as benchmarking different models on the same dataset.\n\n## Training\n\nTraining requires cloning this repository from GitHub. While the model code in `src/benepar` is distributed in the `benepar` package on PyPI, the training and evaluation scripts directly under `src/` are not.\n\n#### Software Requirements for Training\n* Python 3.7 or higher.\n* [PyTorch](http://pytorch.org/) 1.6.0, or any compatible version.\n* All dependencies required by the `benepar` package, including: [NLTK](https://www.nltk.org/) 3.2, [torch-struct](https://github.com/harvardnlp/pytorch-struct) 0.4, [transformers](https://github.com/huggingface/transformers) 4.3.0, or compatible.\n* [pytokenizations](https://github.com/tamuhey/tokenizations/) 0.7.2 or compatible.\n* [EVALB](http://nlp.cs.nyu.edu/evalb/). Before starting, run `make` inside the `EVALB/` directory to compile an `evalb` executable. This will be called from Python for evaluation. If training on the SPMRL datasets, you will need to run `make` inside the `EVALB_SPMRL/` directory instead.\n\n### Training Instructions\n\nA new model can be trained using the command `python src/main.py train ...`. Some of the available arguments are:\n\nArgument | Description | Default\n--- | --- | ---\n`--model-path-base` | Path base to use for saving models | N/A\n`--evalb-dir` |  Path to EVALB directory | `EVALB/`\n`--train-path` | Path to training trees | `data/wsj/train_02-21.LDC99T42`\n`--train-path-text` | Optional non-destructive tokenization of the training data | Guess raw text; see `--text-processing`\n`--dev-path` | Path to development trees | `data/wsj/dev_22.LDC99T42`\n`--dev-path-text` | Optional non-destructive tokenization of the development data | Guess raw text; see `--text-processing`\n`--text-processing` | Heuristics for guessing raw text from descructively tokenized tree files. See `load_trees()` in `src/treebanks.py` | Default rules for languages other than Arabic, Chinese, and Hebrew\n`--subbatch-max-tokens` | Maximum number of tokens to process in parallel while training (a full batch may not fit in GPU memory) | 2000\n`--parallelize` | Distribute pre-trained model (e.g. T5) layers across multiple GPUs. | Use at most one GPU\n`--batch-size` | Number of examples per training update | 32\n`--checks-per-epoch` | Number of development evaluations per epoch | 4\n`--numpy-seed` | NumPy random seed | Random\n`--use-pretrained` | Use pre-trained encoder | Do not use pre-trained encoder\n`--pretrained-model` | Model to use if `--use-pretrained` is passed. May be a path or a model id from the [HuggingFace Model Hub](https://huggingface.co/models)| `bert-base-uncased`\n`--predict-tags` | Adds a part-of-speech tagging component and auxiliary loss to the parser | Do not predict tags\n`--use-chars-lstm` | Use learned CharLSTM word representations | Do not use CharLSTM\n`--use-encoder` | Use learned transformer layers on top of pre-trained model or CharLSTM | Do not use extra transformer layers\n`--num-layers` | Number of transformer layers to use if `--use-encoder` is passed | 8\n`--encoder-max-len` | Maximum sentence length (in words) allowed for extra transformer layers | 512\n\nAdditional arguments are available for other hyperparameters; see `make_hparams()` in `src/main.py`. These can be specified on the command line, such as `--num-layers 2` (for numerical parameters), `--predict-tags` (for boolean parameters that default to False), or `--no-XXX` (for boolean parameters that default to True).\n\nFor each development evaluation, the F-score on the development set is computed and compared to the previous best. If the current model is better, the previous model will be deleted and the current model will be saved. The new filename will be derived from the provided model path base and the development F-score.\n\nPrior to training the parser, you will first need to obtain appropriate training data. We provide [instructions on how to process standard datasets like PTB, CTB, and the SMPRL 2013/2014 Shared Task data](data/README.md). After following the instructions for the English WSJ data, you can use the following command to train an English parser using the default hyperparameters:\n\n```\npython src/main.py train --use-pretrained --model-path-base models/en_bert_base\n```\n\nSee [`EXPERIMENTS.md`](EXPERIMENTS.md) for more examples of good hyperparameter choices.\n\n### Evaluation Instructions\n\nA saved model can be evaluated on a test corpus using the command `python src/main.py test ...` with the following arguments:\n\nArgument | Description | Default\n--- | --- | ---\n`--model-path` | Path of saved model | N/A\n`--evalb-dir` |  Path to EVALB directory | `EVALB/`\n`--test-path` | Path to test trees | `data/23.auto.clean`\n`--test-path-text` | Optional non-destructive tokenization of the test data | Guess raw text; see `--text-processing`\n`--text-processing` | Heuristics for guessing raw text from descructively tokenized tree files. See `load_trees()` in `src/treebanks.py` | Default rules for languages other than Arabic, Chinese, and Hebrew\n`--test-path-raw` | Alternative path to test trees that is used for evalb only (used to double-check that evaluation against pre-processed trees does not contain any bugs) | Compare to trees from `--test-path`\n`--subbatch-max-tokens` | Maximum number of tokens to process in parallel (a GPU does not have enough memory to process the full dataset in one batch) | 500\n`--parallelize` | Distribute pre-trained model (e.g. T5) layers across multiple GPUs. | Use at most one GPU\n`--output-path` | Path to write predicted trees to (use `\"-\"` for stdout). | Do not save predicted trees\n`--no-predict-tags` | Use gold part-of-speech tags when running EVALB. This is the standard for publications, and omitting this flag may give erroneously high F1 scores. | Use predicted part-of-speech tags for EVALB, if available\n\nAs an example, you can evaluate a trained model using the following command:\n```\npython src/main.py test --model-path models/en_bert_base_dev=*.pt\n```\n\n### Exporting Models for Inference\n\nThe `benepar` package can directly use saved checkpoints by replacing a model name like `benepar_en3` with a path such as `models/en_bert_base_dev_dev=95.67.pt`. However, releasing the single-file checkpoints has a few shortcomings:\n* Single-file checkpoints do not include the tokenizer or pre-trained model config. These can generally be downloaded automatically from the HuggingFace model hub, but this requires an Internet connection and may also (incidentally and unnecessarily) download pre-trained weights from the HuggingFace Model Hub\n* Single-file checkpoints are 3x larger than necessary, because they save optimizer state\n\nUse `src/export.py` to convert a checkpoint file into a directory that encapsulates everything about a trained model. For example,\n```\npython src/export.py export \\\n  --model-path models/en_bert_base_dev=*.pt \\\n  --output-dir=models/en_bert_base\n```\n\nWhen exporting, there is also a `--compress` option that slightly adjusts model weights, so that the output directory can be compressed into a ZIP archive of much smaller size. We use this for our official model releases, because it's a hassle to distribute model weights that are 2GB+ in size. When using the `--compress` option, it is recommended to specify a test set in order to verify that compression indeed has minimal impact on parsing accuracy. Using the development data for verification is not recommended, since the development data was already used for the model selection criterion during training.\n```\npython src/export.py export \\\n  --model-path models/en_bert_base_dev=*.pt \\\n  --output-dir=models/en_bert_base \\\n  --test-path=data/wsj/test_23.LDC99T42\n```\n\nThe `src/export.py` script also has a `test` subcommand that's roughly similar to `python src/main.py test`, except that it supports exported models and has slightly different flags. We can run the following command to verify that our English parser using BERT-large-uncased indeed achieves 95.55 F1 on the canonical WSJ test set:\n```\npython src/export.py test --model-path benepar_en3_wsj --test-path data/wsj/test_23.LDC99T42\n```\n\n## Reproducing Experiments\n\nSee [`EXPERIMENTS.md`](EXPERIMENTS.md) for instructions on how to reproduce experiments reported in our ACL 2018 and 2019 papers.\n\n## Citation\n\nIf you use this software for research, please cite our papers as follows:\n\n```\n@inproceedings{kitaev-etal-2019-multilingual,\n    title = \"Multilingual Constituency Parsing with Self-Attention and Pre-Training\",\n    author = \"Kitaev, Nikita  and\n      Cao, Steven  and\n      Klein, Dan\",\n    booktitle = \"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/P19-1340\",\n    doi = \"10.18653/v1/P19-1340\",\n    pages = \"3499--3505\",\n}\n\n@inproceedings{kitaev-klein-2018-constituency,\n    title = \"Constituency Parsing with a Self-Attentive Encoder\",\n    author = \"Kitaev, Nikita  and\n      Klein, Dan\",\n    booktitle = \"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2018\",\n    address = \"Melbourne, Australia\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/P18-1249\",\n    doi = \"10.18653/v1/P18-1249\",\n    pages = \"2676--2686\",\n}\n```\n\n## Credits\n\nThe code in this repository and portions of this README are based on https://github.com/mitchellstern/minimal-span-parser","description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"https://github.com/nikitakit/self-attentive-parser","keywords":"","license":"","maintainer":"","maintainer_email":"","name":"benepar","package_url":"https://pypi.org/project/benepar/","platform":"","project_url":"https://pypi.org/project/benepar/","project_urls":{"Homepage":"https://github.com/nikitakit/self-attentive-parser"},"provides_extra":null,"release_url":"https://pypi.org/project/benepar/0.2.0/","requires_dist":null,"requires_python":">=3.6","summary":"Berkeley Neural Parser","version":"0.2.0","yanked":false,"yanked_reason":null},"last_serial":9411529,"releases":{"0.0.1":[{"comment_text":"","digests":{"blake2b_256":"294f2a975c72f7da21e49576a2fe5974b244c7ed7dd5175dd007d7e532708808","md5":"97874371f433ba2eda89c8fe94d5a70d","sha256":"e382e146e7e792de4d1c6bcefc9e486f71430737fa2b2a188e4d669a00661175"},"downloads":-1,"filename":"benepar-0.0.1.tar.gz","has_sig":false,"md5_digest":"97874371f433ba2eda89c8fe94d5a70d","packagetype":"sdist","python_version":"source","requires_python":null,"size":64499,"upload_time":"2018-05-29T00:04:41","upload_time_iso_8601":"2018-05-29T00:04:41.176834Z","url":"https://files.pythonhosted.org/packages/29/4f/2a975c72f7da21e49576a2fe5974b244c7ed7dd5175dd007d7e532708808/benepar-0.0.1.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1.dev1":[{"comment_text":"","digests":{"blake2b_256":"a4f7d3841c17418b400c30e867483c0fd97b485fdb53849d31490416b03262b5","md5":"38cb3e187fc5273222522d2423bc6a9d","sha256":"f448d25061faeb815bd4050de41c95c8a2bd0b264f357b93e8155b7430063a5b"},"downloads":-1,"filename":"benepar-0.0.1.dev1.tar.gz","has_sig":false,"md5_digest":"38cb3e187fc5273222522d2423bc6a9d","packagetype":"sdist","python_version":"source","requires_python":null,"size":63912,"upload_time":"2018-05-27T22:35:25","upload_time_iso_8601":"2018-05-27T22:35:25.627834Z","url":"https://files.pythonhosted.org/packages/a4/f7/d3841c17418b400c30e867483c0fd97b485fdb53849d31490416b03262b5/benepar-0.0.1.dev1.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1.dev2":[{"comment_text":"","digests":{"blake2b_256":"9992db639161ca952b746ff0a18e360d085d40d3ece1c0bd7b67b895942e1e5f","md5":"c0ec14a3b25a264004ac3a326c54bf75","sha256":"acbc7111026aaf5e3c371f6bef1f8027488eeb824a8fdf47ff8bd296632822eb"},"downloads":-1,"filename":"benepar-0.0.1.dev2.tar.gz","has_sig":false,"md5_digest":"c0ec14a3b25a264004ac3a326c54bf75","packagetype":"sdist","python_version":"source","requires_python":null,"size":64374,"upload_time":"2018-05-28T22:50:52","upload_time_iso_8601":"2018-05-28T22:50:52.931445Z","url":"https://files.pythonhosted.org/packages/99/92/db639161ca952b746ff0a18e360d085d40d3ece1c0bd7b67b895942e1e5f/benepar-0.0.1.dev2.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1.dev3":[{"comment_text":"","digests":{"blake2b_256":"a053a9b94283843b8cb4afe96e119e4c69df50103b136c700185d4fec05e9b00","md5":"a08e0e89410a3751a7f6b1fe5d723ec0","sha256":"9a4a1365f9eff9743c65b67cca658e41271fca4a070a08ebe6757868fc2986bb"},"downloads":-1,"filename":"benepar-0.0.1.dev3.tar.gz","has_sig":false,"md5_digest":"a08e0e89410a3751a7f6b1fe5d723ec0","packagetype":"sdist","python_version":"source","requires_python":null,"size":64369,"upload_time":"2018-05-28T23:03:35","upload_time_iso_8601":"2018-05-28T23:03:35.878694Z","url":"https://files.pythonhosted.org/packages/a0/53/a9b94283843b8cb4afe96e119e4c69df50103b136c700185d4fec05e9b00/benepar-0.0.1.dev3.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1.dev4":[{"comment_text":"","digests":{"blake2b_256":"ad509b18ae53ddaeeca1f1aaf6fbe70c98395611120baabf4ec86285dadad70c","md5":"c3dd730dcf57c17ae06201a9273ec5d3","sha256":"662cc6eedc6175158cad60b1df7f7f4d38c9dcc7e2cb87ff94f8597d6410418a"},"downloads":-1,"filename":"benepar-0.0.1.dev4.tar.gz","has_sig":false,"md5_digest":"c3dd730dcf57c17ae06201a9273ec5d3","packagetype":"sdist","python_version":"source","requires_python":null,"size":64510,"upload_time":"2018-05-28T23:41:05","upload_time_iso_8601":"2018-05-28T23:41:05.501283Z","url":"https://files.pythonhosted.org/packages/ad/50/9b18ae53ddaeeca1f1aaf6fbe70c98395611120baabf4ec86285dadad70c/benepar-0.0.1.dev4.tar.gz","yanked":false,"yanked_reason":null}],"0.0.1.dev5":[{"comment_text":"","digests":{"blake2b_256":"eff2c7c8be7ccb41d37fc7b4d1ce04c216668bc0e83d90c6325200542fbf3843","md5":"09f4bf508a3e5d1ec10b29750168f508","sha256":"776debea490e26d9b52700c80902c5166519f9e213185bb0cf3266286d217f83"},"downloads":-1,"filename":"benepar-0.0.1.dev5.tar.gz","has_sig":false,"md5_digest":"09f4bf508a3e5d1ec10b29750168f508","packagetype":"sdist","python_version":"source","requires_python":null,"size":64514,"upload_time":"2018-05-29T00:01:54","upload_time_iso_8601":"2018-05-29T00:01:54.038215Z","url":"https://files.pythonhosted.org/packages/ef/f2/c7c8be7ccb41d37fc7b4d1ce04c216668bc0e83d90c6325200542fbf3843/benepar-0.0.1.dev5.tar.gz","yanked":false,"yanked_reason":null}],"0.0.2":[{"comment_text":"","digests":{"blake2b_256":"d1ad0a6760cd1e455093820803b768d91c6c16a4672c7fc3a24f51e71d10e6be","md5":"7d7fd1027329b5832a4b90a262330f55","sha256":"4cfe6f2d541f0f9e218ad91c3bdb4885031567cb65896b87571aaab2665b0e07"},"downloads":-1,"filename":"benepar-0.0.2.tar.gz","has_sig":false,"md5_digest":"7d7fd1027329b5832a4b90a262330f55","packagetype":"sdist","python_version":"source","requires_python":null,"size":61076,"upload_time":"2018-06-20T04:33:50","upload_time_iso_8601":"2018-06-20T04:33:50.319082Z","url":"https://files.pythonhosted.org/packages/d1/ad/0a6760cd1e455093820803b768d91c6c16a4672c7fc3a24f51e71d10e6be/benepar-0.0.2.tar.gz","yanked":false,"yanked_reason":null}],"0.0.3":[{"comment_text":"","digests":{"blake2b_256":"12a7f5322420e6eb8528a6874e608aabf539a4759cfb324f5a63997c3eda991d","md5":"98bbf36e147fca63909cd02ac7675ce2","sha256":"7fa5576536375f29007e5d448b04f909fa1c8841daaa65e9554dc93fb3add4e0"},"downloads":-1,"filename":"benepar-0.0.3.tar.gz","has_sig":false,"md5_digest":"98bbf36e147fca63909cd02ac7675ce2","packagetype":"sdist","python_version":"source","requires_python":null,"size":61177,"upload_time":"2018-08-25T03:09:01","upload_time_iso_8601":"2018-08-25T03:09:01.871855Z","url":"https://files.pythonhosted.org/packages/12/a7/f5322420e6eb8528a6874e608aabf539a4759cfb324f5a63997c3eda991d/benepar-0.0.3.tar.gz","yanked":false,"yanked_reason":null}],"0.1.0":[{"comment_text":"","digests":{"blake2b_256":"c4fcd6dbd2ced6a954391e78d9e782a181bd97bcf46bcaac0221d51d6a6ba9e1","md5":"742673a9b12016cd9998cd090267f69c","sha256":"0a2372d0f4a7f97100b69cd5c4f05ab71b103508dd1c2ebe0ac9efcdad45083b"},"downloads":-1,"filename":"benepar-0.1.0.tar.gz","has_sig":false,"md5_digest":"742673a9b12016cd9998cd090267f69c","packagetype":"sdist","python_version":"source","requires_python":null,"size":67342,"upload_time":"2018-12-31T18:20:19","upload_time_iso_8601":"2018-12-31T18:20:19.624740Z","url":"https://files.pythonhosted.org/packages/c4/fc/d6dbd2ced6a954391e78d9e782a181bd97bcf46bcaac0221d51d6a6ba9e1/benepar-0.1.0.tar.gz","yanked":false,"yanked_reason":null}],"0.1.1":[{"comment_text":"","digests":{"blake2b_256":"e6d1ff387f5eefc49fcfc40b1e1c2f31cf9b5f7cd11b6ce922318199e3fa0526","md5":"bbab393e1b77bcf39c311724704d7866","sha256":"d4858e54249fa3c04893a014b0bd54b0bc3bd13fb513443a114a9c5893c43bfa"},"downloads":-1,"filename":"benepar-0.1.1.tar.gz","has_sig":false,"md5_digest":"bbab393e1b77bcf39c311724704d7866","packagetype":"sdist","python_version":"source","requires_python":null,"size":67363,"upload_time":"2018-12-31T20:14:43","upload_time_iso_8601":"2018-12-31T20:14:43.935600Z","url":"https://files.pythonhosted.org/packages/e6/d1/ff387f5eefc49fcfc40b1e1c2f31cf9b5f7cd11b6ce922318199e3fa0526/benepar-0.1.1.tar.gz","yanked":false,"yanked_reason":null}],"0.1.2":[{"comment_text":"","digests":{"blake2b_256":"a07b6cd9c60e1613a5ad388b4f883fa2aeaddcd8a7ad0a8d5ed87e0d23f159d8","md5":"d64ae2e8c35bf0ab20820f11f348f622","sha256":"a1cc18c531e4438f87b5abea93aa876cd96f20ec351f2bbe69c1d9fc96347a39"},"downloads":-1,"filename":"benepar-0.1.2.tar.gz","has_sig":false,"md5_digest":"d64ae2e8c35bf0ab20820f11f348f622","packagetype":"sdist","python_version":"source","requires_python":null,"size":72737,"upload_time":"2019-02-01T05:40:24","upload_time_iso_8601":"2019-02-01T05:40:24.372535Z","url":"https://files.pythonhosted.org/packages/a0/7b/6cd9c60e1613a5ad388b4f883fa2aeaddcd8a7ad0a8d5ed87e0d23f159d8/benepar-0.1.2.tar.gz","yanked":false,"yanked_reason":null}],"0.1.3":[{"comment_text":"","digests":{"blake2b_256":"7263dcd46b4bea9430ac6b4df8415016afa6cd724444edf75de0c94f145672a5","md5":"ac0485f044d59388ce7d21d10d3666ae","sha256":"2e3178300e2a5d51a60cca1504a10b229fd115e8107b12a94fcbf792404f43fc"},"downloads":-1,"filename":"benepar-0.1.3.tar.gz","has_sig":false,"md5_digest":"ac0485f044d59388ce7d21d10d3666ae","packagetype":"sdist","python_version":"source","requires_python":null,"size":68316,"upload_time":"2021-02-05T10:46:18","upload_time_iso_8601":"2021-02-05T10:46:18.674328Z","url":"https://files.pythonhosted.org/packages/72/63/dcd46b4bea9430ac6b4df8415016afa6cd724444edf75de0c94f145672a5/benepar-0.1.3.tar.gz","yanked":false,"yanked_reason":null}],"0.2.0":[{"comment_text":"","digests":{"blake2b_256":"9e17c398a35d0f303a534de8ec6949aa2ee68cc6bdbf0930685d92719b97aa1e","md5":"2f04e5f0d73013cd238725865a5efa2e","sha256":"212177314968f1bc389b3201c06aa06a563c47e89fd566544536c7595ad5f652"},"downloads":-1,"filename":"benepar-0.2.0.tar.gz","has_sig":false,"md5_digest":"2f04e5f0d73013cd238725865a5efa2e","packagetype":"sdist","python_version":"source","requires_python":">=3.6","size":33765,"upload_time":"2021-02-14T03:44:08","upload_time_iso_8601":"2021-02-14T03:44:08.922323Z","url":"https://files.pythonhosted.org/packages/9e/17/c398a35d0f303a534de8ec6949aa2ee68cc6bdbf0930685d92719b97aa1e/benepar-0.2.0.tar.gz","yanked":false,"yanked_reason":null}],"0.2.0a0":[{"comment_text":"","digests":{"blake2b_256":"1e0a69151b8919f30ce5be491c8f6421095c1359550f2f68d6c0546c5892586f","md5":"381e171dea7cdfaa72dd61ef996bdc87","sha256":"422a17fbd3f77452ab189d66fa2faefe2d5f30328ff1c6b13ad66cf03d20d2d4"},"downloads":-1,"filename":"benepar-0.2.0a0.tar.gz","has_sig":false,"md5_digest":"381e171dea7cdfaa72dd61ef996bdc87","packagetype":"sdist","python_version":"source","requires_python":">=3.6","size":33734,"upload_time":"2021-02-05T15:30:20","upload_time_iso_8601":"2021-02-05T15:30:20.976078Z","url":"https://files.pythonhosted.org/packages/1e/0a/69151b8919f30ce5be491c8f6421095c1359550f2f68d6c0546c5892586f/benepar-0.2.0a0.tar.gz","yanked":false,"yanked_reason":null}]},"urls":[{"comment_text":"","digests":{"blake2b_256":"9e17c398a35d0f303a534de8ec6949aa2ee68cc6bdbf0930685d92719b97aa1e","md5":"2f04e5f0d73013cd238725865a5efa2e","sha256":"212177314968f1bc389b3201c06aa06a563c47e89fd566544536c7595ad5f652"},"downloads":-1,"filename":"benepar-0.2.0.tar.gz","has_sig":false,"md5_digest":"2f04e5f0d73013cd238725865a5efa2e","packagetype":"sdist","python_version":"source","requires_python":">=3.6","size":33765,"upload_time":"2021-02-14T03:44:08","upload_time_iso_8601":"2021-02-14T03:44:08.922323Z","url":"https://files.pythonhosted.org/packages/9e/17/c398a35d0f303a534de8ec6949aa2ee68cc6bdbf0930685d92719b97aa1e/benepar-0.2.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}
