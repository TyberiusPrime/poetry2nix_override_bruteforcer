{"info":{"author":"","author_email":"Averell Levan <lvhuyen@yahoo.com>","bugtrack_url":null,"classifiers":["Development Status :: 4 - Beta","Intended Audience :: Developers","License :: OSI Approved :: GNU Lesser General Public License v3 or later (LGPLv3+)","Programming Language :: Python :: 3","Topic :: Software Development :: Build Tools"],"description":"# Spark - Dataframe with complex schema\n\n# Changelog\n### Version 1.0.0:\n* **_[Breaking changes]_** `flatten()` now stops the unpacking of nested data at ArrayType\n  (i.e: any field with DataType = ArrayType will have its nested elements as-is).\n  To have the same result as in the previous version - flatten all array fields, add the param `arrays_to_unpack = [\"*\"]`.\n* Added `snake_case()`\n* Added `json_schema_to_spark_schema()`\n* Added support for providing the param `nested_struct_separator` to `flatten()`.\n  Example: When provided with the value \"___\", the raw schema `{\"parent\": {\"child\": \"some_value\"}}` will be unpacked to `{\"parrent___child\": \"some_value\"}`\n\n# Problem description\nA Spark DataFrame can have a simple schema, where every single column is of a simple datatype like `IntegerType, BooleanType, StringType`,...\nHowever, a column can be of one of the complex types: `ArrayType`, `MapType`, or `StructType`. The schema itself is, actually, an instance of the type `StructType`. So, when a schema has column(s) with DataType is StructType, we have a nested schema.\n\nWorking with nested schema is not always easy. Some notable problems are:\n* Complex SQL queries\n* Difficult to rename/cast datatype of nested columns\n* Unnecessary high IO when reading only some nested columns from Parquet files (_https://issues.apache.org/jira/browse/SPARK-17636_)\n\nThe page *https://docs.databricks.com/delta/data-transformation/complex-types.html* provides a lot of useful tips on dealing with dataframes having a complex schema.<br>\n\nThis page will provide some further tips/utils to work on dataframes with more complex schema:\n* Renaming or casting/parsing nested columns\n* Flattening\n\n_Please note the term **Flattening** in this post **only** means getting rid of StructType column(s) in our DataFrame. This does not include the act of eliminating ArrayType and MapType in the schema, which is usually called **Exploding** in Spark documents. Also, Spark 2.4 introduced the function flatten [https://spark.apache.org/docs/2.4.0/api/java/org/apache/spark/sql/functions.html#flatten-org.apache.spark.sql.Column], which is used on a nested array (an array with 2 layers, an array of arrays of DataType) to make it flat (an array with 1 layer)._\n\n# Solutions\n## Renaming nested columns\nRenaming a column at root level is simple: use the function `withColumnRenamed`.\nHowever, with a nested column, that function does not give any error, but also does not make any effect:\n\n\tdf_struct = spark.createDataFrame([Row(structA=Row(field1=10, field2=1.5), structB=Row(field3=\"one\",field4=False))])\n    df_struct.printSchema()\n\t\n\troot\n     |-- structA: struct (nullable = true)\n     |    |-- field1: long (nullable = true)\n     |    |-- field2: double (nullable = true)\n     |-- structB: struct (nullable = true)\n     |    |-- field3: string (nullable = true)\n     |    |-- field4: boolean (nullable = true)\n    \n    df_struct.withColumnRenamed(\"structA.field1\", \"structA.newField1\") \\\n        .withColumnRenamed(\"structB\", \"newStructB\") \\\n        .printSchema()\n\n\troot\n     |-- structA: struct (nullable = true)\n     |    |-- field1: long (nullable = true)\n     |    |-- field2: double (nullable = true)\n     |-- newStructB: struct (nullable = true)\n     |    |-- field3: string (nullable = true)\n     |    |-- field4: boolean (nullable = true)\n\nTo change the names of nested columns, there are some options:\n1. By building a new struct column on the flight with the `struct()` function:\n\n\t\tfrom pyspark.sql.functions import struct, col\n\t\tdf_renamed = df_struct.withColumn(\"structA\", struct(col(\"structA.field1\").alias(\"newField1\"),\n\t\t                                                    col(\"structA.field2\")))\n\n2. By creating a new *schema* (a `StructType()` object) and use type casting on the original struct column:\n\n\t\tfrom pyspark.sql.types import *\t\t\n\t\tnewStructASchema = StructType([\n\t                            StructField(\"newField1\", LongType()),\n\t                            StructField(\"field2\", DoubleType())\n\t                        ])\n\t    df_renamed = df_struct.withColumn(\"structA\", col(\"structA\").cast(newStructASchema)).printSchema()\n\t    \nBoth options yield the same schema\n\n\t    root\n\t     |-- structA: struct (nullable = true)\n\t     |    |-- newField1: long (nullable = true)\n\t     |    |-- field2: double (nullable = true)\n\t     |-- structB: struct (nullable = true)\n\t     |    |-- field3: string (nullable = true)\n\t     |    |-- field4: boolean (nullable = true)\n\nThe 2nd option is more convenient when building a recursive function to recreate the multi-layer nested schema with new columns names.\n\n---\n\n## Flattening\n### StructType\n\nSample DataFrame:\n\n    from pyspark.sql import Row\n    from pyspark.sql.functions import col\n\n    df_struct = spark.createDataFrame([Row(structA=Row(field1=10, field2=1.5),\n                                           structB=Row(field3=\"one\",field4=False))])\n\tdf_struct.printSchema()\n\n\troot\n\t |-- structA: struct (nullable = true)\n\t |    |-- field1: long (nullable = true)\n\t |    |-- field2: double (nullable = true)\n\t |-- structB: struct (nullable = true)\n\t |    |-- field3: string (nullable = true)\n\t |    |-- field4: boolean (nullable = true)\n\nSpark allows selecting nested columns by using the dot `.` notation:\n\n\tdf_struct.select(\"structA.*\", \"structB.field3\").printSchema()\n\t\n\troot\n     |-- field1: long (nullable = true)\n     |-- field2: double (nullable = true)\n     |-- field3: string (nullable = true)\nPlease note here that the current Spark implementation (2.4.3 or below) doesn't keep the outer layer fieldname (e.g: structA) in the output dataframe\n\n### ArrayType\nTo select only some elements from an ArrayType column, either *`getItem()`* or using brackets (as selecting elements from a legacy array: `[]` in Python `()` in Scala)  would do the trick:\n\n\tdf_array = spark.createDataFrame([Row(arrayA=[1,2,3,4,5],fieldB=\"foo\")])\n\tdf_array.select(col(\"arrayA\").getItem(0).alias(\"element0\"), col(\"arrayA\")[4].alias(\"element5\"), col(\"fieldB\")).show()\n\t\n\t+--------+--------+------+\n    |element0|element5|fieldB|\n    +--------+--------+------+\n    |       1|       5|   foo|\n    +--------+--------+------+\n\n### MapType\nElements from a MapType column can be selected the same way as in the case of ArrayType, but using the key instead of index number. The dot notation (`.`) could also be used instead of `getItem()` or brackets:\n\n    df_map = spark.createDataFrame([Row(mapA={2: \"TWO\", 3: \"THREE\", 0: \"ZERO\"}, fieldB=\"foo\")])\n    df_map.select(col(\"mapA\")[3].alias(\"element3\"), col(\"mapA\").getItem(2).alias(\"element2\"), col(\"mapA.0\").alias(\"element0\"), col(\"mapA\").getItem(1).alias(\"element1\")).show()\n\t\n\t+--------+--------+--------+--------+\n    |element3|element2|element0|element1|\n    +--------+--------+--------+--------+\n    |   THREE|     TWO|    ZERO|    null|\n    +--------+--------+--------+--------+\n\n### StructType nested in StructType\nAs Spark DataFrame.select() supports passing an array of columns to be selected, to fully unflatten a multi-layer nested dataframe, a recursive call would do the trick.\n\nHere is a detailed discussion on StackOverFlow on how to do this:\nhttps://stackoverflow.com/questions/37471346/automatically-and-elegantly-flatten-dataframe-in-spark-sql\n\n### StructType nested in ArrayType\n\tdf_nested = spark.createDataFrame([\n\t    Row(\n\t        arrayA=[\n\t            Row(childStructB=Row(field1=1, field2=\"foo\")),\n\t            Row(childStructB=Row(field1=2, field2=\"bar\"))\n\t        ]\n\t    )])\n\tdf_nested.printSchema()\n\t\n\troot\n     |-- arrayA: array (nullable = true)\n     |    |-- element: struct (containsNull = true)\n     |    |    |-- childStructB: struct (nullable = true)\n     |    |    |    |-- field1: long (nullable = true)\n     |    |    |    |-- field2: string (nullable = true)\n     \n    df_nested.show(1, False)\n    \n    +------------------------+\n    |arrayA                  |\n    +------------------------+\n    |[[[1, foo]], [[2, bar]]]|\n    +------------------------+\n     \nSelecting *field1* or *field2* can be done as with normal structs (not nested inside an array), by using that dot `.` annotation. The result would be of the type `ArrayType[ChildFieldType]`, which has been **_vertically sliced_** from the original array\n\n\tdf_child = df_nested.select(\"arrayA.childStructB.field1\", \"arrayA.childStructB.field2\")\n\tdf_child.printSchema()\n\t\n\troot\n     |-- field1: array (nullable = true)\n     |    |-- element: long (containsNull = true)\n     |-- field2: array (nullable = true)\n     |    |-- element: string (containsNull = true)\n     \n    df_child.show()\n\n    +------+----------+\n    |field1|    field2|\n    +------+----------+\n    |[1, 2]|[foo, bar]|\n    +------+----------+\n\n### StructType nested in MapType\nAs each MapType column has two components, the keys and the values, selecting nested column inside a MapType column is not straight forward - we cannot just use that `.` to take the nested fields as that has already been used for denoting the key. \n\n    df_map_nested = spark.createDataFrame([Row(mapA={\"2\": Row(type_name=\"Arabic number\", equivalent=2), \"THREE\": Row(type_name=\"English Text\", equivalent=3)}, fieldB=\"foo\")])\n    df_map_nested.select(col(\"mapA.type_name\"), col(\"mapA.THREE.type_name\")).show()\n    \n    +---------+------------+\n    |type_name|   type_name|\n    +---------+------------+\n    |     null|English Text|\n    +---------+------------+\n    \nA solution for this is to use the builtin function `map_values()` which has been introduced since Spark 2.3. Note the type of the result column: ArrayType\n\n    from pyspark.sql.functions import map_values\n    result = df_map_nested.select(map_values(\"mapA\")[\"type_name\"], col(\"mapA.THREE.type_name\"))\n    result.show(2,False)\n    result.printSchema()\n\n    +-----------------------------+------------+\n    |map_values(mapA).type_name   |type_name   |\n    +-----------------------------+------------+\n    |[Arabic number, English Text]|English Text|\n    +-----------------------------+------------+\n    \n    root\n     |-- map_values(mapA).type_name: array (nullable = true)\n     |    |-- element: string (containsNull = true)\n     |-- type_name: string (nullable = true)\n     \n\t\n## Hurdles\nThe above steps would work well for most of dataframes. The only dataframes that it fails (as of Spark 2.4.3 or lower) are the ones with a StructType nested inside MORE THAN ONE layers of ArrayType.\nLike this one:\n\n\tdf_nested_B = spark.createDataFrame([\n        Row(\n            arrayA=[[\n                Row(childStructB=Row(field1=1, field2=\"foo\")),\n                Row(childStructB=Row(field1=2, field2=\"bar\"))\n            ]]\n        )])\n    df_nested_B.printSchema()\n    \n    root\n     |-- arrayA: array (nullable = true)\n     |    |-- element: array (containsNull = true)\n     |    |    |-- element: struct (containsNull = true)\n     |    |    |    |-- childStructB: struct (nullable = true)\n     |    |    |    |    |-- field1: long (nullable = true)\n     |    |    |    |    |-- field2: string (nullable = true)\n     \nOr this one\n\n\tdf_nested_C = spark.createDataFrame([\n        Row(\n            arrayA=[\n                Row(childStructB=Row(childArrayC=[Row(field1=1, field2=\"foo\")])),\n                Row(childStructB=Row(childArrayC=[Row(field1=2, field2=\"bar\")])),\n            ]\n        )])\n    df_nested_C.printSchema()\n    \n    root\n     |-- arrayA: array (nullable = true)\n     |    |-- element: struct (containsNull = true)\n     |    |    |-- childStructB: struct (nullable = true)\n     |    |    |    |-- childArrayC: array (nullable = true)\n     |    |    |    |    |-- element: struct (containsNull = true)\n     |    |    |    |    |    |-- field1: long (nullable = true)\n     |    |    |    |    |    |-- field2: string (nullable = true)\n     \nSelecting `arrayA.childStructB.field1` from `df_nested_B` fails with the error message: `AnalysisException: No such struct field field1 in childStructB`.<br>\nWhile selecting `arrayA.childStructB.childArrayC.field1` from `df_nested_C` throws the `AnalysisException`: `cannot resolve 'arrayA.childStructB.childArrayC['field1']' due to data type mismatch: argument 2 requires integral type, however, ''field1'' is of string type.`\n\n## (More) Solutions\nWith the introduction of the SQL function `transform` in Spark 2.4, the error above can be solved by applying `transform` on every layer of the array.\n\nA comprehensive implementation of a flatten function can be found in the Python package `sparkaid`:\n\n\tfrom sparkaid import flatten\n\t\n\tflatten(df_nested_B).printSchema()\n\t\n\troot\n\t |-- arrayA__childStructB_field1: array (nullable = true)\n\t |    |-- element: array (containsNull = true)\n\t |    |    |-- element: long (containsNull = true)\n\t |-- arrayA__childStructB_field2: array (nullable = true)\n\t |    |-- element: array (containsNull = true)\n\t |    |    |-- element: string (containsNull = true)\n<p>\n\n\tflatten(df_nested_B).show()\n\t\n\t+---------------------------+---------------------------+\n    |arrayA__childStructB_field1|arrayA__childStructB_field2|\n    +---------------------------+---------------------------+\n    |                   [[1, 2]]|               [[foo, bar]]|\n    +---------------------------+---------------------------+\n<p>\n  \n    flatten(df_nested_C).printSchema()\n\n    root\n     |-- arrayA_childStructB_childArrayC_field1: array (nullable = true)\n     |    |-- element: array (containsNull = true)\n     |    |    |-- element: long (containsNull = true)\n     |-- arrayA_childStructB_childArrayC_field2: array (nullable = true)\n     |    |-- element: array (containsNull = true)\n     |    |    |-- element: string (containsNull = true)\n\n[]: https://spark.apache.org/docs/2.4.0/api/java/org/apache/spark/sql/functions.html#flatten-org.apache.spark.sql.Column\n","description_content_type":"text/markdown","docs_url":null,"download_url":"","downloads":{"last_day":-1,"last_month":-1,"last_week":-1},"dynamic":null,"home_page":"","keywords":"SPARK,DATAFRAME,MANIPULATE","license":"","maintainer":"","maintainer_email":"","name":"sparkaid","package_url":"https://pypi.org/project/sparkaid/","platform":null,"project_url":"https://pypi.org/project/sparkaid/","project_urls":{"Homepage":"https://github.com/lvhuyen/SparkAid"},"provides_extra":null,"release_url":"https://pypi.org/project/sparkaid/1.0.0/","requires_dist":["pyspark (>=2.4.0)","pip-tools ; extra == 'dev'","pytest ; extra == 'dev'"],"requires_python":">=3.4","summary":"Utils for working with Spark","version":"1.0.0","yanked":false,"yanked_reason":null},"last_serial":14921631,"releases":{"0.1-beta":[{"comment_text":"","digests":{"blake2b_256":"a2e5380082b01d4532faf1867a41458c419999b24d384f582afd65d56dff6562","md5":"0b84f971d0f0878872cf47f88b9b563d","sha256":"dfb5e5320cd48a40fd4f2c3842a9b34239bc0280e88c30610469e8cf8d8de00c"},"downloads":-1,"filename":"sparkaid-0.1-beta.tar.gz","has_sig":false,"md5_digest":"0b84f971d0f0878872cf47f88b9b563d","packagetype":"sdist","python_version":"source","requires_python":null,"size":1613,"upload_time":"2019-08-23T05:45:41","upload_time_iso_8601":"2019-08-23T05:45:41.363444Z","url":"https://files.pythonhosted.org/packages/a2/e5/380082b01d4532faf1867a41458c419999b24d384f582afd65d56dff6562/sparkaid-0.1-beta.tar.gz","yanked":false,"yanked_reason":null}],"1.0.0":[{"comment_text":"","digests":{"blake2b_256":"59564cf599e0d4c9ff7ac88f1a53f5604f67c5ac67343b1b7ebce651700df1c9","md5":"eb8d62cc0604b9ccb8dc153357143eef","sha256":"84dcda3045860241de5697846573ffa012adcc69baa99db7de92de47c52ce332"},"downloads":-1,"filename":"sparkaid-1.0.0-py3-none-any.whl","has_sig":false,"md5_digest":"eb8d62cc0604b9ccb8dc153357143eef","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.4","size":8612,"upload_time":"2022-08-29T07:57:11","upload_time_iso_8601":"2022-08-29T07:57:11.532179Z","url":"https://files.pythonhosted.org/packages/59/56/4cf599e0d4c9ff7ac88f1a53f5604f67c5ac67343b1b7ebce651700df1c9/sparkaid-1.0.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"c11e92e74e641719430d3d6216e4eef34e90a89876080c2013d8a5aa557b4284","md5":"0234ddb182483f592240a49714015e04","sha256":"914b4521f87d0f5f0d92e3e7f90f3b974651a168d8c7a547dfd0d8c657f33073"},"downloads":-1,"filename":"sparkaid-1.0.0.tar.gz","has_sig":false,"md5_digest":"0234ddb182483f592240a49714015e04","packagetype":"sdist","python_version":"source","requires_python":">=3.4","size":12174,"upload_time":"2022-08-29T07:57:13","upload_time_iso_8601":"2022-08-29T07:57:13.276361Z","url":"https://files.pythonhosted.org/packages/c1/1e/92e74e641719430d3d6216e4eef34e90a89876080c2013d8a5aa557b4284/sparkaid-1.0.0.tar.gz","yanked":false,"yanked_reason":null}]},"urls":[{"comment_text":"","digests":{"blake2b_256":"59564cf599e0d4c9ff7ac88f1a53f5604f67c5ac67343b1b7ebce651700df1c9","md5":"eb8d62cc0604b9ccb8dc153357143eef","sha256":"84dcda3045860241de5697846573ffa012adcc69baa99db7de92de47c52ce332"},"downloads":-1,"filename":"sparkaid-1.0.0-py3-none-any.whl","has_sig":false,"md5_digest":"eb8d62cc0604b9ccb8dc153357143eef","packagetype":"bdist_wheel","python_version":"py3","requires_python":">=3.4","size":8612,"upload_time":"2022-08-29T07:57:11","upload_time_iso_8601":"2022-08-29T07:57:11.532179Z","url":"https://files.pythonhosted.org/packages/59/56/4cf599e0d4c9ff7ac88f1a53f5604f67c5ac67343b1b7ebce651700df1c9/sparkaid-1.0.0-py3-none-any.whl","yanked":false,"yanked_reason":null},{"comment_text":"","digests":{"blake2b_256":"c11e92e74e641719430d3d6216e4eef34e90a89876080c2013d8a5aa557b4284","md5":"0234ddb182483f592240a49714015e04","sha256":"914b4521f87d0f5f0d92e3e7f90f3b974651a168d8c7a547dfd0d8c657f33073"},"downloads":-1,"filename":"sparkaid-1.0.0.tar.gz","has_sig":false,"md5_digest":"0234ddb182483f592240a49714015e04","packagetype":"sdist","python_version":"source","requires_python":">=3.4","size":12174,"upload_time":"2022-08-29T07:57:13","upload_time_iso_8601":"2022-08-29T07:57:13.276361Z","url":"https://files.pythonhosted.org/packages/c1/1e/92e74e641719430d3d6216e4eef34e90a89876080c2013d8a5aa557b4284/sparkaid-1.0.0.tar.gz","yanked":false,"yanked_reason":null}],"vulnerabilities":[]}
